"""
Cross-Modal Fusion Mechanisms
è·¨æ¨¡æ…‹èåˆæ©Ÿåˆ¶å¯¦ä½œ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union
import math


class CrossModalAttention(nn.Module):
    """
    è·¨æ¨¡æ…‹æ³¨æ„åŠ›æ©Ÿåˆ¶
    è¨ˆç®—ä¸åŒæ¨¡æ…‹é–“çš„äº¤äº’æ³¨æ„åŠ›

    æ”¯æ´çš„æ¨¡æ…‹å°:
    - Visual â†” Audio: å‹•ä½œèˆ‡ç™¼éŸ³çš„æ™‚åºå°æ‡‰
    - Visual â†” Text: æ‰‹å‹¢èˆ‡èªç¾©çš„æ¦‚å¿µæ˜ å°„
    - Audio â†” Text: ç™¼éŸ³èˆ‡è©å½™çš„èªéŸ³å°é½Š
    """

    def __init__(
        self,
        feature_dim: int = 512,
        num_heads: int = 8,
        dropout: float = 0.1,
        temperature: float = 1.0
    ):
        super().__init__()

        self.feature_dim = feature_dim
        self.num_heads = num_heads
        self.head_dim = feature_dim // num_heads
        self.temperature = temperature

        assert feature_dim % num_heads == 0, "feature_dim must be divisible by num_heads"

        # å¤šé ­æ³¨æ„åŠ›æ©Ÿåˆ¶
        self.attention = nn.MultiheadAttention(
            embed_dim=feature_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )

        # å±¤æ­£è¦åŒ–
        self.layer_norm = nn.LayerNorm(feature_dim)

        # ä½ç½®ç·¨ç¢¼ (ç”¨æ–¼åºåˆ—æ¨¡æ…‹)
        self.pos_encoding = nn.Parameter(torch.randn(1, 100, feature_dim) * 0.1)

    def forward(
        self,
        query_modal: torch.Tensor,
        key_value_modal: torch.Tensor,
        is_query_sequence: bool = False,
        is_kv_sequence: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è¨ˆç®—è·¨æ¨¡æ…‹æ³¨æ„åŠ›

        Args:
            query_modal: æŸ¥è©¢æ¨¡æ…‹ç‰¹å¾µ (batch, [seq_len,] feature_dim)
            key_value_modal: éµå€¼æ¨¡æ…‹ç‰¹å¾µ (batch, [seq_len,] feature_dim)
            is_query_sequence: æŸ¥è©¢æ¨¡æ…‹æ˜¯å¦ç‚ºåºåˆ—
            is_kv_sequence: éµå€¼æ¨¡æ…‹æ˜¯å¦ç‚ºåºåˆ—

        Returns:
            attended_features: æ³¨æ„åŠ›åŠ æ¬Šå¾Œçš„ç‰¹å¾µ
            attention_weights: æ³¨æ„åŠ›æ¬Šé‡
        """
        batch_size = query_modal.size(0)

        # è™•ç†éåºåˆ—æ•¸æ“šï¼Œæ“´å±•ç‚ºåºåˆ—
        if not is_query_sequence:
            query_modal = query_modal.unsqueeze(1)  # (batch, 1, feature_dim)
        if not is_kv_sequence:
            key_value_modal = key_value_modal.unsqueeze(1)  # (batch, 1, feature_dim)

        # æ·»åŠ ä½ç½®ç·¨ç¢¼ (åƒ…å°çœŸæ­£çš„åºåˆ—æ•¸æ“š)
        if is_query_sequence and query_modal.size(1) <= 100:
            seq_len = query_modal.size(1)
            query_modal = query_modal + self.pos_encoding[:, :seq_len, :]

        if is_kv_sequence and key_value_modal.size(1) <= 100:
            seq_len = key_value_modal.size(1)
            key_value_modal = key_value_modal + self.pos_encoding[:, :seq_len, :]

        # å¤šé ­æ³¨æ„åŠ›è¨ˆç®—
        attended_features, attention_weights = self.attention(
            query=query_modal,
            key=key_value_modal,
            value=key_value_modal
        )

        # å±¤æ­£è¦åŒ–å’Œæ®˜å·®é€£æ¥
        attended_features = self.layer_norm(attended_features + query_modal)

        # å¦‚æœåŸå§‹è¼¸å…¥ä¸æ˜¯åºåˆ—ï¼Œå‰‡å£“ç¸®å›åŸå§‹å½¢ç‹€
        if not is_query_sequence:
            attended_features = attended_features.squeeze(1)
            attention_weights = attention_weights.squeeze(1)

        return attended_features, attention_weights


class AdaptiveModalWeighting(nn.Module):
    """
    è‡ªé©æ‡‰æ¨¡æ…‹æ¬Šé‡å­¸ç¿’
    æ ¹æ“šè¼¸å…¥å‹•æ…‹èª¿æ•´ä¸åŒæ¨¡æ…‹çš„é‡è¦æ€§
    """

    def __init__(
        self,
        feature_dim: int = 512,
        max_modalities: int = 3,
        hidden_dim: int = 256
    ):
        super().__init__()

        self.feature_dim = feature_dim
        self.max_modalities = max_modalities

        # ç‚ºä¸åŒæ•¸é‡çš„æ¨¡æ…‹å‰µå»ºæ¬Šé‡ç”Ÿæˆç¶²è·¯
        self.weight_generators = nn.ModuleDict()
        for num_modals in range(1, max_modalities + 1):
            self.weight_generators[str(num_modals)] = nn.Sequential(
                nn.Linear(feature_dim * num_modals, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, num_modals),
                nn.Softmax(dim=-1)
            )

    def forward(self, modal_features: List[torch.Tensor]) -> torch.Tensor:
        """
        è¨ˆç®—è‡ªé©æ‡‰æ¬Šé‡

        Args:
            modal_features: æ¨¡æ…‹ç‰¹å¾µåˆ—è¡¨

        Returns:
            weights: è‡ªé©æ‡‰æ¬Šé‡ (batch, num_modalities)
        """
        num_modalities = len(modal_features)

        if num_modalities == 1:
            # å–®æ¨¡æ…‹æƒ…æ³ï¼Œæ¬Šé‡ç‚º1
            batch_size = modal_features[0].size(0)
            return torch.ones(batch_size, 1, device=modal_features[0].device)

        # æ‹¼æ¥æ‰€æœ‰æ¨¡æ…‹ç‰¹å¾µ
        concatenated = torch.cat(modal_features, dim=-1)

        # æ ¹æ“šæ¨¡æ…‹æ•¸é‡é¸æ“‡å°æ‡‰çš„æ¬Šé‡ç”Ÿæˆå™¨
        generator_key = str(num_modalities)
        if generator_key not in self.weight_generators:
            raise ValueError(f"ä¸æ”¯æ´{num_modalities}å€‹æ¨¡æ…‹çš„æ¬Šé‡ç”Ÿæˆ")

        # ç”Ÿæˆæ¬Šé‡
        weights = self.weight_generators[generator_key](concatenated)

        return weights


class ModalFusion(nn.Module):
    """
    å¤šæ¨¡æ…‹èåˆæ¨¡çµ„
    æ”¯æ´å¤šç¨®èåˆç­–ç•¥å’Œéˆæ´»çš„æ¨¡æ…‹çµ„åˆ
    """

    def __init__(
        self,
        feature_dim: int = 512,
        fusion_strategy: str = 'attention',  # 'attention', 'concat', 'weighted_avg'
        num_attention_heads: int = 8,
        dropout: float = 0.1,
        use_adaptive_weighting: bool = True
    ):
        super().__init__()

        self.feature_dim = feature_dim
        self.fusion_strategy = fusion_strategy
        self.use_adaptive_weighting = use_adaptive_weighting

        # è·¨æ¨¡æ…‹æ³¨æ„åŠ›æ¨¡çµ„
        self.cross_attention = CrossModalAttention(
            feature_dim=feature_dim,
            num_heads=num_attention_heads,
            dropout=dropout
        )

        # è‡ªé©æ‡‰æ¬Šé‡æ¨¡çµ„
        if use_adaptive_weighting:
            self.adaptive_weighting = AdaptiveModalWeighting(feature_dim)

        # èåˆå¾Œçš„æŠ•å½±å±¤
        if fusion_strategy == 'concat':
            # æ‹¼æ¥èåˆéœ€è¦é™ç¶­
            self.fusion_projection = nn.Sequential(
                nn.Linear(feature_dim * 3, feature_dim),  # å‡è¨­æœ€å¤š3å€‹æ¨¡æ…‹
                nn.LayerNorm(feature_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout)
            )
        else:
            # å…¶ä»–ç­–ç•¥ä¿æŒåŸç¶­åº¦
            self.fusion_projection = nn.Sequential(
                nn.LayerNorm(feature_dim),
                nn.Dropout(dropout)
            )

        # æ¨¡æ…‹ç‰¹å®šçš„æ³¨æ„åŠ›æ¬Šé‡ (å¯å­¸ç¿’åƒæ•¸)
        self.modal_attention_weights = nn.Parameter(
            torch.ones(3, 3) / 3  # 3Ã—3 æ¨¡æ…‹å°æ³¨æ„åŠ›æ¬Šé‡çŸ©é™£
        )

    def forward(
        self,
        visual_features: Optional[torch.Tensor] = None,
        audio_features: Optional[torch.Tensor] = None,
        text_features: Optional[torch.Tensor] = None,
        return_attention_weights: bool = False
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Dict]]:
        """
        å¤šæ¨¡æ…‹èåˆå‰å‘å‚³æ’­

        Args:
            visual_features: è¦–è¦ºç‰¹å¾µ (batch, feature_dim)
            audio_features: éŸ³è¨Šç‰¹å¾µ (batch, feature_dim)
            text_features: æ–‡å­—ç‰¹å¾µ (batch, feature_dim)
            return_attention_weights: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ¬Šé‡

        Returns:
            fused_features: èåˆå¾Œçš„ç‰¹å¾µ (batch, feature_dim)
            attention_weights: æ³¨æ„åŠ›æ¬Šé‡å­—å…¸ (å¯é¸)
        """
        available_modalities = []
        modal_features = []

        # æ”¶é›†å¯ç”¨çš„æ¨¡æ…‹
        if visual_features is not None:
            available_modalities.append('visual')
            modal_features.append(visual_features)

        if audio_features is not None:
            available_modalities.append('audio')
            modal_features.append(audio_features)

        if text_features is not None:
            available_modalities.append('text')
            modal_features.append(text_features)

        if len(modal_features) == 0:
            raise ValueError("è‡³å°‘éœ€è¦ä¸€å€‹æ¨¡æ…‹çš„ç‰¹å¾µ")

        # å–®æ¨¡æ…‹æƒ…æ³ï¼šç›´æ¥è¿”å›
        if len(modal_features) == 1:
            fused = self.fusion_projection(modal_features[0])
            if return_attention_weights:
                return fused, {}
            return fused

        # å¤šæ¨¡æ…‹èåˆ
        fused_features, attention_weights = self._fuse_multiple_modalities(
            modal_features, available_modalities
        )

        if return_attention_weights:
            return fused_features, attention_weights
        return fused_features

    def _fuse_multiple_modalities(
        self,
        modal_features: List[torch.Tensor],
        modality_names: List[str]
    ) -> Tuple[torch.Tensor, Dict]:
        """èåˆå¤šå€‹æ¨¡æ…‹"""
        attention_weights = {}

        if self.fusion_strategy == 'attention':
            # æ³¨æ„åŠ›èåˆç­–ç•¥
            fused_features = self._attention_fusion(modal_features, modality_names, attention_weights)

        elif self.fusion_strategy == 'concat':
            # æ‹¼æ¥èåˆç­–ç•¥
            fused_features = self._concat_fusion(modal_features)

        elif self.fusion_strategy == 'weighted_avg':
            # åŠ æ¬Šå¹³å‡èåˆç­–ç•¥
            fused_features = self._weighted_avg_fusion(modal_features)

        else:
            raise ValueError(f"ä¸æ”¯æ´çš„èåˆç­–ç•¥: {self.fusion_strategy}")

        return fused_features, attention_weights

    def _attention_fusion(
        self,
        modal_features: List[torch.Tensor],
        modality_names: List[str],
        attention_weights: Dict
    ) -> torch.Tensor:
        """æ³¨æ„åŠ›èåˆå¯¦ä½œ"""
        num_modalities = len(modal_features)
        batch_size = modal_features[0].size(0)

        # è¨ˆç®—æ‰€æœ‰æ¨¡æ…‹å°çš„äº¤äº’æ³¨æ„åŠ›
        attended_features = []
        for i in range(num_modalities):
            query_modal = modal_features[i]
            attended_sum = torch.zeros_like(query_modal)

            for j in range(num_modalities):
                if i != j:
                    key_value_modal = modal_features[j]

                    # è¨ˆç®—è·¨æ¨¡æ…‹æ³¨æ„åŠ›
                    attended, attn_weights = self.cross_attention(
                        query_modal=query_modal,
                        key_value_modal=key_value_modal,
                        is_query_sequence=False,
                        is_kv_sequence=False
                    )

                    # è¨˜éŒ„æ³¨æ„åŠ›æ¬Šé‡
                    pair_name = f"{modality_names[i]}_{modality_names[j]}"
                    attention_weights[pair_name] = attn_weights

                    # æ‡‰ç”¨å¯å­¸ç¿’çš„æ¨¡æ…‹å°æ¬Šé‡
                    modal_weight = self.modal_attention_weights[i, j]
                    attended_sum += modal_weight * attended

            attended_features.append(attended_sum)

        # è‡ªé©æ‡‰æ¬Šé‡èåˆ
        if self.use_adaptive_weighting and len(attended_features) > 1:
            adaptive_weights = self.adaptive_weighting(attended_features)

            # åŠ æ¬Šå¹³å‡
            fused = torch.zeros_like(attended_features[0])
            for i, features in enumerate(attended_features):
                fused += adaptive_weights[:, i:i+1] * features
        else:
            # ç°¡å–®å¹³å‡
            fused = torch.stack(attended_features, dim=0).mean(dim=0)

        return self.fusion_projection(fused)

    def _concat_fusion(self, modal_features: List[torch.Tensor]) -> torch.Tensor:
        """æ‹¼æ¥èåˆå¯¦ä½œ"""
        # ç°¡å–®æ‹¼æ¥æ‰€æœ‰æ¨¡æ…‹ç‰¹å¾µ
        concatenated = torch.cat(modal_features, dim=-1)
        return self.fusion_projection(concatenated)

    def _weighted_avg_fusion(self, modal_features: List[torch.Tensor]) -> torch.Tensor:
        """åŠ æ¬Šå¹³å‡èåˆå¯¦ä½œ"""
        if self.use_adaptive_weighting:
            # ä½¿ç”¨è‡ªé©æ‡‰æ¬Šé‡
            adaptive_weights = self.adaptive_weighting(modal_features)

            fused = torch.zeros_like(modal_features[0])
            for i, features in enumerate(modal_features):
                fused += adaptive_weights[:, i:i+1] * features
        else:
            # ç°¡å–®å¹³å‡
            fused = torch.stack(modal_features, dim=0).mean(dim=0)

        return self.fusion_projection(fused)


class ContrastiveLoss(nn.Module):
    """
    è·¨æ¨¡æ…‹å°æ¯”å­¸ç¿’æå¤±
    ä¿ƒé€²åŒé¡æ¨£æœ¬çš„è·¨æ¨¡æ…‹ç‰¹å¾µå°é½Š
    """

    def __init__(self, temperature: float = 0.1, margin: float = 0.2):
        super().__init__()
        self.temperature = temperature
        self.margin = margin

    def forward(
        self,
        features_a: torch.Tensor,
        features_b: torch.Tensor,
        labels: torch.Tensor
    ) -> torch.Tensor:
        """
        è¨ˆç®—å°æ¯”æå¤±

        Args:
            features_a: æ¨¡æ…‹Açš„ç‰¹å¾µ (batch, feature_dim)
            features_b: æ¨¡æ…‹Bçš„ç‰¹å¾µ (batch, feature_dim)
            labels: æ¨™ç±¤ (batch,)

        Returns:
            loss: å°æ¯”æå¤±å€¼
        """
        # L2æ­£è¦åŒ–
        features_a = F.normalize(features_a, p=2, dim=1)
        features_b = F.normalize(features_b, p=2, dim=1)

        # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
        similarity_matrix = torch.matmul(features_a, features_b.T) / self.temperature

        # å‰µå»ºæ­£æ¨£æœ¬é®ç½© (åŒé¡åˆ¥ç‚ºæ­£æ¨£æœ¬)
        labels_expanded_a = labels.unsqueeze(1).expand(-1, labels.size(0))
        labels_expanded_b = labels.unsqueeze(0).expand(labels.size(0), -1)
        positive_mask = (labels_expanded_a == labels_expanded_b).float()

        # å°è§’ç·šé®ç½© (æ’é™¤è‡ªå·±èˆ‡è‡ªå·±çš„ç›¸ä¼¼åº¦)
        eye_mask = torch.eye(labels.size(0), device=labels.device)
        positive_mask = positive_mask * (1 - eye_mask)

        # è¨ˆç®—InfoNCEæå¤±
        exp_sim = torch.exp(similarity_matrix)

        # æ­£æ¨£æœ¬çš„å°æ•¸ä¼¼ç„¶
        positive_sum = (positive_mask * exp_sim).sum(dim=1)

        # æ‰€æœ‰æ¨£æœ¬çš„å°æ•¸ä¼¼ç„¶
        all_sum = exp_sim.sum(dim=1)

        # é¿å…é™¤é›¶
        positive_sum = torch.clamp(positive_sum, min=1e-8)

        # è¨ˆç®—æå¤±
        loss = -torch.log(positive_sum / all_sum).mean()

        return loss


def test_fusion_mechanisms():
    """æ¸¬è©¦èåˆæ©Ÿåˆ¶"""
    print("ğŸ§ª æ¸¬è©¦è·¨æ¨¡æ…‹èåˆæ©Ÿåˆ¶...")

    batch_size = 4
    feature_dim = 512
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    visual_feat = torch.randn(batch_size, feature_dim).to(device)
    audio_feat = torch.randn(batch_size, feature_dim).to(device)
    text_feat = torch.randn(batch_size, feature_dim).to(device)
    labels = torch.randint(0, 30, (batch_size,)).to(device)

    # æ¸¬è©¦è·¨æ¨¡æ…‹æ³¨æ„åŠ›
    print("\n1. æ¸¬è©¦è·¨æ¨¡æ…‹æ³¨æ„åŠ›")
    cross_attention = CrossModalAttention().to(device)
    attended_feat, attn_weights = cross_attention(visual_feat, audio_feat)
    print(f"   è¼¸å…¥å½¢ç‹€: {visual_feat.shape}, {audio_feat.shape}")
    print(f"   è¼¸å‡ºå½¢ç‹€: {attended_feat.shape}")
    print(f"   æ³¨æ„åŠ›æ¬Šé‡å½¢ç‹€: {attn_weights.shape}")

    # æ¸¬è©¦æ¨¡æ…‹èåˆ
    print("\n2. æ¸¬è©¦æ¨¡æ…‹èåˆ")
    fusion_module = ModalFusion(fusion_strategy='attention').to(device)

    # æ¸¬è©¦ä¸‰æ¨¡æ…‹èåˆ
    fused_feat, attention_dict = fusion_module(
        visual_features=visual_feat,
        audio_features=audio_feat,
        text_features=text_feat,
        return_attention_weights=True
    )
    print(f"   ä¸‰æ¨¡æ…‹èåˆè¼¸å‡º: {fused_feat.shape}")
    print(f"   æ³¨æ„åŠ›æ¬Šé‡æ•¸é‡: {len(attention_dict)}")

    # æ¸¬è©¦é›™æ¨¡æ…‹èåˆ
    dual_fused = fusion_module(visual_features=visual_feat, audio_features=audio_feat)
    print(f"   é›™æ¨¡æ…‹èåˆè¼¸å‡º: {dual_fused.shape}")

    # æ¸¬è©¦å–®æ¨¡æ…‹
    single_fused = fusion_module(visual_features=visual_feat)
    print(f"   å–®æ¨¡æ…‹èåˆè¼¸å‡º: {single_fused.shape}")

    # æ¸¬è©¦å°æ¯”æå¤±
    print("\n3. æ¸¬è©¦å°æ¯”æå¤±")
    contrastive_loss = ContrastiveLoss().to(device)
    loss_value = contrastive_loss(visual_feat, audio_feat, labels)
    print(f"   å°æ¯”æå¤±å€¼: {loss_value.item():.4f}")

    # æ¸¬è©¦ä¸åŒèåˆç­–ç•¥
    print("\n4. æ¸¬è©¦ä¸åŒèåˆç­–ç•¥")
    for strategy in ['attention', 'concat', 'weighted_avg']:
        fusion = ModalFusion(fusion_strategy=strategy).to(device)
        output = fusion(visual_feat, audio_feat, text_feat)
        print(f"   {strategy:>12} ç­–ç•¥è¼¸å‡º: {output.shape}")

    print("\nâœ… èåˆæ©Ÿåˆ¶æ¸¬è©¦å®Œæˆ!")

    # è¨ˆç®—åƒæ•¸é‡
    def count_parameters(model):
        return sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"\nğŸ“Š èåˆæ¨¡çµ„åƒæ•¸çµ±è¨ˆ:")
    print(f"   è·¨æ¨¡æ…‹æ³¨æ„åŠ›: {count_parameters(cross_attention):,} åƒæ•¸")
    print(f"   æ¨¡æ…‹èåˆæ¨¡çµ„: {count_parameters(fusion_module):,} åƒæ•¸")


if __name__ == "__main__":
    test_fusion_mechanisms()