"""
Multi-Modal Encoders for Sign Language Recognition
å¤šæ¨¡æ…‹ç·¨ç¢¼å™¨å¯¦ä½œ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple


class PositionalEncoding(nn.Module):
    """
    ä½ç½®ç·¨ç¢¼æ¨¡çµ„
    ç‚ºåºåˆ—æ•¸æ“šæ·»åŠ ä½ç½®è³‡è¨Š
    """

    def __init__(self, d_model: int, max_seq_len: int = 200, dropout: float = 0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        # å‰µå»ºä½ç½®ç·¨ç¢¼çŸ©é™£
        pe = torch.zeros(max_seq_len, d_model)
        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                           (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)  # (max_seq_len, 1, d_model)

        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Tensor, shape [seq_len, batch_size, embedding_dim]
        """
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)


class VisualEncoder(nn.Module):
    """
    è¦–è¦ºç·¨ç¢¼å™¨
    è™•ç†æ‰‹èªè¦–è¦ºåºåˆ—ç‰¹å¾µ (MediaPipe landmarks)

    æ¶æ§‹:
    Input: (batch, 100, 417) - 100å¹€çš„417ç¶­MediaPipeç‰¹å¾µ
    â†“
    1D Conv: æ™‚åºç‰¹å¾µæå–
    â†“
    Transformer: é•·æœŸä¾è³´å»ºæ¨¡
    â†“
    Global Pool: å…¨åŸŸç‰¹å¾µèšåˆ
    â†“
    Output: (batch, 512) - çµ±ä¸€è¦–è¦ºè¡¨ç¤º
    """

    def __init__(
        self,
        input_dim: int = 417,
        hidden_dim: int = 256,
        output_dim: int = 512,
        num_transformer_layers: int = 4,
        num_attention_heads: int = 8,
        dropout: float = 0.1,
        conv_kernel_size: int = 5
    ):
        super().__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim

        # 1. æ™‚åºå·ç©å±¤ - æå–å±€éƒ¨æ™‚åºæ¨¡å¼
        self.temporal_conv = nn.Sequential(
            nn.Conv1d(input_dim, hidden_dim, kernel_size=conv_kernel_size, padding=conv_kernel_size//2),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout)
        )

        # 2. ä½ç½®ç·¨ç¢¼
        self.pos_encoding = PositionalEncoding(hidden_dim, max_seq_len=100, dropout=dropout)

        # 3. Transformerç·¨ç¢¼å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_attention_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=dropout,
            activation='relu',
            batch_first=False  # (seq_len, batch, feature)
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_transformer_layers)

        # 4. å…¨åŸŸæ± åŒ–
        self.global_pool = nn.AdaptiveAvgPool1d(1)

        # 5. è¼¸å‡ºæŠ•å½±
        self.output_projection = nn.Sequential(
            nn.Linear(hidden_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout)
        )

        # åˆå§‹åŒ–æ¬Šé‡
        self._init_weights()

    def _init_weights(self):
        """åˆå§‹åŒ–ç¶²è·¯æ¬Šé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘å‚³æ’­

        Args:
            x: è¦–è¦ºç‰¹å¾µåºåˆ— (batch, seq_len, feature_dim)
            mask: å¯é¸çš„æ³¨æ„åŠ›é®ç½© (batch, seq_len)

        Returns:
            encoded_features: ç·¨ç¢¼å¾Œçš„ç‰¹å¾µ (batch, output_dim)
        """
        batch_size, seq_len, feature_dim = x.shape

        # 1. æ™‚åºå·ç© - éœ€è¦ (batch, feature, seq_len) æ ¼å¼
        x_conv = x.transpose(1, 2)  # (batch, feature_dim, seq_len)
        x_conv = self.temporal_conv(x_conv)  # (batch, hidden_dim, seq_len)
        x_conv = x_conv.transpose(1, 2)  # (batch, seq_len, hidden_dim)

        # 2. æº–å‚™Transformerè¼¸å…¥ - éœ€è¦ (seq_len, batch, feature) æ ¼å¼
        x_transformer = x_conv.transpose(0, 1)  # (seq_len, batch, hidden_dim)

        # 3. æ·»åŠ ä½ç½®ç·¨ç¢¼
        x_transformer = self.pos_encoding(x_transformer)

        # 4. Transformerç·¨ç¢¼
        if mask is not None:
            # å‰µå»ºæ³¨æ„åŠ›é®ç½© (å¯é¸)
            attention_mask = self._create_attention_mask(mask)
            encoded = self.transformer(x_transformer, src_key_padding_mask=attention_mask)
        else:
            encoded = self.transformer(x_transformer)

        # 5. è½‰æ›å› (batch, seq_len, feature) æ ¼å¼
        encoded = encoded.transpose(0, 1)  # (batch, seq_len, hidden_dim)

        # 6. å…¨åŸŸæ± åŒ– - éœ€è¦ (batch, feature, seq_len) æ ¼å¼
        pooled = encoded.transpose(1, 2)  # (batch, hidden_dim, seq_len)
        pooled = self.global_pool(pooled).squeeze(-1)  # (batch, hidden_dim)

        # 7. è¼¸å‡ºæŠ•å½±
        output = self.output_projection(pooled)  # (batch, output_dim)

        return output

    def _create_attention_mask(self, mask: torch.Tensor) -> torch.Tensor:
        """å‰µå»ºTransformerçš„æ³¨æ„åŠ›é®ç½©"""
        # mask: (batch, seq_len) - Trueç‚ºæœ‰æ•ˆä½ç½®
        # è¿”å›: (batch, seq_len) - Trueç‚ºéœ€è¦é®è”½çš„ä½ç½®
        return ~mask


class AudioEncoder(nn.Module):
    """
    éŸ³è¨Šç·¨ç¢¼å™¨
    è™•ç†éŸ³è¨Šç‰¹å¾µ (MFCC + Spectral + Temporal)

    æ¶æ§‹:
    Input: (batch, 24) - 24ç¶­éŸ³è¨Šç‰¹å¾µ
    â†“
    Multi-layer MLP with BatchNorm and Dropout
    â†“
    Output: (batch, 512) - çµ±ä¸€éŸ³è¨Šè¡¨ç¤º
    """

    def __init__(
        self,
        input_dim: int = 24,
        hidden_dims: list = [128, 256],
        output_dim: int = 512,
        dropout: float = 0.3,
        use_batch_norm: bool = True
    ):
        super().__init__()

        self.input_dim = input_dim
        self.output_dim = output_dim

        # æ§‹å»ºå¤šå±¤æ„ŸçŸ¥å™¨
        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            if use_batch_norm:
                layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.ReLU(inplace=True))
            layers.append(nn.Dropout(dropout))
            prev_dim = hidden_dim

        # è¼¸å‡ºå±¤
        layers.append(nn.Linear(prev_dim, output_dim))

        self.encoder = nn.Sequential(*layers)

        # åˆå§‹åŒ–æ¬Šé‡
        self._init_weights()

    def _init_weights(self):
        """åˆå§‹åŒ–ç¶²è·¯æ¬Šé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘å‚³æ’­

        Args:
            x: éŸ³è¨Šç‰¹å¾µ (batch, input_dim)

        Returns:
            encoded_features: ç·¨ç¢¼å¾Œçš„ç‰¹å¾µ (batch, output_dim)
        """
        return self.encoder(x)


class TextEncoder(nn.Module):
    """
    æ–‡å­—ç·¨ç¢¼å™¨
    è™•ç†æ–‡å­—åµŒå…¥ç‰¹å¾µ (Word2Vec/FastText/BERT/Unified)

    æ¶æ§‹:
    Input: (batch, embedding_dim) - è©åµŒå…¥ç‰¹å¾µ
    â†“
    Layer Normalization + MLP
    â†“
    Output: (batch, 512) - çµ±ä¸€æ–‡å­—è¡¨ç¤º
    """

    def __init__(
        self,
        embedding_dims: dict = {
            'unified': 300,
            'word2vec': 300,
            'fasttext': 300,
            'bert': 768
        },
        output_dim: int = 512,
        hidden_dim: int = 256,
        dropout: float = 0.2,
        embedding_type: str = 'unified'
    ):
        super().__init__()

        self.embedding_dims = embedding_dims
        self.output_dim = output_dim
        self.embedding_type = embedding_type

        # ç‚ºæ¯ç¨®åµŒå…¥é¡å‹å‰µå»ºæŠ•å½±å±¤
        self.projections = nn.ModuleDict()
        for emb_type, emb_dim in embedding_dims.items():
            self.projections[emb_type] = nn.Sequential(
                nn.LayerNorm(emb_dim),
                nn.Linear(emb_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, output_dim)
            )

        # åˆå§‹åŒ–æ¬Šé‡
        self._init_weights()

    def _init_weights(self):
        """åˆå§‹åŒ–ç¶²è·¯æ¬Šé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(
        self,
        x: torch.Tensor,
        embedding_type: Optional[str] = None
    ) -> torch.Tensor:
        """
        å‰å‘å‚³æ’­

        Args:
            x: æ–‡å­—åµŒå…¥ç‰¹å¾µ (batch, embedding_dim)
            embedding_type: æŒ‡å®šåµŒå…¥é¡å‹ï¼Œå¦‚æœç‚ºNoneå‰‡ä½¿ç”¨é è¨­é¡å‹

        Returns:
            encoded_features: ç·¨ç¢¼å¾Œçš„ç‰¹å¾µ (batch, output_dim)
        """
        if embedding_type is None:
            embedding_type = self.embedding_type

        if embedding_type not in self.projections:
            raise ValueError(f"ä¸æ”¯æ´çš„åµŒå…¥é¡å‹: {embedding_type}")

        return self.projections[embedding_type](x)

    def set_embedding_type(self, embedding_type: str):
        """è¨­å®šé è¨­åµŒå…¥é¡å‹"""
        if embedding_type not in self.embedding_dims:
            raise ValueError(f"ä¸æ”¯æ´çš„åµŒå…¥é¡å‹: {embedding_type}")
        self.embedding_type = embedding_type


class AdaptivePooling1D(nn.Module):
    """
    è‡ªé©æ‡‰æ± åŒ–å±¤
    å¯ä»¥è™•ç†ä¸åŒé•·åº¦çš„åºåˆ—
    """

    def __init__(self, output_size: int = 1, pooling_type: str = 'avg'):
        super().__init__()
        self.output_size = output_size
        self.pooling_type = pooling_type

        if pooling_type == 'avg':
            self.pool = nn.AdaptiveAvgPool1d(output_size)
        elif pooling_type == 'max':
            self.pool = nn.AdaptiveMaxPool1d(output_size)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æ± åŒ–é¡å‹: {pooling_type}")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (batch, feature, seq_len)
        Returns:
            pooled: (batch, feature, output_size)
        """
        return self.pool(x)


def test_encoders():
    """æ¸¬è©¦ç·¨ç¢¼å™¨åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦ç·¨ç¢¼å™¨æ¨¡çµ„...")

    batch_size = 4
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # æ¸¬è©¦è¦–è¦ºç·¨ç¢¼å™¨
    print("\n1. æ¸¬è©¦è¦–è¦ºç·¨ç¢¼å™¨")
    visual_encoder = VisualEncoder().to(device)
    visual_input = torch.randn(batch_size, 100, 417).to(device)

    visual_output = visual_encoder(visual_input)
    print(f"   è¼¸å…¥å½¢ç‹€: {visual_input.shape}")
    print(f"   è¼¸å‡ºå½¢ç‹€: {visual_output.shape}")
    assert visual_output.shape == (batch_size, 512), "è¦–è¦ºç·¨ç¢¼å™¨è¼¸å‡ºå½¢ç‹€éŒ¯èª¤"

    # æ¸¬è©¦éŸ³è¨Šç·¨ç¢¼å™¨
    print("\n2. æ¸¬è©¦éŸ³è¨Šç·¨ç¢¼å™¨")
    audio_encoder = AudioEncoder().to(device)
    audio_input = torch.randn(batch_size, 24).to(device)

    audio_output = audio_encoder(audio_input)
    print(f"   è¼¸å…¥å½¢ç‹€: {audio_input.shape}")
    print(f"   è¼¸å‡ºå½¢ç‹€: {audio_output.shape}")
    assert audio_output.shape == (batch_size, 512), "éŸ³è¨Šç·¨ç¢¼å™¨è¼¸å‡ºå½¢ç‹€éŒ¯èª¤"

    # æ¸¬è©¦æ–‡å­—ç·¨ç¢¼å™¨
    print("\n3. æ¸¬è©¦æ–‡å­—ç·¨ç¢¼å™¨")
    text_encoder = TextEncoder().to(device)
    text_input = torch.randn(batch_size, 300).to(device)  # unifiedåµŒå…¥

    text_output = text_encoder(text_input)
    print(f"   è¼¸å…¥å½¢ç‹€: {text_input.shape}")
    print(f"   è¼¸å‡ºå½¢ç‹€: {text_output.shape}")
    assert text_output.shape == (batch_size, 512), "æ–‡å­—ç·¨ç¢¼å™¨è¼¸å‡ºå½¢ç‹€éŒ¯èª¤"

    # æ¸¬è©¦ä¸åŒåµŒå…¥é¡å‹
    bert_input = torch.randn(batch_size, 768).to(device)
    bert_output = text_encoder(bert_input, embedding_type='bert')
    print(f"   BERTè¼¸å…¥å½¢ç‹€: {bert_input.shape}")
    print(f"   BERTè¼¸å‡ºå½¢ç‹€: {bert_output.shape}")
    assert bert_output.shape == (batch_size, 512), "BERTç·¨ç¢¼å™¨è¼¸å‡ºå½¢ç‹€éŒ¯èª¤"

    print("\nâœ… æ‰€æœ‰ç·¨ç¢¼å™¨æ¸¬è©¦é€šé!")

    # è¨ˆç®—åƒæ•¸é‡
    def count_parameters(model):
        return sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"\nğŸ“Š æ¨¡å‹åƒæ•¸çµ±è¨ˆ:")
    print(f"   è¦–è¦ºç·¨ç¢¼å™¨: {count_parameters(visual_encoder):,} åƒæ•¸")
    print(f"   éŸ³è¨Šç·¨ç¢¼å™¨: {count_parameters(audio_encoder):,} åƒæ•¸")
    print(f"   æ–‡å­—ç·¨ç¢¼å™¨: {count_parameters(text_encoder):,} åƒæ•¸")
    total_params = count_parameters(visual_encoder) + count_parameters(audio_encoder) + count_parameters(text_encoder)
    print(f"   ç¸½è¨ˆ: {total_params:,} åƒæ•¸")


if __name__ == "__main__":
    test_encoders()