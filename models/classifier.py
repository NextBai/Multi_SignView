"""
Multi-Modal Sign Language Classifier
å¤šæ¨¡æ…‹æ‰‹èªåˆ†é¡å™¨ä¸»æ¨¡çµ„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union

from .encoders import VisualEncoder, AudioEncoder, TextEncoder
from .fusion import ModalFusion, ContrastiveLoss


class MultiModalSignClassifier(nn.Module):
    """
    å®Œæ•´çš„å¤šæ¨¡æ…‹æ‰‹èªåˆ†é¡å™¨

    ç‰¹é»:
    âœ… éˆæ´»æ¨¡æ…‹çµ„åˆ: æ”¯æ´1-3å€‹æ¨¡æ…‹ä»»æ„çµ„åˆæ¨ç†
    âœ… çµ±ä¸€ç‰¹å¾µç©ºé–“: æ‰€æœ‰æ¨¡æ…‹æ˜ å°„åˆ°512ç¶­
    âœ… æ³¨æ„åŠ›æ©Ÿåˆ¶: è‡ªå‹•å­¸ç¿’æ¨¡æ…‹é‡è¦æ€§æ¬Šé‡
    âœ… ç«¯åˆ°ç«¯è¨“ç·´: å¾åŸå§‹ç‰¹å¾µåˆ°åˆ†é¡çµæœ

    åƒæ•¸é‡ä¼°ç®—:
    - Visual Encoder: ~3.8M
    - Audio Encoder: ~0.17M
    - Text Encoder: ~0.96M
    - Fusion Module: ~1.9M
    - Classifier: ~0.1M
    ç¸½è¨ˆ: ~7M parameters
    """

    def __init__(
        self,
        num_classes: int = 30,
        modalities: List[str] = ['visual', 'audio', 'text'],
        fusion_strategy: str = 'attention',
        text_embedding_type: str = 'unified',
        dropout: float = 0.1,
        use_contrastive_loss: bool = True
    ):
        super().__init__()

        self.num_classes = num_classes
        self.modalities = modalities
        self.text_embedding_type = text_embedding_type
        self.use_contrastive_loss = use_contrastive_loss

        # æ¨¡æ…‹ç·¨ç¢¼å™¨
        self.encoders = nn.ModuleDict()

        if 'visual' in modalities:
            self.encoders['visual'] = VisualEncoder(
                input_dim=417,
                hidden_dim=256,
                output_dim=512,
                dropout=dropout
            )

        if 'audio' in modalities:
            self.encoders['audio'] = AudioEncoder(
                input_dim=24,
                hidden_dims=[128, 256],
                output_dim=512,
                dropout=dropout
            )

        if 'text' in modalities:
            self.encoders['text'] = TextEncoder(
                embedding_type=text_embedding_type,
                output_dim=512,
                dropout=dropout
            )

        # è·¨æ¨¡æ…‹èåˆæ¨¡çµ„
        self.fusion = ModalFusion(
            feature_dim=512,
            fusion_strategy=fusion_strategy,
            dropout=dropout
        )

        # åˆ†é¡é ­
        self.classifier = nn.Sequential(
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout/2),
            nn.Linear(128, num_classes)
        )

        # å°æ¯”å­¸ç¿’æå¤±
        if use_contrastive_loss:
            self.contrastive_loss = ContrastiveLoss()

        # åˆå§‹åŒ–æ¬Šé‡
        self._init_weights()

    def _init_weights(self):
        """åˆå§‹åŒ–ç¶²è·¯æ¬Šé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(
        self,
        features: Dict[str, torch.Tensor],
        return_embeddings: bool = False,
        return_attention_weights: bool = False
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, ...]]:
        """
        å‰å‘å‚³æ’­

        Args:
            features: ç‰¹å¾µå­—å…¸ï¼Œå¯åŒ…å« 'visual', 'audio', 'text'
            return_embeddings: æ˜¯å¦è¿”å›å„æ¨¡æ…‹ç·¨ç¢¼å¾Œçš„åµŒå…¥
            return_attention_weights: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ¬Šé‡

        Returns:
            logits: åˆ†é¡logits (batch, num_classes)
            embeddings: å„æ¨¡æ…‹åµŒå…¥å­—å…¸ (å¯é¸)
            attention_weights: æ³¨æ„åŠ›æ¬Šé‡å­—å…¸ (å¯é¸)
        """
        # 1. å„æ¨¡æ…‹ç·¨ç¢¼
        modal_embeddings = {}

        if 'visual' in features and 'visual' in self.encoders:
            modal_embeddings['visual'] = self.encoders['visual'](features['visual'])

        if 'audio' in features and 'audio' in self.encoders:
            modal_embeddings['audio'] = self.encoders['audio'](features['audio'])

        if 'text' in features and 'text' in self.encoders:
            modal_embeddings['text'] = self.encoders['text'](features['text'])

        # 2. è·¨æ¨¡æ…‹èåˆ
        if return_attention_weights:
            fused_features, attention_weights = self.fusion(
                visual_features=modal_embeddings.get('visual'),
                audio_features=modal_embeddings.get('audio'),
                text_features=modal_embeddings.get('text'),
                return_attention_weights=True
            )
        else:
            fused_features = self.fusion(
                visual_features=modal_embeddings.get('visual'),
                audio_features=modal_embeddings.get('audio'),
                text_features=modal_embeddings.get('text')
            )

        # 3. åˆ†é¡é æ¸¬
        logits = self.classifier(fused_features)

        # 4. æ§‹å»ºè¿”å›å€¼
        outputs = [logits]

        if return_embeddings:
            outputs.append(modal_embeddings)

        if return_attention_weights:
            outputs.append(attention_weights)

        if len(outputs) == 1:
            return outputs[0]
        else:
            return tuple(outputs)

    def compute_contrastive_loss(
        self,
        modal_embeddings: Dict[str, torch.Tensor],
        labels: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """
        è¨ˆç®—è·¨æ¨¡æ…‹å°æ¯”æå¤±

        Args:
            modal_embeddings: å„æ¨¡æ…‹åµŒå…¥å­—å…¸
            labels: æ¨™ç±¤ (batch,)

        Returns:
            losses: å°æ¯”æå¤±å­—å…¸
        """
        if not self.use_contrastive_loss:
            return {}

        contrastive_losses = {}
        modality_names = list(modal_embeddings.keys())

        # è¨ˆç®—æ‰€æœ‰æ¨¡æ…‹å°çš„å°æ¯”æå¤±
        for i in range(len(modality_names)):
            for j in range(i + 1, len(modality_names)):
                modal_a = modality_names[i]
                modal_b = modality_names[j]

                loss_key = f"{modal_a}_{modal_b}_contrastive"
                contrastive_losses[loss_key] = self.contrastive_loss(
                    modal_embeddings[modal_a],
                    modal_embeddings[modal_b],
                    labels
                )

        return contrastive_losses

    def set_modalities(self, modalities: List[str]):
        """å‹•æ…‹è¨­å®šä½¿ç”¨çš„æ¨¡æ…‹"""
        self.modalities = modalities

    def freeze_encoders(self, modalities: Optional[List[str]] = None):
        """å‡çµæŒ‡å®šæ¨¡æ…‹çš„ç·¨ç¢¼å™¨åƒæ•¸"""
        if modalities is None:
            modalities = self.modalities

        for modality in modalities:
            if modality in self.encoders:
                for param in self.encoders[modality].parameters():
                    param.requires_grad = False

    def unfreeze_encoders(self, modalities: Optional[List[str]] = None):
        """è§£å‡æŒ‡å®šæ¨¡æ…‹çš„ç·¨ç¢¼å™¨åƒæ•¸"""
        if modalities is None:
            modalities = self.modalities

        for modality in modalities:
            if modality in self.encoders:
                for param in self.encoders[modality].parameters():
                    param.requires_grad = True

    def get_model_info(self) -> Dict[str, Union[int, str, List]]:
        """ç²å–æ¨¡å‹è³‡è¨Š"""
        def count_parameters(model):
            return sum(p.numel() for p in model.parameters() if p.requires_grad)

        info = {
            'num_classes': self.num_classes,
            'modalities': self.modalities,
            'text_embedding_type': self.text_embedding_type,
            'total_parameters': count_parameters(self),
            'encoder_parameters': {},
            'fusion_parameters': count_parameters(self.fusion),
            'classifier_parameters': count_parameters(self.classifier)
        }

        for modality, encoder in self.encoders.items():
            info['encoder_parameters'][modality] = count_parameters(encoder)

        return info


class EnsembleClassifier(nn.Module):
    """
    é›†æˆåˆ†é¡å™¨
    çµåˆå¤šå€‹ä¸åŒé…ç½®çš„åˆ†é¡å™¨é€²è¡Œé æ¸¬
    """

    def __init__(self, classifiers: List[MultiModalSignClassifier], voting_strategy: str = 'soft'):
        super().__init__()

        self.classifiers = nn.ModuleList(classifiers)
        self.voting_strategy = voting_strategy
        self.num_classes = classifiers[0].num_classes

        # é©—è­‰æ‰€æœ‰åˆ†é¡å™¨çš„é¡åˆ¥æ•¸ä¸€è‡´
        for classifier in classifiers:
            assert classifier.num_classes == self.num_classes

    def forward(self, features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        é›†æˆå‰å‘å‚³æ’­

        Args:
            features: ç‰¹å¾µå­—å…¸

        Returns:
            ensemble_logits: é›†æˆå¾Œçš„åˆ†é¡logits
        """
        predictions = []

        for classifier in self.classifiers:
            logits = classifier(features)
            if self.voting_strategy == 'soft':
                predictions.append(F.softmax(logits, dim=-1))
            else:  # hard voting
                predictions.append(logits)

        # å¹³å‡é›†æˆ
        if self.voting_strategy == 'soft':
            ensemble_probs = torch.stack(predictions, dim=0).mean(dim=0)
            ensemble_logits = torch.log(ensemble_probs + 1e-8)  # è½‰å›logits
        else:
            ensemble_logits = torch.stack(predictions, dim=0).mean(dim=0)

        return ensemble_logits


def create_multimodal_classifier(
    num_classes: int = 30,
    modalities: List[str] = ['visual', 'audio', 'text'],
    fusion_strategy: str = 'attention',
    text_embedding_type: str = 'unified',
    pretrained_encoders: Optional[Dict[str, str]] = None
) -> MultiModalSignClassifier:
    """
    å‰µå»ºå¤šæ¨¡æ…‹åˆ†é¡å™¨çš„å·¥å» å‡½æ•¸

    Args:
        num_classes: åˆ†é¡é¡åˆ¥æ•¸
        modalities: ä½¿ç”¨çš„æ¨¡æ…‹åˆ—è¡¨
        fusion_strategy: èåˆç­–ç•¥
        text_embedding_type: æ–‡å­—åµŒå…¥é¡å‹
        pretrained_encoders: é è¨“ç·´ç·¨ç¢¼å™¨æ¬Šé‡è·¯å¾‘å­—å…¸

    Returns:
        model: å¤šæ¨¡æ…‹åˆ†é¡å™¨å¯¦ä¾‹
    """
    model = MultiModalSignClassifier(
        num_classes=num_classes,
        modalities=modalities,
        fusion_strategy=fusion_strategy,
        text_embedding_type=text_embedding_type
    )

    # è¼‰å…¥é è¨“ç·´ç·¨ç¢¼å™¨æ¬Šé‡
    if pretrained_encoders:
        for modality, weight_path in pretrained_encoders.items():
            if modality in model.encoders:
                try:
                    state_dict = torch.load(weight_path, map_location='cpu')
                    model.encoders[modality].load_state_dict(state_dict)
                    print(f"âœ… è¼‰å…¥ {modality} ç·¨ç¢¼å™¨é è¨“ç·´æ¬Šé‡: {weight_path}")
                except Exception as e:
                    print(f"âš ï¸  è¼‰å…¥ {modality} ç·¨ç¢¼å™¨æ¬Šé‡å¤±æ•—: {e}")

    return model


def test_classifier():
    """æ¸¬è©¦å®Œæ•´åˆ†é¡å™¨"""
    print("ğŸ§ª æ¸¬è©¦å¤šæ¨¡æ…‹åˆ†é¡å™¨...")

    batch_size = 4
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    test_features = {
        'visual': torch.randn(batch_size, 100, 417).to(device),
        'audio': torch.randn(batch_size, 24).to(device),
        'text': torch.randn(batch_size, 300).to(device)
    }
    labels = torch.randint(0, 30, (batch_size,)).to(device)

    # æ¸¬è©¦å®Œæ•´åˆ†é¡å™¨
    print("\n1. æ¸¬è©¦å®Œæ•´ä¸‰æ¨¡æ…‹åˆ†é¡å™¨")
    model = MultiModalSignClassifier().to(device)

    # åŸºç¤å‰å‘å‚³æ’­
    logits = model(test_features)
    print(f"   è¼¸å‡ºlogitså½¢ç‹€: {logits.shape}")
    assert logits.shape == (batch_size, 30), "åˆ†é¡å™¨è¼¸å‡ºå½¢ç‹€éŒ¯èª¤"

    # å¸¶åµŒå…¥å’Œæ³¨æ„åŠ›æ¬Šé‡çš„å‰å‘å‚³æ’­
    logits, embeddings, attention_weights = model(
        test_features,
        return_embeddings=True,
        return_attention_weights=True
    )
    print(f"   åµŒå…¥æ•¸é‡: {len(embeddings)}")
    print(f"   æ³¨æ„åŠ›æ¬Šé‡æ•¸é‡: {len(attention_weights)}")

    # æ¸¬è©¦å°æ¯”æå¤±
    contrastive_losses = model.compute_contrastive_loss(embeddings, labels)
    print(f"   å°æ¯”æå¤±æ•¸é‡: {len(contrastive_losses)}")

    # æ¸¬è©¦ä¸åŒæ¨¡æ…‹çµ„åˆ
    print("\n2. æ¸¬è©¦ä¸åŒæ¨¡æ…‹çµ„åˆ")
    for modalities in [['visual'], ['audio'], ['text'], ['visual', 'audio'], ['visual', 'text'], ['audio', 'text']]:
        subset_model = MultiModalSignClassifier(modalities=modalities).to(device)
        subset_features = {k: v for k, v in test_features.items() if k in modalities}
        subset_logits = subset_model(subset_features)
        print(f"   {'+'.join(modalities):>15} æ¨¡æ…‹: {subset_logits.shape}")

    # æ¸¬è©¦æ¨¡å‹è³‡è¨Š
    print("\n3. æ¨¡å‹è³‡è¨Š")
    model_info = model.get_model_info()
    print(f"   ç¸½åƒæ•¸æ•¸é‡: {model_info['total_parameters']:,}")
    print(f"   èåˆæ¨¡çµ„åƒæ•¸: {model_info['fusion_parameters']:,}")
    print(f"   åˆ†é¡å™¨åƒæ•¸: {model_info['classifier_parameters']:,}")
    for modality, params in model_info['encoder_parameters'].items():
        print(f"   {modality} ç·¨ç¢¼å™¨åƒæ•¸: {params:,}")

    # æ¸¬è©¦é›†æˆåˆ†é¡å™¨
    print("\n4. æ¸¬è©¦é›†æˆåˆ†é¡å™¨")
    classifiers = [
        MultiModalSignClassifier(fusion_strategy='attention').to(device),
        MultiModalSignClassifier(fusion_strategy='concat').to(device),
        MultiModalSignClassifier(fusion_strategy='weighted_avg').to(device)
    ]
    ensemble = EnsembleClassifier(classifiers).to(device)
    ensemble_logits = ensemble(test_features)
    print(f"   é›†æˆè¼¸å‡ºå½¢ç‹€: {ensemble_logits.shape}")

    print("\nâœ… åˆ†é¡å™¨æ¸¬è©¦å®Œæˆ!")


if __name__ == "__main__":
    test_classifier()