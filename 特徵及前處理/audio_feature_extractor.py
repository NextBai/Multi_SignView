#!/usr/bin/env python3
"""
è©å½™èªéŸ³ç‰¹å¾µæå–å™¨ - Multi_SignView å¤šæ¨¡æ…‹æ‰‹èªè¾¨è­˜ç³»çµ±
å°ˆç‚º30å€‹æ‰‹èªè©å½™å°æ‡‰çš„è‹±æ–‡å–®è©èªéŸ³é€²è¡Œç‰¹å¾µæå–
æ”¯æ´å¤šæ¨£åŒ–èªéŸ³ä¸‹è¼‰ã€MFCC/Spectralç‰¹å¾µæå–ã€å¤šæ¨¡æ…‹èåˆ

æŠ€è¡“ç‰¹é»ï¼š
- è‡ªå‹•ä¸‹è¼‰30å€‹è‹±æ–‡å–®è©çš„å¤šæ¨£åŒ–èªéŸ³éŸ³æª”ï¼ˆä¸åŒå£éŸ³ã€èªé€Ÿã€æ€§åˆ¥ï¼‰
- MFCC(13) + Spectral(7) + Temporal(4) = 24ç¶­èªéŸ³ç‰¹å¾µ
- æ”¯æ´ TTS èªéŸ³ç”Ÿæˆå’Œç·šä¸ŠèªéŸ³è³‡æºä¸‹è¼‰
- èªéŸ³å“è³ªè©•ä¼°å’Œç¯©é¸æ©Ÿåˆ¶
- å¤šèªéŸ³ç‰ˆæœ¬ç‰¹å¾µèåˆç­–ç•¥

è©å½™åˆ—è¡¨ï¼š
again, bird, book, computer, cousin, deaf, drink, eat, finish, fish,
friend, good, happy, learn, like, mother, need, nice, no, orange,
school, sister, student, table, teacher, tired, want, what, white, yes

Author: Claude Code + Multi_SignView Team
Date: 2024
"""

import os
import sys
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
import time
from tqdm import tqdm
import json
import numpy as np
import warnings
from typing import Dict, List, Optional, Tuple, Union
import hashlib

# éŸ³è¨Šè™•ç†æ ¸å¿ƒä¾è³´
import librosa
import soundfile as sf
from scipy.signal import butter, filtfilt
from scipy.stats import zscore

# TTS å’ŒèªéŸ³ä¸‹è¼‰ä¾è³´
import requests
import urllib.request
from urllib.parse import quote
from gtts import gTTS
from pydub import AudioSegment
from pydub.generators import Sine
import tempfile
import shutil

# é™åˆ¶å¤šåŸ·è¡Œç·’ï¼Œé¿å…èˆ‡å¤šé€²ç¨‹äº’æ¶ CPU
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")

# å°ˆæ¡ˆè·¯å¾‘é…ç½®
PROJECT_ROOT = Path(__file__).parent
AUDIO_DATA_ROOT = PROJECT_ROOT / "vocabulary_audio"  # è©å½™èªéŸ³å­˜æ”¾ç›®éŒ„
OUTPUT_ROOT = PROJECT_ROOT / "features" / "audio_features"
NUM_WORKERS = 2  # ä¿å®ˆè¨­å®šï¼Œé¿å…è³‡æºç«¶çˆ­
TARGET_SAMPLE_RATE = 16000  # æ¨™æº–åŒ–æ¡æ¨£ç‡

# 30å€‹æ‰‹èªè©å½™åˆ—è¡¨
SIGN_LANGUAGE_VOCABULARY = [
    'again', 'bird', 'book', 'computer', 'cousin', 'deaf', 'drink', 'eat',
    'finish', 'fish', 'friend', 'good', 'happy', 'learn', 'like', 'mother',
    'need', 'nice', 'no', 'orange', 'school', 'sister', 'student', 'table',
    'teacher', 'tired', 'want', 'what', 'white', 'yes'
]

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)


class VocabularyAudioDownloader:
    """
    è©å½™èªéŸ³ä¸‹è¼‰å™¨

    æ”¯æ´å¤šç¨®èªéŸ³ä¾†æºçš„è‡ªå‹•ä¸‹è¼‰å’Œç®¡ç†
    """

    def __init__(self, vocabulary: List[str] = SIGN_LANGUAGE_VOCABULARY,
                 audio_root: Path = AUDIO_DATA_ROOT):
        """
        åˆå§‹åŒ–èªéŸ³ä¸‹è¼‰å™¨

        Args:
            vocabulary: è©å½™åˆ—è¡¨
            audio_root: èªéŸ³å­˜æ”¾æ ¹ç›®éŒ„
        """
        self.vocabulary = vocabulary
        self.audio_root = Path(audio_root)
        self.audio_root.mkdir(parents=True, exist_ok=True)

        # TTS é…ç½®
        self.tts_languages = {
            'en-us': 'American English',
            'en-gb': 'British English',
            'en-au': 'Australian English',
            'en-ca': 'Canadian English'
        }

        print(f"ğŸ¤ VocabularyAudioDownloader åˆå§‹åŒ–å®Œæˆ")
        print(f"   - è©å½™æ•¸é‡: {len(vocabulary)}")
        print(f"   - èªéŸ³å­˜æ”¾ç›®éŒ„: {self.audio_root}")

    def download_tts_audio(self, word: str, lang: str = 'en',
                          tld: str = 'com', slow: bool = False) -> Optional[Path]:
        """
        ä½¿ç”¨ gTTS ä¸‹è¼‰å–®è©èªéŸ³

        Args:
            word: å–®è©
            lang: èªè¨€ä»£ç¢¼
            tld: é ‚ç´šåŸŸåï¼ˆå½±éŸ¿å£éŸ³ï¼‰
            slow: æ˜¯å¦æ…¢é€Ÿç™¼éŸ³

        Returns:
            èªéŸ³æª”æ¡ˆè·¯å¾‘æˆ– None
        """
        try:
            # å»ºç«‹è©å½™ç›®éŒ„
            word_dir = self.audio_root / word
            word_dir.mkdir(exist_ok=True)

            # æª”æ¡ˆå‘½å
            speed_suffix = "_slow" if slow else "_normal"
            filename = f"{word}_{lang}_{tld}{speed_suffix}.wav"
            output_path = word_dir / filename

            if output_path.exists():
                return output_path

            # ç”Ÿæˆ TTS èªéŸ³
            tts = gTTS(text=word, lang=lang, tld=tld, slow=slow)

            # æš«å­˜ mp3 æª”æ¡ˆ
            with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as tmp_file:
                tts.save(tmp_file.name)

                # è½‰æ›ç‚º wav æ ¼å¼
                audio = AudioSegment.from_mp3(tmp_file.name)
                audio = audio.set_frame_rate(TARGET_SAMPLE_RATE)
                audio = audio.set_channels(1)  # å–®è²é“
                audio.export(str(output_path), format="wav")

                # æ¸…ç†æš«å­˜æª”
                os.unlink(tmp_file.name)

            print(f"âœ… TTS èªéŸ³ä¸‹è¼‰å®Œæˆ: {filename}")
            return output_path

        except Exception as e:
            print(f"âŒ TTS èªéŸ³ä¸‹è¼‰å¤±æ•— {word} ({lang}_{tld}): {str(e)}")
            return None

    def download_forvo_audio(self, word: str) -> List[Path]:
        """
        å¾ Forvo ä¸‹è¼‰çœŸäººç™¼éŸ³ï¼ˆéœ€è¦ç¶²è·¯é€£ç·šï¼‰

        Args:
            word: å–®è©

        Returns:
            ä¸‹è¼‰çš„èªéŸ³æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
        """
        try:
            word_dir = self.audio_root / word
            word_dir.mkdir(exist_ok=True)

            # Forvo API æˆ–ç¶²é çˆ¬å–ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
            # é€™è£¡å¯¦ä½œåŸºæœ¬çš„ä¸‹è¼‰é‚è¼¯
            downloaded_files = []

            # æ¨¡æ“¬ä¸åŒç™¼éŸ³ç‰ˆæœ¬çš„ä¸‹è¼‰
            for i, accent in enumerate(['us', 'uk', 'au']):
                filename = f"{word}_forvo_{accent}.wav"
                output_path = word_dir / filename

                if not output_path.exists():
                    # é€™è£¡æ‡‰è©²å¯¦ä½œçœŸæ­£çš„ Forvo API å‘¼å«
                    # æš«æ™‚è·³é
                    continue

                downloaded_files.append(output_path)

            return downloaded_files

        except Exception as e:
            print(f"âŒ Forvo èªéŸ³ä¸‹è¼‰å¤±æ•— {word}: {str(e)}")
            return []

    def download_all_vocabulary_audio(self, max_versions_per_word: int = 4) -> Dict[str, List[Path]]:
        """
        ä¸‹è¼‰æ‰€æœ‰è©å½™çš„å¤šæ¨£åŒ–èªéŸ³

        Args:
            max_versions_per_word: æ¯å€‹å–®è©æœ€å¤šä¸‹è¼‰çš„èªéŸ³ç‰ˆæœ¬æ•¸

        Returns:
            {è©å½™: [èªéŸ³æª”æ¡ˆè·¯å¾‘åˆ—è¡¨]} å­—å…¸
        """
        print(f"ğŸ¤ é–‹å§‹ä¸‹è¼‰ {len(self.vocabulary)} å€‹è©å½™çš„èªéŸ³...")

        vocabulary_audio = {}

        for word in tqdm(self.vocabulary, desc="ä¸‹è¼‰è©å½™èªéŸ³"):
            audio_files = []

            # 1. ä¸‹è¼‰ gTTS å¤šç¨®å£éŸ³ç‰ˆæœ¬
            tts_configs = [
                ('en', 'com', False),    # ç¾å¼è‹±èªï¼Œæ­£å¸¸èªé€Ÿ
                ('en', 'co.uk', False),  # è‹±å¼è‹±èªï¼Œæ­£å¸¸èªé€Ÿ
                ('en', 'com.au', False), # æ¾³å¼è‹±èªï¼Œæ­£å¸¸èªé€Ÿ
                ('en', 'com', True),     # ç¾å¼è‹±èªï¼Œæ…¢é€Ÿ
            ]

            for lang, tld, slow in tts_configs[:max_versions_per_word]:
                audio_file = self.download_tts_audio(word, lang, tld, slow)
                if audio_file:
                    audio_files.append(audio_file)

            # 2. å˜—è©¦ä¸‹è¼‰ Forvo çœŸäººç™¼éŸ³ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            # forvo_files = self.download_forvo_audio(word)
            # audio_files.extend(forvo_files)

            vocabulary_audio[word] = audio_files

            if audio_files:
                print(f"   ğŸ“ {word}: {len(audio_files)} å€‹èªéŸ³ç‰ˆæœ¬")
            else:
                print(f"   âŒ {word}: æ²’æœ‰ä¸‹è¼‰åˆ°èªéŸ³")

        total_files = sum(len(files) for files in vocabulary_audio.values())
        print(f"ğŸ¤ èªéŸ³ä¸‹è¼‰å®Œæˆï¼ç¸½å…± {total_files} å€‹èªéŸ³æª”æ¡ˆ")

        return vocabulary_audio

    def evaluate_audio_quality(self, audio_path: Path) -> Dict[str, float]:
        """
        è©•ä¼°èªéŸ³æª”æ¡ˆå“è³ª

        Args:
            audio_path: èªéŸ³æª”æ¡ˆè·¯å¾‘

        Returns:
            å“è³ªè©•ä¼°æŒ‡æ¨™å­—å…¸
        """
        try:
            audio, sr = librosa.load(str(audio_path), sr=TARGET_SAMPLE_RATE)

            # å“è³ªè©•ä¼°æŒ‡æ¨™
            duration = len(audio) / sr
            rms_energy = np.sqrt(np.mean(audio**2))
            zcr = np.mean(librosa.feature.zero_crossing_rate(audio)[0])
            spectral_centroid = np.mean(librosa.feature.spectral_centroid(audio, sr=sr)[0])

            quality_metrics = {
                'duration': duration,
                'rms_energy': rms_energy,
                'zero_crossing_rate': zcr,
                'spectral_centroid': spectral_centroid,
                'snr_estimate': rms_energy / (zcr + 1e-8)  # ç°¡å–®çš„ä¿¡å™ªæ¯”ä¼°è¨ˆ
            }

            return quality_metrics

        except Exception as e:
            print(f"âŒ èªéŸ³å“è³ªè©•ä¼°å¤±æ•— {audio_path}: {str(e)}")
            return {}


class AudioFeatureExtractor:
    """
    å¤šç¶­éŸ³è¨Šç‰¹å¾µæå–å™¨

    æ•´åˆ MFCCã€Chromaã€Spectralã€Temporal å››å¤§é¡éŸ³è¨Šç‰¹å¾µ
    æ”¯æ´æ™‚åºå°é½Šã€æ‰¹æ¬¡è™•ç†ã€å¤šé€²ç¨‹åŠ é€Ÿ
    """

    def __init__(self,
                 sample_rate: int = TARGET_SAMPLE_RATE,
                 n_mfcc: int = 13,
                 window_size: int = 2048,
                 hop_length: int = 512):
        """
        åˆå§‹åŒ–éŸ³è¨Šç‰¹å¾µæå–å™¨

        Args:
            sample_rate: ç›®æ¨™æ¡æ¨£ç‡ (Hz)
            n_mfcc: MFCC ä¿‚æ•¸æ•¸é‡
            window_size: FFT çª—å£å¤§å°
            hop_length: è·³èºé•·åº¦
        """
        self.sample_rate = sample_rate
        self.n_mfcc = n_mfcc
        self.window_size = window_size
        self.hop_length = hop_length

        # ç‰¹å¾µç¶­åº¦é…ç½®ï¼ˆç§»é™¤ Chromaï¼Œå°ˆæ³¨æ–¼èªéŸ³ç‰¹å¾µï¼‰
        self.feature_dims = {
            'mfcc': n_mfcc,           # 13ç¶­ MFCC ä¿‚æ•¸
            'spectral': 7,            # 7ç¶­ Spectral ç‰¹å¾µ
            'temporal': 4             # 4ç¶­ Temporal ç‰¹å¾µ
        }
        self.total_dims = sum(self.feature_dims.values())  # ç¸½è¨ˆ24ç¶­

        print(f"ğŸµ AudioFeatureExtractor åˆå§‹åŒ–å®Œæˆ")
        print(f"   - æ¡æ¨£ç‡: {self.sample_rate} Hz")
        print(f"   - ç‰¹å¾µç¶­åº¦: MFCC({n_mfcc}) + Spectral(7) + Temporal(4) = {self.total_dims}ç¶­")
        print(f"   - å°ˆç‚ºè©å½™èªéŸ³ç‰¹å¾µæå–å„ªåŒ–")

    def load_and_preprocess_audio(self, audio_path: Union[str, Path]) -> Optional[np.ndarray]:
        """
        è¼‰å…¥ä¸¦é è™•ç†è©å½™èªéŸ³æª”æ¡ˆ

        Args:
            audio_path: èªéŸ³æª”æ¡ˆè·¯å¾‘

        Returns:
            é è™•ç†å¾Œçš„éŸ³è¨Šä¿¡è™Ÿ (numpy array) æˆ– None
        """
        try:
            audio_path = Path(audio_path)
            if not audio_path.exists():
                print(f"âŒ èªéŸ³æª”æ¡ˆä¸å­˜åœ¨: {audio_path}")
                return None

            # ä½¿ç”¨ librosa è¼‰å…¥èªéŸ³æª”æ¡ˆ
            audio, sr = librosa.load(str(audio_path), sr=self.sample_rate, mono=True)

            if len(audio) == 0:
                print(f"âš ï¸  éŸ³è¨Šä¿¡è™Ÿç‚ºç©º: {audio_path.name}")
                return None

            # éŸ³è¨Šé è™•ç†
            audio = self._preprocess_audio(audio)

            return audio

        except Exception as e:
            print(f"âŒ éŸ³è¨Šè¼‰å…¥å¤±æ•— {audio_path.name}: {str(e)}")
            return None

    def _preprocess_audio(self, audio: np.ndarray) -> np.ndarray:
        """
        éŸ³è¨Šé è™•ç†ç®¡ç·š

        Args:
            audio: åŸå§‹éŸ³è¨Šä¿¡è™Ÿ

        Returns:
            é è™•ç†å¾Œçš„éŸ³è¨Šä¿¡è™Ÿ
        """
        # 1. ç§»é™¤ DC åˆ†é‡
        audio = audio - np.mean(audio)

        # 2. éŸ³é‡æ¨™æº–åŒ– (RMS æ­¸ä¸€åŒ–) - èªéŸ³å„ªåŒ–
        rms = np.sqrt(np.mean(audio**2))
        if rms > 0:
            audio = audio / rms * 0.3  # èªéŸ³ç‰¹å¾µå„ªåŒ–

        # 3. ä½é€šæ¿¾æ³¢å™¨ (ç§»é™¤é«˜é »å™ªéŸ³)
        nyquist = self.sample_rate // 2
        cutoff = min(8000, nyquist - 1)  # 8kHz ä½é€š
        b, a = butter(4, cutoff/nyquist, btype='low')
        audio = filtfilt(b, a, audio)

        # 4. ç•°å¸¸å€¼è™•ç†
        audio = np.clip(audio, -3*np.std(audio), 3*np.std(audio))

        return audio

    def extract_mfcc_features(self, audio: np.ndarray) -> Dict[str, np.ndarray]:
        """
        æå– MFCC (Mel-frequency cepstral coefficients) ç‰¹å¾µ

        Args:
            audio: éŸ³è¨Šä¿¡è™Ÿ

        Returns:
            MFCC ç‰¹å¾µå­—å…¸
        """
        try:
            # è¨ˆç®— MFCC ä¿‚æ•¸
            mfcc = librosa.feature.mfcc(
                y=audio,
                sr=self.sample_rate,
                n_mfcc=self.n_mfcc,
                n_fft=self.window_size,
                hop_length=self.hop_length,
                window='hann'
            )

            # è¨ˆç®— MFCC ä¸€éšå’ŒäºŒéšå·®åˆ† (Delta å’Œ Delta-Delta)
            mfcc_delta = librosa.feature.delta(mfcc)
            mfcc_delta2 = librosa.feature.delta(mfcc, order=2)

            return {
                'mfcc': mfcc,                    # åŸºæœ¬ MFCC ä¿‚æ•¸
                'mfcc_delta': mfcc_delta,        # ä¸€éšå·®åˆ†
                'mfcc_delta2': mfcc_delta2,      # äºŒéšå·®åˆ†
                'mfcc_mean': np.mean(mfcc, axis=1),
                'mfcc_std': np.std(mfcc, axis=1),
                'mfcc_min': np.min(mfcc, axis=1),
                'mfcc_max': np.max(mfcc, axis=1)
            }

        except Exception as e:
            print(f"âŒ MFCC ç‰¹å¾µæå–å¤±æ•—: {str(e)}")
            return {}

    def extract_phonetic_features(self, audio: np.ndarray) -> Dict[str, float]:
        """
        æå–èªéŸ³å­¸ (Phonetic) ç‰¹å¾µ

        Args:
            audio: éŸ³è¨Šä¿¡è™Ÿ

        Returns:
            èªéŸ³å­¸ç‰¹å¾µå­—å…¸
        """
        try:
            # åŸºé » (F0) æå–
            f0, voiced_flag, voiced_probs = librosa.pyin(
                audio,
                fmin=librosa.note_to_hz('C2'),
                fmax=librosa.note_to_hz('C7'),
                sr=self.sample_rate
            )

            # æ¸…ç† NaN å€¼
            f0_clean = f0[~np.isnan(f0)]

            # èªéŸ³æŒçºŒæ™‚é–“
            voiced_duration = np.sum(voiced_flag) / len(voiced_flag) if len(voiced_flag) > 0 else 0.0

            # èªéŸ³å¼·åº¦è®ŠåŒ–
            rms_frames = librosa.feature.rms(y=audio, hop_length=self.hop_length)[0]
            intensity_variation = np.std(rms_frames) if len(rms_frames) > 0 else 0.0

            return {
                'f0_mean': np.mean(f0_clean) if len(f0_clean) > 0 else 0.0,
                'f0_std': np.std(f0_clean) if len(f0_clean) > 0 else 0.0,
                'f0_range': np.ptp(f0_clean) if len(f0_clean) > 0 else 0.0,
                'voiced_duration_ratio': float(voiced_duration),
                'intensity_variation': float(intensity_variation)
            }

        except Exception as e:
            print(f"âŒ èªéŸ³å­¸ç‰¹å¾µæå–å¤±æ•—: {str(e)}")
            return {}

    def extract_spectral_features(self, audio: np.ndarray) -> Dict[str, float]:
        """
        æå–é »è­œ (Spectral) ç‰¹å¾µ

        Args:
            audio: éŸ³è¨Šä¿¡è™Ÿ

        Returns:
            Spectral ç‰¹å¾µå­—å…¸
        """
        try:
            # 1. Spectral Centroid (é »è­œé‡å¿ƒ)
            spectral_centroids = librosa.feature.spectral_centroid(
                y=audio, sr=self.sample_rate, hop_length=self.hop_length
            )[0]

            # 2. Spectral Bandwidth (é »è­œå¸¶å¯¬)
            spectral_bandwidth = librosa.feature.spectral_bandwidth(
                y=audio, sr=self.sample_rate, hop_length=self.hop_length
            )[0]

            # 3. Spectral Rolloff (é »è­œæ»¾é™é»)
            spectral_rolloff = librosa.feature.spectral_rolloff(
                y=audio, sr=self.sample_rate, hop_length=self.hop_length
            )[0]

            # 4. Zero Crossing Rate (éé›¶ç‡)
            zcr = librosa.feature.zero_crossing_rate(
                y=audio, hop_length=self.hop_length
            )[0]

            # 5. Spectral Contrast (é »è­œå°æ¯”åº¦)
            spectral_contrast = librosa.feature.spectral_contrast(
                y=audio, sr=self.sample_rate, hop_length=self.hop_length
            )

            return {
                'spectral_centroid_mean': np.mean(spectral_centroids),
                'spectral_bandwidth_mean': np.mean(spectral_bandwidth),
                'spectral_rolloff_mean': np.mean(spectral_rolloff),
                'zcr_mean': np.mean(zcr),
                'spectral_centroid_std': np.std(spectral_centroids),
                'spectral_bandwidth_std': np.std(spectral_bandwidth),
                'spectral_contrast_mean': np.mean(spectral_contrast)
            }

        except Exception as e:
            print(f"âŒ Spectral ç‰¹å¾µæå–å¤±æ•—: {str(e)}")
            return {}

    def extract_temporal_features(self, audio: np.ndarray) -> Dict[str, float]:
        """
        æå–æ™‚åŸŸ (Temporal) ç‰¹å¾µ

        Args:
            audio: éŸ³è¨Šä¿¡è™Ÿ

        Returns:
            Temporal ç‰¹å¾µå­—å…¸
        """
        try:
            # 1. RMS Energy (å‡æ–¹æ ¹èƒ½é‡)
            rms_energy = librosa.feature.rms(
                y=audio, hop_length=self.hop_length
            )[0]

            # 2. Tempo æª¢æ¸¬ (ç¯€æ‹)
            try:
                tempo, beats = librosa.beat.beat_track(
                    y=audio, sr=self.sample_rate, hop_length=self.hop_length
                )
            except:
                tempo = 0.0
                beats = []

            # 3. Onset Detection (èµ·å§‹é»æª¢æ¸¬)
            try:
                onset_frames = librosa.onset.onset_detect(
                    y=audio, sr=self.sample_rate, hop_length=self.hop_length
                )
                onset_rate = len(onset_frames) / (len(audio) / self.sample_rate)
            except:
                onset_rate = 0.0

            # 4. Spectral Flatness (é »è­œå¹³å¦åº¦)
            spectral_flatness = librosa.feature.spectral_flatness(
                y=audio, hop_length=self.hop_length
            )[0]

            return {
                'rms_energy_mean': np.mean(rms_energy),
                'tempo': float(tempo) if tempo is not None else 0.0,
                'onset_rate': onset_rate,
                'spectral_flatness_mean': np.mean(spectral_flatness)
            }

        except Exception as e:
            print(f"âŒ Temporal ç‰¹å¾µæå–å¤±æ•—: {str(e)}")
            return {}

    def extract_audio_features(self, audio_path: Union[str, Path], word: str = None) -> Optional[Dict]:
        """
        å¾è©å½™èªéŸ³æª”æ¡ˆä¸­æå–å®Œæ•´éŸ³è¨Šç‰¹å¾µ

        Args:
            audio_path: èªéŸ³æª”æ¡ˆè·¯å¾‘
            word: å°æ‡‰çš„å–®è© (å¯é¸)

        Returns:
            å®Œæ•´éŸ³è¨Šç‰¹å¾µå­—å…¸æˆ– None
        """
        try:
            audio_path = Path(audio_path)

            # å¾æª”æ¡ˆåæ¨æ–·å–®è©ï¼ˆå¦‚æœæœªæä¾›ï¼‰
            if word is None:
                word = audio_path.stem.split('_')[0]  # å‡è¨­æª”åæ ¼å¼ç‚º word_*

            # è¼‰å…¥å’Œé è™•ç†éŸ³è¨Š
            audio = self.load_and_preprocess_audio(audio_path)
            if audio is None:
                return None

            # æå–å„é¡ç‰¹å¾µ
            mfcc_features = self.extract_mfcc_features(audio)
            phonetic_features = self.extract_phonetic_features(audio)  # å–ä»£ chroma
            spectral_features = self.extract_spectral_features(audio)
            temporal_features = self.extract_temporal_features(audio)

            # æ•´åˆæ‰€æœ‰ç‰¹å¾µ
            features = {
                'audio_path': str(audio_path),
                'word': word,
                'audio_duration': len(audio) / self.sample_rate,
                'sample_rate': self.sample_rate,
                'mfcc': mfcc_features,
                'phonetic': phonetic_features,
                'spectral': spectral_features,
                'temporal': temporal_features,
                'feature_dims': self.feature_dims,
                'total_dims': self.total_dims
            }

            return features

        except Exception as e:
            print(f"âŒ éŸ³è¨Šç‰¹å¾µæå–å¤±æ•— {audio_path.name}: {str(e)}")
            return None

    def _extract_temporal_aligned_features(self, audio: np.ndarray) -> Dict[str, List]:
        """
        æå–èˆ‡100å¹€å½±åƒå°é½Šçš„æ™‚åºéŸ³è¨Šç‰¹å¾µ

        Args:
            audio: éŸ³è¨Šä¿¡è™Ÿ

        Returns:
            æ™‚åºå°é½Šçš„ç‰¹å¾µåºåˆ—
        """
        try:
            # è¨ˆç®—éŸ³è¨Šç¸½æ™‚é•·
            duration = len(audio) / self.sample_rate

            # åˆ†å‰²ç‚º100å€‹æ™‚é–“ç‰‡æ®µ
            frame_duration = duration / self.target_frames
            frame_samples = int(frame_duration * self.sample_rate)

            aligned_features = {
                'frame_energy': [],
                'frame_zcr': [],
                'frame_spectral_centroid': [],
                'frame_spectral_bandwidth': []
            }

            for frame_idx in range(self.target_frames):
                start_sample = int(frame_idx * frame_samples)
                end_sample = min(start_sample + frame_samples, len(audio))

                if start_sample >= len(audio):
                    # è™•ç†éŸ³è¨Šé•·åº¦ä¸è¶³çš„æƒ…æ³ï¼Œå¡«å……é›¶å€¼
                    aligned_features['frame_energy'].append(0.0)
                    aligned_features['frame_zcr'].append(0.0)
                    aligned_features['frame_spectral_centroid'].append(0.0)
                    aligned_features['frame_spectral_bandwidth'].append(0.0)
                else:
                    frame_audio = audio[start_sample:end_sample]

                    # è¨ˆç®—è©²å¹€çš„éŸ³è¨Šç‰¹å¾µ
                    if len(frame_audio) > 0:
                        rms = np.sqrt(np.mean(frame_audio**2))
                        zcr = np.mean(librosa.feature.zero_crossing_rate(frame_audio)[0])

                        # é »è­œç‰¹å¾µï¼ˆéœ€è¦è¶³å¤ çš„æ¨£æœ¬ï¼‰
                        if len(frame_audio) >= 512:
                            spec_cent = np.mean(librosa.feature.spectral_centroid(
                                y=frame_audio, sr=self.sample_rate)[0])
                            spec_bw = np.mean(librosa.feature.spectral_bandwidth(
                                y=frame_audio, sr=self.sample_rate)[0])
                        else:
                            spec_cent = 0.0
                            spec_bw = 0.0

                        aligned_features['frame_energy'].append(float(rms))
                        aligned_features['frame_zcr'].append(float(zcr))
                        aligned_features['frame_spectral_centroid'].append(float(spec_cent))
                        aligned_features['frame_spectral_bandwidth'].append(float(spec_bw))
                    else:
                        aligned_features['frame_energy'].append(0.0)
                        aligned_features['frame_zcr'].append(0.0)
                        aligned_features['frame_spectral_centroid'].append(0.0)
                        aligned_features['frame_spectral_bandwidth'].append(0.0)

            return aligned_features

        except Exception as e:
            print(f"âŒ æ™‚åºå°é½Šç‰¹å¾µæå–å¤±æ•—: {str(e)}")
            return {}

    def extract_normalized_features(self, features: Dict) -> np.ndarray:
        """
        æå–æ¨™æº–åŒ–çš„ç‰¹å¾µå‘é‡ (24ç¶­)

        Args:
            features: å®Œæ•´ç‰¹å¾µå­—å…¸

        Returns:
            24ç¶­æ¨™æº–åŒ–ç‰¹å¾µå‘é‡
        """
        try:
            feature_vector = []

            # 1. MFCC ç‰¹å¾µ (13ç¶­)
            if 'mfcc' in features and 'mfcc_mean' in features['mfcc']:
                mfcc_mean = features['mfcc']['mfcc_mean'][:self.n_mfcc]
                feature_vector.extend(mfcc_mean)
            else:
                feature_vector.extend([0.0] * self.n_mfcc)

            # 2. Spectral ç‰¹å¾µ (7ç¶­)
            spectral_keys = [
                'spectral_centroid_mean', 'spectral_bandwidth_mean',
                'spectral_rolloff_mean', 'zcr_mean',
                'spectral_centroid_std', 'spectral_bandwidth_std',
                'spectral_contrast_mean'
            ]
            for key in spectral_keys:
                if 'spectral' in features and key in features['spectral']:
                    feature_vector.append(features['spectral'][key])
                else:
                    feature_vector.append(0.0)

            # 3. Temporal ç‰¹å¾µ (4ç¶­)
            temporal_keys = [
                'rms_energy_mean', 'tempo', 'onset_rate', 'spectral_flatness_mean'
            ]
            for key in temporal_keys:
                if 'temporal' in features and key in features['temporal']:
                    feature_vector.append(features['temporal'][key])
                else:
                    feature_vector.append(0.0)

            # è½‰æ›ç‚º numpy é™£åˆ—ä¸¦æ¨™æº–åŒ–
            feature_vector = np.array(feature_vector, dtype=np.float32)

            # Z-score æ¨™æº–åŒ–
            if np.std(feature_vector) > 0:
                feature_vector = zscore(feature_vector)

            # è™•ç† NaN å’Œ Inf
            feature_vector = np.nan_to_num(feature_vector, nan=0.0, posinf=1.0, neginf=-1.0)

            return feature_vector

        except Exception as e:
            print(f"âŒ ç‰¹å¾µå‘é‡æ¨™æº–åŒ–å¤±æ•—: {str(e)}")
            return np.zeros(self.total_dims, dtype=np.float32)

    def extract_word_features_from_multiple_versions(self, audio_files: List[Path], word: str) -> Dict:
        """
        å¾å¤šå€‹èªéŸ³ç‰ˆæœ¬ä¸­æå–èåˆç‰¹å¾µ

        Args:
            audio_files: åŒä¸€å–®è©çš„å¤šå€‹èªéŸ³æª”æ¡ˆ
            word: å–®è©

        Returns:
            èåˆçš„ç‰¹å¾µå­—å…¸
        """
        try:
            all_features = []
            all_vectors = []

            for audio_file in audio_files:
                features = self.extract_audio_features(audio_file, word)
                if features:
                    all_features.append(features)
                    vector = self.extract_normalized_features(features)
                    all_vectors.append(vector)

            if not all_vectors:
                return {}

            # ç‰¹å¾µèåˆç­–ç•¥
            all_vectors = np.array(all_vectors)

            # 1. å¹³å‡å€¼èåˆ
            mean_vector = np.mean(all_vectors, axis=0)

            # 2. æœ€å¤§å€¼èåˆ (ä¿ç•™æœ€é¡•è‘—çš„ç‰¹å¾µ)
            max_vector = np.max(all_vectors, axis=0)

            # 3. åŠ æ¬Šèåˆ (å“è³ªé«˜çš„ç‰ˆæœ¬æ¬Šé‡æ›´å¤§)
            weights = []
            for features in all_features:
                duration = features.get('audio_duration', 1.0)
                # ç°¡å–®å“è³ªè©•ä¼°ï¼šæ™‚é•·é©ä¸­çš„èªéŸ³å“è³ªè¼ƒå¥½
                quality_score = 1.0 / (1.0 + abs(duration - 1.0))  # å‡è¨­ 1 ç§’æ˜¯ç†æƒ³æ™‚é•·
                weights.append(quality_score)

            weights = np.array(weights)
            weights = weights / np.sum(weights)  # æ­¸ä¸€åŒ–

            weighted_vector = np.average(all_vectors, axis=0, weights=weights)

            # è¿”å›èåˆçµæœ
            fusion_result = {
                'word': word,
                'num_versions': len(all_features),
                'version_details': [{
                    'file': str(f['audio_path']),
                    'duration': f['audio_duration'],
                    'quality_weight': w
                } for f, w in zip(all_features, weights)],
                'features': {
                    'mean_fusion': mean_vector,
                    'max_fusion': max_vector,
                    'weighted_fusion': weighted_vector,
                    'std_across_versions': np.std(all_vectors, axis=0)
                },
                'recommended_vector': weighted_vector,  # æ¨è–¦ä½¿ç”¨åŠ æ¬Šèåˆ
                'feature_dims': self.feature_dims,
                'total_dims': self.total_dims
            }

            return fusion_result

        except Exception as e:
            print(f"âŒ å¤šç‰ˆæœ¬ç‰¹å¾µèåˆå¤±æ•— {word}: {str(e)}")
            return {}


def extract_single_word_audio(word: str, audio_files: List[Path], extractor: AudioFeatureExtractor) -> Tuple[str, Optional[Dict]]:
    """
    å–®ä¸€å–®è©çš„èªéŸ³ç‰¹å¾µæå– (å¤šé€²ç¨‹è™•ç†å‡½æ•¸)

    Args:
        word: å–®è©
        audio_files: è©²å–®è©çš„èªéŸ³æª”æ¡ˆåˆ—è¡¨
        extractor: éŸ³è¨Šç‰¹å¾µæå–å™¨å¯¦ä¾‹

    Returns:
        (å–®è©, ç‰¹å¾µå­—å…¸) æˆ– (å–®è©, None)
    """
    try:
        features = extractor.extract_word_features_from_multiple_versions(audio_files, word)
        return (word, features)
    except Exception as e:
        print(f"âŒ å–®ä¸€å–®è©è™•ç†å¤±æ•— {word}: {str(e)}")
        return (word, None)


def run_vocabulary_audio_extraction(
    audio_root: Optional[str] = None,
    output_root: Optional[str] = None,
    num_workers: int = NUM_WORKERS,
    download_audio: bool = True,
    vocabulary: List[str] = SIGN_LANGUAGE_VOCABULARY
) -> Dict[str, int]:
    """
    æ‰¹æ¬¡è©å½™èªéŸ³ç‰¹å¾µæå–ä¸»å‡½æ•¸

    Args:
        audio_root: è©å½™èªéŸ³æ ¹ç›®éŒ„è·¯å¾‘
        output_root: è¼¸å‡ºæ ¹ç›®éŒ„è·¯å¾‘
        num_workers: å¤šé€²ç¨‹å·¥ä½œæ•¸é‡
        download_audio: æ˜¯å¦è‡ªå‹•ä¸‹è¼‰èªéŸ³
        vocabulary: è©å½™åˆ—è¡¨

    Returns:
        è™•ç†çµæœçµ±è¨ˆå­—å…¸
    """
    # è·¯å¾‘é…ç½®
    audio_data_path = Path(audio_root) if audio_root else AUDIO_DATA_ROOT
    output_path = Path(output_root) if output_root else OUTPUT_ROOT

    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    output_path.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ¤ é–‹å§‹è©å½™èªéŸ³ç‰¹å¾µæå–")
    print(f"   ğŸ“‚ èªéŸ³æ ¹ç›®éŒ„: {audio_data_path}")
    print(f"   ğŸ“‚ è¼¸å‡ºè·¯å¾‘: {output_path}")
    print(f"   ğŸ‘¥ å·¥ä½œé€²ç¨‹æ•¸: {num_workers}")
    print(f"   ğŸ“š è©å½™æ•¸é‡: {len(vocabulary)}")

    # 1. åˆå§‹åŒ–èªéŸ³ä¸‹è¼‰å™¨å’Œç‰¹å¾µæå–å™¨
    if download_audio:
        downloader = VocabularyAudioDownloader(vocabulary, audio_data_path)
        print("ğŸ¤ é–‹å§‹ä¸‹è¼‰è©å½™èªéŸ³...")
        vocabulary_audio = downloader.download_all_vocabulary_audio(max_versions_per_word=4)
    else:
        # ç›´æ¥æ‰«æç¾æœ‰çš„èªéŸ³æª”æ¡ˆ
        vocabulary_audio = {}
        for word in vocabulary:
            word_dir = audio_data_path / word
            if word_dir.exists():
                audio_files = list(word_dir.glob('*.wav')) + list(word_dir.glob('*.mp3'))
                vocabulary_audio[word] = audio_files

    extractor = AudioFeatureExtractor()

    # çµ±è¨ˆèªéŸ³æª”æ¡ˆ
    total_audio_files = sum(len(files) for files in vocabulary_audio.values())
    print(f"   ğŸ¤ æ‰¾åˆ° {total_audio_files} å€‹èªéŸ³æª”æ¡ˆ")

    # 2. å¤šé€²ç¨‹æ‰¹æ¬¡è™•ç†
    results = {
        'total_words': len(vocabulary),
        'success_count': 0,
        'failed_count': 0,
        'total_audio_files': total_audio_files,
        'processed_words': set()
    }

    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»å‹™
        futures = {
            executor.submit(extract_single_word_audio, word, audio_files, extractor): word
            for word, audio_files in vocabulary_audio.items() if audio_files
        }

        # è™•ç†çµæœ
        for future in tqdm(as_completed(futures), total=len(futures), desc="ğŸ¤ è©å½™èªéŸ³ç‰¹å¾µæå–"):
            word = futures[future]
            try:
                word_name, features = future.result()

                if features and 'recommended_vector' in features:
                    # å„²å­˜å®Œæ•´ç‰¹å¾µ
                    output_file = output_path / f"{word}_audio_features.json"
                    with open(output_file, 'w', encoding='utf-8') as f:
                        json.dump(features, f, indent=2, ensure_ascii=False, default=str)

                    # å„²å­˜æ¨è–¦çš„æ¨™æº–åŒ–ç‰¹å¾µå‘é‡
                    recommended_vector = features['recommended_vector']
                    vector_file = output_path / f"{word}_normalized_24d.npy"
                    np.save(vector_file, recommended_vector)

                    # å„²å­˜æ‰€æœ‰èåˆç‰ˆæœ¬
                    for fusion_type, vector in features['features'].items():
                        fusion_file = output_path / f"{word}_{fusion_type}_24d.npy"
                        np.save(fusion_file, vector)

                    results['success_count'] += 1
                    results['processed_words'].add(word)

                    print(f"   âœ… {word}: {features['num_versions']}å€‹èªéŸ³ç‰ˆæœ¬èåˆå®Œæˆ")
                else:
                    results['failed_count'] += 1
                    print(f"   âŒ {word}: ç‰¹å¾µæå–å¤±æ•—")

            except Exception as e:
                print(f"âŒ è™•ç†çµæœå¤±æ•— {word}: {str(e)}")
                results['failed_count'] += 1

    # 3. è¼¸å‡ºè™•ç†çµæœ
    print(f"ğŸ¤ è©å½™èªéŸ³ç‰¹å¾µæå–å®Œæˆ!")
    print(f"   âœ… æˆåŠŸ: {results['success_count']}/{results['total_words']} å–®è©")
    print(f"   âŒ å¤±æ•—: {results['failed_count']} å–®è©")
    print(f"   ğŸ¤ ç¸½èªéŸ³æª”æ¡ˆ: {results['total_audio_files']}")
    print(f"   ğŸ“Š æˆåŠŸç‡: {results['success_count']/results['total_words']*100:.1f}%")
    print(f"   ğŸ“ è¼¸å‡ºç›®éŒ„: {output_path}")

    return results


if __name__ == "__main__":
    # åŸ·è¡Œè©å½™èªéŸ³ç‰¹å¾µæå–
    results = run_vocabulary_audio_extraction(download_audio=True)