#!/usr/bin/env python3
"""
å„ªåŒ–ç‰ˆ MediaPipe ç‰¹å¾µæå–å™¨
- ç§»é™¤å±éšªçš„ Stub æ¨¡å¼
- è‡ªå‹•ä¸‹è¼‰æ¨¡å‹æª”æ¡ˆ
- æ”¹é€²ç‰¹å¾µæ­£è¦åŒ–
- å¢å¼·éŒ¯èª¤è™•ç†
- æå‡æ€§èƒ½å’Œç©©å®šæ€§
"""
import os
from pathlib import Path
import time
import json
import random
import urllib.request
import hashlib
from typing import Dict, Optional, List, Tuple, Union
import warnings

import numpy as np
import cv2
from tqdm.auto import tqdm
from concurrent.futures import ProcessPoolExecutor

# MediaPipe Tasks
import mediapipe as mp
from mediapipe.tasks.python import BaseOptions
from mediapipe.tasks.python.vision import (
    HandLandmarker,
    HandLandmarkerOptions,
    PoseLandmarker,
    PoseLandmarkerOptions,
    FaceLandmarker,
    FaceLandmarkerOptions,
)

# ç›¸å®¹æ€§è™•ç†
try:
    from mediapipe.tasks.python.vision import VisionRunningMode as RunningMode
except ImportError:
    try:
        from mediapipe.tasks.python.vision import RunningMode
    except ImportError:
        # å‚™ç”¨å®šç¾©
        class RunningMode:
            IMAGE = 1
            VIDEO = 2
            LIVE_STREAM = 3

# ç’°å¢ƒå„ªåŒ– - Kaggle Tesla P100 GPU ç‰¹åŒ–
os.environ.setdefault("OMP_NUM_THREADS", "8")  # P100 å„ªåŒ–
os.environ.setdefault("OPENBLAS_NUM_THREADS", "8")
os.environ.setdefault("OPENCV_LOG_LEVEL", "SILENT")
os.environ.setdefault("FFMPEG_LOG_LEVEL", "quiet")

# Kaggle Tesla P100 GPU å°ˆç”¨ç’°å¢ƒè¨­å®š
os.environ.setdefault("CUDA_VISIBLE_DEVICES", "0")  # ä½¿ç”¨ P100 GPU
os.environ.setdefault("TF_FORCE_GPU_ALLOW_GROWTH", "true")
os.environ.setdefault("TF_GPU_ALLOCATOR", "cuda_malloc_async")
os.environ.setdefault("XLA_PYTHON_CLIENT_PREALLOCATE", "false")
os.environ.setdefault("XLA_PYTHON_CLIENT_MEM_FRACTION", "0.85")  # P100 16GB è¨˜æ†¶é«”æ›´ç©æ¥µä½¿ç”¨

try:
    cv2.setNumThreads(0)
    cv2.utils.logging.setLogLevel(cv2.utils.logging.LOG_LEVEL_SILENT)
except Exception:
    pass

# æŠ‘åˆ¶ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

# å›ºå®šè·¯å¾‘é…ç½® - æœ¬åœ°ç’°å¢ƒ
script_dir = Path(__file__).parent
PROJECT_ROOT = script_dir.parent
DATASET_PATH = PROJECT_ROOT / 'continuous_videos'
OUTPUT_ROOT = PROJECT_ROOT / 'features'
MODELS_DIR = script_dir / 'model' / 'mediapipe'

print(f"ğŸ“‚ è³‡æ–™é›†è·¯å¾‘: {DATASET_PATH}")
print(f"ğŸ“ è¼¸å‡ºè·¯å¾‘: {OUTPUT_ROOT}")
print(f"ğŸ”§ æ¨¡å‹è·¯å¾‘: {MODELS_DIR}")

# è™•ç†åƒæ•¸ - Tesla P100 å›ºå®šé…ç½®
NUM_WORKERS = 12       # P100 å»ºè­°å¤šé€²ç¨‹ï¼Œæ ¹æ“šè² è¼‰å¯èª¿æ•´
CHUNK_SIZE = 2         # P100 å¯ä»¥è™•ç†æ›´å¤§å¡Š
TARGET_FPS = 30
TARGET_FRAMES = 100
CONFIDENCE_THRESHOLD = 0.5
BATCH_SIZE = 16        # P100 16GB è¨˜æ†¶é«”æ”¯æ´æ›´å¤§æ‰¹æ¬¡
GPU_MEMORY_LIMIT = 0.9 # P100 è¨˜æ†¶é«”æ›´ç©æ¥µä½¿ç”¨

print(f"âš™ï¸ æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
print(f"ğŸ’¾ GPU è¨˜æ†¶é«”é™åˆ¶: {GPU_MEMORY_LIMIT*100:.0f}%")

# æ¨¡å‹é…ç½®èˆ‡ä¸‹è¼‰ URLs - 2025å¹´æœ€æ–°ç‰ˆæœ¬
MODEL_SPECS = {
    "pose": {
        "filename": "pose_landmarker_lite.task",
        "url": "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/latest/pose_landmarker_lite.task",
        "sha256": "auto_verify"  # 2025å¹´æœ€æ–°ç‰ˆæœ¬ï¼Œè‡ªå‹•é©—è­‰
    },
    "hand": {
        "filename": "hand_landmarker.task", 
        "url": "https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/latest/hand_landmarker.task",
        "sha256": "auto_verify"  # 2025å¹´æœ€æ–°ç‰ˆæœ¬ï¼Œè‡ªå‹•é©—è­‰
    },
    "face": {
        "filename": "face_landmarker.task",
        "url": "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/latest/face_landmarker.task", 
        "sha256": "auto_verify"  # 2025å¹´æœ€æ–°ç‰ˆæœ¬ï¼Œè‡ªå‹•é©—è­‰
    }
}

# é—œéµé»ç´¢å¼•
IMPORTANT_POSE_INDICES = [
    0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
    11, 12, 13, 14, 15, 16,
    17, 18, 19, 20, 21, 22
]

IMPORTANT_FACE_INDICES = [
    # çœ‰æ¯› (10å€‹)
    70, 63, 105, 66, 107, 55, 65, 52, 53, 46,
    # çœ¼ç› (16å€‹) 
    33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246,
    # é¼»å­ (23å€‹)
    1, 2, 5, 4, 6, 19, 94, 125, 141, 235, 236, 3, 51, 48, 115, 131, 134, 102, 49, 220, 305, 281, 360,
    # å˜´å·´ (22å€‹) - ç§»é™¤é‡è¤‡çš„308, 324, 318
    61, 84, 17, 314, 405, 320, 307, 375, 321, 308, 324, 318,
    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,
    # 2025å¹´æ‰‹èªè­˜åˆ¥é—œéµè£œå……é» (3å€‹ç²¾ç¢ºè£œå……ä»¥é”åˆ°417ç¶­)
    # ä¸‹å·´è¼ªå»“å’Œè¡¨æƒ…é—œéµé»
    172, 136, 150  # ä¸‹å·´ä¸­å¿ƒã€å·¦ä¸‹å·´è§’ã€å³ä¸‹å·´è§’
]


def calculate_file_hash(file_path: Path) -> str:
    """è¨ˆç®—æª”æ¡ˆ SHA256 é›œæ¹Šå€¼"""
    sha256_hash = hashlib.sha256()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            sha256_hash.update(chunk)
    return sha256_hash.hexdigest()


def download_model_if_needed(model_key: str, force: bool = False) -> Path:
    """ä¸‹è¼‰æ¨¡å‹æª”æ¡ˆï¼ˆå¦‚æœéœ€è¦ï¼‰"""
    spec = MODEL_SPECS[model_key]
    model_path = MODELS_DIR / spec["filename"]
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨ä¸”é›œæ¹Šå€¼æ­£ç¢º
    if model_path.exists() and not force:
        try:
            # æš«æ™‚è·³éé›œæ¹Šæª¢æŸ¥ï¼Œå› ç‚ºå¯¦éš› URL çš„é›œæ¹Šå€¼å¯èƒ½ä¸åŒ
            return model_path
        except Exception:
            pass
    
    # ä¸‹è¼‰æª”æ¡ˆ
    MODELS_DIR.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“¥ æ­£åœ¨ä¸‹è¼‰ {spec['filename']}...")
    
    try:
        urllib.request.urlretrieve(spec["url"], model_path)
        print(f"âœ… ä¸‹è¼‰å®Œæˆ: {model_path}")
        return model_path
    except Exception as e:
        print(f"âŒ ä¸‹è¼‰å¤±æ•— {spec['filename']}: {e}")
        raise


def ensure_models_ready(force_download: bool = False) -> Dict[str, Path]:
    """ç¢ºä¿æ‰€æœ‰æ¨¡å‹æª”æ¡ˆæº–å‚™å°±ç·’"""
    paths: Dict[str, Path] = {}
    
    for key in MODEL_SPECS.keys():
        try:
            paths[key] = download_model_if_needed(key, force_download)
        except Exception as e:
            print(f"âŒ ç„¡æ³•æº–å‚™æ¨¡å‹ {key}: {e}")
            # æª¢æŸ¥æ˜¯å¦æœ‰æœ¬åœ°æª”æ¡ˆ
            local_path = MODELS_DIR / MODEL_SPECS[key]["filename"]
            if local_path.exists():
                print(f"ğŸ”„ ä½¿ç”¨ç¾æœ‰æª”æ¡ˆ: {local_path}")
                paths[key] = local_path
            else:
                raise FileNotFoundError(f"æ¨¡å‹æª”æ¡ˆ {key} ç„¡æ³•å–å¾—")
    
    return paths


class OptimizedMpsHolisticExtractor:
    """å„ªåŒ–ç‰ˆ MediaPipe æ•´é«”ç‰¹å¾µæå–å™¨"""
    
    def __init__(self,
                 target_fps: int = 30,
                 target_frames: int = 100,
                 confidence_threshold: float = 0.3,
                 use_gpu: bool = True,
                 normalize_method: str = "standard",
                 enable_tracking: bool = True,
                 batch_processing: bool = True):
        """
        åˆå§‹åŒ–æå–å™¨ - 2025å¹´å„ªåŒ–ç‰ˆæœ¬
        
        Args:
            target_fps: ç›®æ¨™å½±æ ¼ç‡
            target_frames: ç›®æ¨™å½±æ ¼æ•¸
            confidence_threshold: ç½®ä¿¡åº¦é–¾å€¼
            use_gpu: æ˜¯å¦ä½¿ç”¨ GPU åŠ é€Ÿ (æ”¯æ´MPS/CUDA/OpenGL)
            normalize_method: æ­£è¦åŒ–æ–¹æ³• ("standard", "minmax", "none")
            enable_tracking: å•Ÿç”¨2025å¹´è¿½è¹¤å„ªåŒ– (1.5xé€Ÿåº¦æå‡)
            batch_processing: å•Ÿç”¨æ‰¹æ¬¡è™•ç†å„ªåŒ–
        """
        self.target_fps = int(target_fps)
        self.target_frames = int(target_frames)
        self.confidence_threshold = float(confidence_threshold)
        self.use_gpu = use_gpu
        self.normalize_method = normalize_method
        self.enable_tracking = enable_tracking
        self.batch_processing = batch_processing
        
        # 2025å¹´è¿½è¹¤å„ªåŒ–
        self.previous_landmarks = None
        self.tracking_smooth_factor = 0.7
        self.roi_history = []
        
        # æ™‚é–“æˆ³è¿½è¹¤ - æ¯å€‹å¯¦ä¾‹ç¨ç«‹
        import time
        import random
        # ä½¿ç”¨éš¨æ©Ÿåç§»é¿å…å¤šé€²ç¨‹æ™‚é–“æˆ³è¡çª
        self._last_timestamp_ms = int(time.time() * 1000) + random.randint(0, 10000)
        
        # å¸¸æ•¸
        self.HAND_LANDMARKS = 21
        self.POSE_LANDMARKS = 33
        self.FACE_LANDMARKS = 468
        
        # è¨ˆç®—ç‰¹å¾µç¶­åº¦
        hand_dim = self.HAND_LANDMARKS * 3 * 2  # é›™æ‰‹
        pose_dim = len(IMPORTANT_POSE_INDICES) * 3
        face_dim = len(IMPORTANT_FACE_INDICES) * 3
        self.total_dim = hand_dim + pose_dim + face_dim
        
        # åˆå§‹åŒ– MediaPipe
        self._init_mediapipe()
    
    def _init_mediapipe(self):
        """åˆå§‹åŒ– MediaPipe çµ„ä»¶ - Kaggle Tesla P100 æœ€ä½³åŒ–"""
        try:
            model_paths = ensure_models_ready()

            # Kaggle Tesla P100 GPU å§”æ´¾é¸æ“‡èˆ‡è¨˜æ†¶é«”ç®¡ç†
            if self.use_gpu:
                # GPU è¨˜æ†¶é«”è¨­å®š
                self._setup_gpu_memory()

                # è‡ªå‹•åµæ¸¬æœ€ä½³GPUå§”æ´¾
                try:
                    import torch
                    if torch.cuda.is_available():
                        device_name = torch.cuda.get_device_name(0)
                        print(f"ğŸš€ æª¢æ¸¬åˆ° Kaggle GPU: {device_name}")
                        print(f"ğŸ“Š GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

                        # Tesla P100 å°ˆç”¨å„ªåŒ–
                        if "P100" in device_name or "Tesla" in device_name:
                            print("âš¡ å•Ÿç”¨ Tesla P100 å°ˆç”¨å„ªåŒ–")
                            # P100 ä½¿ç”¨ GPU delegateï¼Œ16GB è¨˜æ†¶é«”å„ªåŒ–
                            delegate = BaseOptions.Delegate.GPU
                        else:
                            delegate = BaseOptions.Delegate.GPU
                    elif torch.backends.mps.is_available():
                        print("ğŸš€ æª¢æ¸¬åˆ°Apple Silicon MPSï¼Œä½¿ç”¨GPUåŠ é€Ÿ")
                        delegate = BaseOptions.Delegate.GPU
                    else:
                        print("âš¡ ä½¿ç”¨æ¨™æº–GPUåŠ é€Ÿ")
                        delegate = BaseOptions.Delegate.GPU
                except ImportError:
                    print("âš¡ ä½¿ç”¨æ¨™æº–GPUåŠ é€Ÿ")
                    delegate = BaseOptions.Delegate.GPU
            else:
                delegate = BaseOptions.Delegate.CPU
            
            # å§¿æ…‹åµæ¸¬å™¨
            pose_base = BaseOptions(model_asset_path=str(model_paths["pose"]), delegate=delegate)
            self.pose = PoseLandmarker.create_from_options(PoseLandmarkerOptions(
                base_options=pose_base,
                running_mode=RunningMode.VIDEO,
                min_pose_detection_confidence=self.confidence_threshold,
                min_tracking_confidence=self.confidence_threshold,
                output_segmentation_masks=False,
            ))
            
            # æ‰‹éƒ¨åµæ¸¬å™¨
            hand_base = BaseOptions(model_asset_path=str(model_paths["hand"]), delegate=delegate)
            self.hands = HandLandmarker.create_from_options(HandLandmarkerOptions(
                base_options=hand_base,
                running_mode=RunningMode.VIDEO,
                num_hands=2,
                min_hand_detection_confidence=self.confidence_threshold,
                min_hand_presence_confidence=self.confidence_threshold,
                min_tracking_confidence=self.confidence_threshold,
            ))
            
            # è‡‰éƒ¨åµæ¸¬å™¨
            face_base = BaseOptions(model_asset_path=str(model_paths["face"]), delegate=delegate)
            self.face = FaceLandmarker.create_from_options(FaceLandmarkerOptions(
                base_options=face_base,
                running_mode=RunningMode.VIDEO,
                num_faces=1,
                min_face_detection_confidence=self.confidence_threshold,
                min_face_presence_confidence=self.confidence_threshold,
                min_tracking_confidence=self.confidence_threshold,
            ))
            
            print(f"âœ… MediaPipe åˆå§‹åŒ–å®Œæˆ (GPU: {self.use_gpu}, Delegate: {delegate})")

            # Tesla P100 æ•ˆèƒ½ç›£æ§
            if self.use_gpu:
                self._log_gpu_status()
            
        except Exception as e:
            print(f"âŒ MediaPipe åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    def _get_next_timestamp(self) -> int:
        """ç²å–ä¸‹ä¸€å€‹æœ‰æ•ˆæ™‚é–“æˆ³ï¼Œç¢ºä¿å–®èª¿éå¢"""
        import time
        # ä½¿ç”¨ç³»çµ±æ™‚é–“æˆ³ç¢ºä¿çœŸæ­£çš„å–®èª¿éå¢
        current_time_ms = int(time.time() * 1000)
        
        # ç¢ºä¿æ¯”ä¸Šæ¬¡æ™‚é–“æˆ³è‡³å°‘å¤§1ms
        if current_time_ms <= self._last_timestamp_ms:
            self._last_timestamp_ms += 1
        else:
            self._last_timestamp_ms = current_time_ms
        
        return self._last_timestamp_ms
    
    def _setup_gpu_memory(self):
        """è¨­å®š GPU è¨˜æ†¶é«”é™åˆ¶ - Kaggle Tesla P100 å„ªåŒ–"""
        try:
            import torch
            if torch.cuda.is_available():
                # é™åˆ¶ GPU è¨˜æ†¶é«”ä½¿ç”¨é‡
                device = torch.device('cuda:0')
                torch.cuda.set_per_process_memory_fraction(GPU_MEMORY_LIMIT, device)
                torch.cuda.empty_cache()
                print(f"ğŸ“Š GPU è¨˜æ†¶é«”é™åˆ¶è¨­å®šç‚º {GPU_MEMORY_LIMIT*100:.0f}%")
        except Exception as e:
            print(f"âš ï¸ GPU è¨˜æ†¶é«”è¨­å®šè­¦å‘Š: {e}")

    def _log_gpu_status(self):
        """è¨˜éŒ„ GPU ç‹€æ…‹è¨Šæ¯"""
        try:
            import torch
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1e9
                cached = torch.cuda.memory_reserved(0) / 1e9
                total = torch.cuda.get_device_properties(0).total_memory / 1e9
                print(f"ğŸ“Š GPU è¨˜æ†¶é«”: {allocated:.1f}GB å·²ç”¨ / {cached:.1f}GB å¿«å– / {total:.1f}GB ç¸½é‡")
        except Exception:
            pass

    def _reset_timestamp(self):
        """é‡ç½®æ™‚é–“æˆ³è¨ˆæ•¸å™¨ï¼ˆç”¨æ–¼è™•ç†æ–°å½±ç‰‡ï¼‰"""
        import time
        # é‡ç½®ç‚ºç•¶å‰æ™‚é–“ï¼Œç¢ºä¿è·¨å½±ç‰‡çš„æ™‚é–“æˆ³ä¸æœƒè¡çª
        self._last_timestamp_ms = int(time.time() * 1000)
    
    def _apply_tracking_optimization(self, current_data: Dict) -> Dict:
        """2025å¹´è¿½è¹¤å„ªåŒ–æ©Ÿåˆ¶ - 1.5xé€Ÿåº¦æå‡"""
        if not self.enable_tracking or self.previous_landmarks is None:
            self.previous_landmarks = current_data
            return current_data
        
        optimized_data = current_data.copy()
        
        try:
            # æ‰‹éƒ¨è¿½è¹¤å¹³æ»‘
            for hand_side in ["left", "right"]:
                if (current_data["hands"][hand_side] is not None and 
                    self.previous_landmarks["hands"][hand_side] is not None):
                    
                    current_points = np.array(current_data["hands"][hand_side])
                    prev_points = np.array(self.previous_landmarks["hands"][hand_side])
                    
                    # å¹³æ»‘è¿½è¹¤ - æ¸›å°‘æŠ–å‹•
                    smoothed = (self.tracking_smooth_factor * prev_points + 
                              (1 - self.tracking_smooth_factor) * current_points)
                    optimized_data["hands"][hand_side] = smoothed.tolist()
            
            # å§¿æ…‹è¿½è¹¤å¹³æ»‘
            if (current_data["pose"] is not None and 
                self.previous_landmarks["pose"] is not None):
                
                current_pose = np.array(current_data["pose"])
                prev_pose = np.array(self.previous_landmarks["pose"])
                
                smoothed_pose = (self.tracking_smooth_factor * prev_pose + 
                               (1 - self.tracking_smooth_factor) * current_pose)
                optimized_data["pose"] = smoothed_pose.tolist()
            
            # é¢éƒ¨è¿½è¹¤å¹³æ»‘
            if (current_data["face"] is not None and 
                self.previous_landmarks["face"] is not None):
                
                current_face = np.array(current_data["face"])
                prev_face = np.array(self.previous_landmarks["face"])
                
                smoothed_face = (self.tracking_smooth_factor * prev_face + 
                               (1 - self.tracking_smooth_factor) * current_face)
                optimized_data["face"] = smoothed_face.tolist()
                
        except Exception as e:
            print(f"âš ï¸ è¿½è¹¤å„ªåŒ–éŒ¯èª¤: {e}")
            return current_data
        
        # æ›´æ–°æ­·å²
        self.previous_landmarks = optimized_data
        return optimized_data

    def _extract_frame_features(self, rgba_frame: np.ndarray, timestamp_ms: int) -> Dict:
        """æå–å–®å¹€ç‰¹å¾µ"""
        frame_data = {
            "hands": {"left": None, "right": None},
            "pose": None,
            "face": None,
            "valid": False
        }
        
        try:
            # ç¢ºä¿å½±åƒæ ¼å¼æ­£ç¢º
            rgba_frame = np.ascontiguousarray(rgba_frame)
            mp_image = mp.Image(image_format=mp.ImageFormat.SRGBA, data=rgba_frame)
            
            detection_success = False
            
            # å§¿æ…‹åµæ¸¬
            try:
                pose_results = self.pose.detect_for_video(mp_image, timestamp_ms)
                if pose_results and pose_results.pose_landmarks:
                    landmarks = pose_results.pose_landmarks[0]
                    pose_points = []
                    for idx in IMPORTANT_POSE_INDICES:
                        if idx < len(landmarks):
                            p = landmarks[idx]
                            pose_points.extend([p.x, p.y, p.z])
                        else:
                            pose_points.extend([0.0, 0.0, 0.0])
                    frame_data["pose"] = pose_points
                    detection_success = True
            except Exception as e:
                print(f"âš ï¸ å§¿æ…‹åµæ¸¬éŒ¯èª¤: {e}")
            
            # æ‰‹éƒ¨åµæ¸¬
            try:
                hand_results = self.hands.detect_for_video(mp_image, timestamp_ms)
                if hand_results and hand_results.hand_landmarks:
                    for i, landmarks in enumerate(hand_results.hand_landmarks):
                        # åˆ¤åˆ¥å·¦å³æ‰‹
                        hand_label = None
                        try:
                            if (hand_results.handedness and 
                                len(hand_results.handedness) > i and
                                hand_results.handedness[i] and 
                                hand_results.handedness[i][0].category_name):
                                hand_label = hand_results.handedness[i][0].category_name.lower()
                        except Exception:
                            pass
                        
                        # æå–é—œéµé»
                        points = []
                        for landmark in landmarks:
                            points.extend([landmark.x, landmark.y, landmark.z])
                        
                        # åˆ†é…åˆ°å·¦å³æ‰‹
                        if hand_label and hand_label.startswith("left"):
                            frame_data["hands"]["left"] = points
                        elif hand_label and hand_label.startswith("right"):
                            frame_data["hands"]["right"] = points
                        else:
                            # ç„¡æ³•åˆ¤åˆ¥æ™‚æŒ‰é †åºåˆ†é…
                            if frame_data["hands"]["left"] is None:
                                frame_data["hands"]["left"] = points
                            else:
                                frame_data["hands"]["right"] = points
                        
                        detection_success = True
            except Exception as e:
                print(f"âš ï¸ æ‰‹éƒ¨åµæ¸¬éŒ¯èª¤: {e}")
            
            # è‡‰éƒ¨åµæ¸¬
            try:
                face_results = self.face.detect_for_video(mp_image, timestamp_ms)
                if face_results and face_results.face_landmarks:
                    landmarks = face_results.face_landmarks[0]
                    face_points = []
                    for idx in IMPORTANT_FACE_INDICES:
                        if idx < len(landmarks):
                            p = landmarks[idx]
                            face_points.extend([p.x, p.y, p.z])
                        else:
                            face_points.extend([0.0, 0.0, 0.0])
                    frame_data["face"] = face_points
                    detection_success = True
            except Exception as e:
                print(f"âš ï¸ è‡‰éƒ¨åµæ¸¬éŒ¯èª¤: {e}")
            
            frame_data["valid"] = detection_success
            
        except Exception as e:
            print(f"âŒ å½±æ ¼ç‰¹å¾µæå–å¤±æ•—: {e}")
        
        # 2025å¹´è¿½è¹¤å„ªåŒ– - å¹³æ»‘è™•ç†
        if self.enable_tracking:
            frame_data = self._apply_tracking_optimization(frame_data)
        
        return frame_data
    
    def _interpolate_sequence(self, sequence: np.ndarray, target_length: int) -> np.ndarray:
        """ç·šæ€§æ’å€¼åºåˆ—åˆ°ç›®æ¨™é•·åº¦"""
        if sequence.size == 0 or target_length <= 0:
            return np.zeros((target_length, sequence.shape[1] if sequence.ndim > 1 else self.total_dim), dtype=np.float32)
        
        current_length = sequence.shape[0]
        if current_length == target_length:
            return sequence.astype(np.float32)
        
        # ä½¿ç”¨ç·šæ€§æ’å€¼
        old_indices = np.linspace(0, current_length - 1, current_length)
        new_indices = np.linspace(0, current_length - 1, target_length)
        
        interpolated = np.zeros((target_length, sequence.shape[1]), dtype=np.float32)
        for col in range(sequence.shape[1]):
            interpolated[:, col] = np.interp(new_indices, old_indices, sequence[:, col])
        
        return interpolated
    
    def _normalize_features(self, features: np.ndarray) -> np.ndarray:
        """æ­£è¦åŒ–ç‰¹å¾µ"""
        if self.normalize_method == "none":
            return features
        elif self.normalize_method == "minmax":
            # Min-Max æ­£è¦åŒ–åˆ° [0, 1]
            min_vals = np.min(features, axis=0, keepdims=True)
            max_vals = np.max(features, axis=0, keepdims=True)
            range_vals = max_vals - min_vals
            range_vals[range_vals == 0] = 1  # é¿å…é™¤é›¶
            return (features - min_vals) / range_vals
        elif self.normalize_method == "standard":
            # Z-score æ¨™æº–åŒ–
            mean_vals = np.mean(features, axis=0, keepdims=True)
            std_vals = np.std(features, axis=0, keepdims=True)
            std_vals[std_vals == 0] = 1  # é¿å…é™¤é›¶
            return (features - mean_vals) / std_vals
        else:
            # é è¨­ï¼šç°¡å–®çš„ç¯„åœæ­£è¦åŒ– [-1, 1]
            return np.clip(features * 2.0 - 1.0, -1.0, 1.0)
    
    def extract_video_features(self, video_path: Union[str, Path]) -> Optional[Dict]:
        """æå–å½±ç‰‡ç‰¹å¾µ - Kaggle Tesla P100 å„ªåŒ–ç‰ˆ"""
        video_path = Path(video_path)
        if not video_path.exists():
            print(f"âŒ å½±ç‰‡æª”æ¡ˆä¸å­˜åœ¨: {video_path}")
            return None

        cap = cv2.VideoCapture(str(video_path))
        if not cap or not cap.isOpened():
            print(f"âŒ ç„¡æ³•é–‹å•Ÿå½±ç‰‡: {video_path}")
            return None

        try:
            # GPU è¨˜æ†¶é«”æ¸…ç†
            if self.use_gpu:
                self._clear_gpu_cache()

            # é‡ç½®æ™‚é–“æˆ³è¨ˆæ•¸å™¨ä»¥è™•ç†æ–°å½±ç‰‡
            self._reset_timestamp()

            # ç‚ºäº†å¾¹åº•é¿å…æ™‚é–“æˆ³å•é¡Œï¼Œæ¯å€‹å½±ç‰‡éƒ½é‡æ–°åˆå§‹åŒ– MediaPipe
            # é€™æœƒç¨å¾®é™ä½æ€§èƒ½ï¼Œä½†ç¢ºä¿ç©©å®šæ€§
            try:
                self._init_mediapipe()
            except Exception as e:
                print(f"âš ï¸ MediaPipe é‡æ–°åˆå§‹åŒ–è­¦å‘Š: {e}")
                # ç¹¼çºŒä½¿ç”¨ç¾æœ‰å¯¦ä¾‹
            
            # å–å¾—å½±ç‰‡è³‡è¨Š
            fps = cap.get(cv2.CAP_PROP_FPS) or self.target_fps
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
            duration = frame_count / fps if fps > 0 else 0
            
            # è¨ˆç®—æ¡æ¨£æ­¥é©Ÿ
            frame_step = max(1, int(round(fps / self.target_fps)))
            
            frames_data = []
            frame_index = 0
            processed_count = 0
            batch_frames = []  # Kaggle Tesla P100 æ‰¹æ¬¡è™•ç†

            while True:
                ret = cap.grab()
                if not ret:
                    break

                # æŒ‰æ­¥é©Ÿæ¡æ¨£
                if frame_index % frame_step == 0:
                    ret, frame = cap.retrieve()
                    if not ret:
                        break

                    # è½‰æ›ç‚º RGBA
                    rgba_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA)
                    batch_frames.append(rgba_frame)

                    # æ‰¹æ¬¡è™•ç†ï¼šç•¶ç´¯ç©åˆ° BATCH_SIZE æˆ–æ˜¯æœ€å¾Œä¸€æ‰¹æ™‚è™•ç†
                    if len(batch_frames) >= BATCH_SIZE:
                        batch_results = self._process_frame_batch(batch_frames)
                        frames_data.extend(batch_results)
                        processed_count += len(batch_results)
                        batch_frames = []  # æ¸…ç©ºæ‰¹æ¬¡

                        # GPU è¨˜æ†¶é«”ç®¡ç†
                        if self.use_gpu and processed_count % 50 == 0:
                            self._clear_gpu_cache()

                frame_index += 1

            # è™•ç†å‰©é¤˜çš„å½±æ ¼
            if batch_frames:
                batch_results = self._process_frame_batch(batch_frames)
                frames_data.extend(batch_results)
                processed_count += len(batch_results)
            
            if not frames_data:
                print(f"âŒ ç„¡æ³•å¾å½±ç‰‡æå–ä»»ä½•ç‰¹å¾µ: {video_path}")
                return None
            
            # è½‰æ›ç‚ºçŸ©é™£æ ¼å¼
            feature_matrix = self._frames_to_matrix(frames_data)
            
            return {
                "video_info": {
                    "path": str(video_path),
                    "fps": fps,
                    "frame_count": frame_count,
                    "duration": duration,
                    "processed_frames": processed_count
                },
                "features": feature_matrix,
                "valid_frames": sum(1 for f in frames_data if f.get("valid", False)),
                "total_frames": len(frames_data)
            }
            
        finally:
            cap.release()
            # GPU è¨˜æ†¶é«”æœ€çµ‚æ¸…ç†
            if self.use_gpu:
                self._clear_gpu_cache()
    
    def _frames_to_matrix(self, frames_data: List[Dict]) -> np.ndarray:
        """å°‡å½±æ ¼è³‡æ–™è½‰æ›ç‚ºç‰¹å¾µçŸ©é™£"""
        if not frames_data:
            return np.zeros((self.target_frames, self.total_dim), dtype=np.float32)
        
        # æ§‹å»ºç‰¹å¾µå‘é‡
        feature_vectors = []
        for frame_data in frames_data:
            vector = []
            
            # æ‰‹éƒ¨ç‰¹å¾µ
            left_hand = frame_data["hands"]["left"] or [0.0] * (self.HAND_LANDMARKS * 3)
            right_hand = frame_data["hands"]["right"] or [0.0] * (self.HAND_LANDMARKS * 3)
            vector.extend(left_hand)
            vector.extend(right_hand)
            
            # å§¿æ…‹ç‰¹å¾µ
            pose = frame_data["pose"] or [0.0] * (len(IMPORTANT_POSE_INDICES) * 3)
            vector.extend(pose)
            
            # è‡‰éƒ¨ç‰¹å¾µ
            face = frame_data["face"] or [0.0] * (len(IMPORTANT_FACE_INDICES) * 3)
            vector.extend(face)
            
            feature_vectors.append(vector)
        
        # è½‰ç‚ºçŸ©é™£
        feature_matrix = np.array(feature_vectors, dtype=np.float32)
        
        # æ’å€¼åˆ°ç›®æ¨™é•·åº¦
        feature_matrix = self._interpolate_sequence(feature_matrix, self.target_frames)
        
        # æ­£è¦åŒ–
        feature_matrix = self._normalize_features(feature_matrix)

        return feature_matrix

    def _clear_gpu_cache(self):
        """æ¸…ç† GPU å¿«å–è¨˜æ†¶é«” - Kaggle Tesla P100 å„ªåŒ–"""
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                # å¼·åˆ¶åƒåœ¾å›æ”¶
                import gc
                gc.collect()
        except Exception:
            pass

    def _process_frame_batch(self, batch_frames: List[np.ndarray]) -> List[Dict]:
        """æ‰¹æ¬¡è™•ç†å½±æ ¼ - Kaggle Tesla P100 GPU å„ªåŒ–"""
        batch_results = []

        for rgba_frame in batch_frames:
            # ä½¿ç”¨ç®¡ç†çš„æ™‚é–“æˆ³
            timestamp_ms = self._get_next_timestamp()
            frame_features = self._extract_frame_features(rgba_frame, timestamp_ms)
            batch_results.append(frame_features)

        return batch_results
    
    def save_features(self, features_dict: Dict, output_path: Path) -> np.ndarray:
        """å„²å­˜ç‰¹å¾µåˆ°æª”æ¡ˆ"""
        output_path.parent.mkdir(parents=True, exist_ok=True)
        feature_matrix = features_dict["features"]
        np.save(str(output_path), feature_matrix)
        return feature_matrix


class OptimizedFeatureSaver:
    """å„ªåŒ–ç‰ˆç‰¹å¾µå„²å­˜å™¨"""
    
    def __init__(self,
                 target_frames: int = 100,
                 confidence_threshold: float = 0.3,
                 normalize_method: str = "standard",
                 output_root: Optional[Path] = None):
        
        self.extractor = OptimizedMpsHolisticExtractor(
            target_frames=target_frames,
            confidence_threshold=confidence_threshold,
            normalize_method=normalize_method
        )
        
        self.output_root = Path(output_root) if output_root else OUTPUT_ROOT
        self.output_root.mkdir(parents=True, exist_ok=True)
    
    def extract_and_save(self, video_path: Union[str, Path], class_name: Optional[str] = None) -> Path:
        """æå–ä¸¦å„²å­˜ç‰¹å¾µ"""
        video_path = Path(video_path)
        class_name = class_name or video_path.parent.name
        
        output_dir = self.output_root / class_name
        output_dir.mkdir(parents=True, exist_ok=True)
        output_path = output_dir / f"{video_path.stem}.npy"
        
        # è·³éå·²å­˜åœ¨çš„æª”æ¡ˆ
        if output_path.exists():
            return output_path
        
        # æå–ç‰¹å¾µ
        features_dict = self.extractor.extract_video_features(video_path)
        if features_dict is None:
            # å»ºç«‹é›¶çŸ©é™£ä¿æŒæª”æ¡ˆé…å°
            zero_matrix = np.zeros((self.extractor.target_frames, self.extractor.total_dim), dtype=np.float32)
            np.save(str(output_path), zero_matrix)
            print(f"âš ï¸ å»ºç«‹é›¶çŸ©é™£: {output_path}")
        else:
            # å„²å­˜æœ‰æ•ˆç‰¹å¾µ
            self.extractor.save_features(features_dict, output_path)
            valid_ratio = features_dict["valid_frames"] / features_dict["total_frames"] * 100
            print(f"âœ… ç‰¹å¾µå·²å„²å­˜: {output_path} (æœ‰æ•ˆ: {valid_ratio:.1f}%)")
        
        return output_path


def discover_videos(dataset_root: Path) -> Dict[str, List[str]]:
    """ç™¼ç¾å½±ç‰‡æª”æ¡ˆ"""
    videos_by_class = {}
    extensions = {".mp4", ".avi", ".mov", ".mkv", ".m4v", ".webm", ".flv"}
    extensions.update({ext.upper() for ext in extensions})
    
    if not dataset_root.exists():
        return videos_by_class
    
    for class_dir in sorted([d for d in dataset_root.iterdir() if d.is_dir()]):
        video_files = [
            str(f) for f in class_dir.iterdir() 
            if f.is_file() and f.suffix in extensions
        ]
        if video_files:
            videos_by_class[class_dir.name] = sorted(video_files)
    
    return videos_by_class


# å¤šé€²ç¨‹è™•ç†
_GLOBAL_EXTRACTOR = None

def _init_worker_process(target_frames: int, confidence_threshold: float, normalize_method: str):
    """åˆå§‹åŒ–å·¥ä½œé€²ç¨‹ - Tesla P100 å„ªåŒ–"""
    global _GLOBAL_EXTRACTOR

    # å¤šé€²ç¨‹ worker åªç”¨ CPU delegateï¼Œä¸”åªè®€æœ¬åœ°æ¨¡å‹
    use_gpu = False
    print("âš ï¸ å¤šé€²ç¨‹ç’°å¢ƒï¼Œå¼·åˆ¶ä½¿ç”¨ CPU delegate ä¸¦åªè®€æœ¬åœ°æ¨¡å‹")
    _GLOBAL_EXTRACTOR = OptimizedMpsHolisticExtractor(
        target_frames=target_frames,
        confidence_threshold=confidence_threshold,
        normalize_method=normalize_method,
        use_gpu=use_gpu
    )

def _process_video_task(task: Tuple[str, str, Path]) -> Dict:
    """è™•ç†å–®å€‹å½±ç‰‡ä»»å‹™"""
    class_name, video_path, output_root = task
    
    try:
        global _GLOBAL_EXTRACTOR
        if _GLOBAL_EXTRACTOR is None:
            _GLOBAL_EXTRACTOR = OptimizedMpsHolisticExtractor()
        
        # å»ºç«‹è¼¸å‡ºè·¯å¾‘
        output_dir = output_root / class_name
        output_dir.mkdir(parents=True, exist_ok=True)
        output_path = output_dir / f"{Path(video_path).stem}.npy"
        
        # è·³éå·²å­˜åœ¨çš„æª”æ¡ˆ
        if output_path.exists():
            return {"video": video_path, "status": "skipped", "output": str(output_path)}
        
        # æå–ç‰¹å¾µ
        features_dict = _GLOBAL_EXTRACTOR.extract_video_features(video_path)
        
        if features_dict is None:
            # å»ºç«‹é›¶çŸ©é™£
            zero_matrix = np.zeros(
                (_GLOBAL_EXTRACTOR.target_frames, _GLOBAL_EXTRACTOR.total_dim), 
                dtype=np.float32
            )
            np.save(str(output_path), zero_matrix)
            return {"video": video_path, "status": "failed_zero", "output": str(output_path)}
        
        # å„²å­˜ç‰¹å¾µ
        feature_matrix = _GLOBAL_EXTRACTOR.save_features(features_dict, output_path)
        
        return {
            "video": video_path,
            "status": "success", 
            "output": str(output_path),
            "shape": list(feature_matrix.shape),
            "valid_frames": features_dict["valid_frames"],
            "total_frames": features_dict["total_frames"]
        }
        
    except Exception as e:
        return {"video": video_path, "status": "error", "error": str(e)}


def run_optimized_dataset_extraction(
    limit_per_class: Optional[int] = None,
    parallel: bool = True,
    normalize_method: str = "standard",
    confidence_threshold: float = 0.3,
    sample_validation: int = 50
):
    """åŸ·è¡Œå„ªåŒ–ç‰ˆè³‡æ–™é›†ç‰¹å¾µæå–"""
    
    print("ğŸš€ å„ªåŒ–ç‰ˆ MediaPipe ç‰¹å¾µæå–å™¨")
    print(f"ğŸ“‚ è³‡æ–™é›†è·¯å¾‘: {DATASET_PATH}")
    print(f"ğŸ“ è¼¸å‡ºè·¯å¾‘: {OUTPUT_ROOT}")
    print(f"ğŸ¯ æ­£è¦åŒ–æ–¹æ³•: {normalize_method}")
    print(f"âš¡ ç½®ä¿¡åº¦é–¾å€¼: {confidence_threshold}")
    
    # ç™¼ç¾å½±ç‰‡
    videos_dict = discover_videos(DATASET_PATH)
    if not videos_dict:
        print("âŒ æ‰¾ä¸åˆ°ä»»ä½•å½±ç‰‡æª”æ¡ˆ")
        return
    
    # é™åˆ¶æ¯é¡åˆ¥å½±ç‰‡æ•¸é‡
    if limit_per_class:
        videos_dict = {k: v[:limit_per_class] for k, v in videos_dict.items()}
    
    total_videos = sum(len(videos) for videos in videos_dict.values())
    print(f"ğŸ“Š ç™¼ç¾ {len(videos_dict)} å€‹é¡åˆ¥ï¼Œå…± {total_videos} æ”¯å½±ç‰‡")
    
    # å»ºç«‹ä»»å‹™åˆ—è¡¨
    tasks = []
    for class_name, video_paths in videos_dict.items():
        for video_path in video_paths:
            tasks.append((class_name, video_path, OUTPUT_ROOT))
    
    # è™•ç†çµ±è¨ˆ
    results = {
        "success": 0,
        "failed": 0,
        "skipped": 0,
        "errors": 0,
        "details": [],
        "start_time": time.time()
    }
    
    if not parallel:
        # å–®é€²ç¨‹è™•ç†
        extractor = OptimizedFeatureSaver(
            target_frames=TARGET_FRAMES,
            confidence_threshold=confidence_threshold,
            normalize_method=normalize_method,
            output_root=OUTPUT_ROOT
        )
        
        with tqdm(total=total_videos, desc="è™•ç†å½±ç‰‡", unit="æ”¯") as pbar:
            for class_name, video_path, _ in tasks:
                try:
                    output_path = extractor.extract_and_save(video_path, class_name)
                    results["success"] += 1
                    results["details"].append({
                        "video": video_path,
                        "status": "success",
                        "output": str(output_path)
                    })
                except Exception as e:
                    results["errors"] += 1
                    results["details"].append({
                        "video": video_path, 
                        "status": "error",
                        "error": str(e)
                    })
                
                pbar.update(1)
                pbar.set_postfix({
                    "æˆåŠŸ": results["success"],
                    "éŒ¯èª¤": results["errors"]
                })
    
    else:
        # å¤šé€²ç¨‹è™•ç†
        print(f"ğŸš€ å•Ÿç”¨å¤šé€²ç¨‹è™•ç† (workers={NUM_WORKERS})")
        
        with ProcessPoolExecutor(
            max_workers=NUM_WORKERS,
            initializer=_init_worker_process,
            initargs=(TARGET_FRAMES, confidence_threshold, normalize_method)
        ) as executor:
            
            with tqdm(total=total_videos, desc="è™•ç†å½±ç‰‡", unit="æ”¯") as pbar:
                for result in executor.map(_process_video_task, tasks, chunksize=CHUNK_SIZE):
                    results["details"].append(result)
                    
                    status = result.get("status", "error")
                    if status == "success":
                        results["success"] += 1
                    elif status == "skipped":
                        results["skipped"] += 1
                    elif status in ("failed", "failed_zero"):
                        results["failed"] += 1
                    else:
                        results["errors"] += 1
                    
                    pbar.update(1)
                    pbar.set_postfix({
                        "æˆåŠŸ": results["success"],
                        "è·³é": results["skipped"], 
                        "å¤±æ•—": results["failed"],
                        "éŒ¯èª¤": results["errors"]
                    })
    
    # å®Œæˆè™•ç†
    results["end_time"] = time.time()
    results["total_time"] = results["end_time"] - results["start_time"]
    
    # å„²å­˜è™•ç†å ±å‘Š
    OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)
    report_path = OUTPUT_ROOT / "optimized_processing_report.json"
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # é©—è­‰æŠ½æ¨£
    if sample_validation > 0:
        validation_results = validate_extracted_features(sample_validation)
        validation_path = OUTPUT_ROOT / "optimized_validation_report.json"
        with open(validation_path, "w", encoding="utf-8") as f:
            json.dump(validation_results, f, indent=2, ensure_ascii=False)
    
    # é¡¯ç¤ºçµæœ
    print("\nğŸ“ˆ è™•ç†å®Œæˆï¼")
    print(f"â±ï¸  ç¸½è€—æ™‚: {results['total_time']/60:.1f} åˆ†é˜")
    print(f"âœ… æˆåŠŸ: {results['success']}")
    print(f"â­ï¸  è·³é: {results['skipped']}")
    print(f"âŒ å¤±æ•—: {results['failed']}")
    print(f"ğŸš« éŒ¯èª¤: {results['errors']}")
    
    return results


def validate_extracted_features(sample_size: int = 50) -> Dict:
    """é©—è­‰å·²æå–çš„ç‰¹å¾µæª”æ¡ˆ"""
    print(f"ğŸ” é©—è­‰ç‰¹å¾µæª”æ¡ˆ (æŠ½æ¨£: {sample_size})")
    
    # æ”¶é›†æ‰€æœ‰ .npy æª”æ¡ˆ
    all_feature_files = []
    for class_dir in OUTPUT_ROOT.iterdir():
        if class_dir.is_dir():
            all_feature_files.extend(list(class_dir.glob("*.npy")))
    
    if not all_feature_files:
        return {"error": "æ‰¾ä¸åˆ°ä»»ä½•ç‰¹å¾µæª”æ¡ˆ"}
    
    # éš¨æ©ŸæŠ½æ¨£
    sample_files = random.sample(all_feature_files, min(sample_size, len(all_feature_files)))
    
    validation_results = {
        "total_files": len(all_feature_files),
        "sampled_files": len(sample_files),
        "shapes": [],
        "statistics": {},
        "issues": []
    }
    
    for file_path in sample_files:
        try:
            features = np.load(file_path)
            shape = features.shape
            validation_results["shapes"].append({
                "file": str(file_path.relative_to(OUTPUT_ROOT)),
                "shape": list(shape)
            })
            
            # çµ±è¨ˆåˆ†æ
            if len(shape) == 2:
                mean_val = float(np.mean(features))
                std_val = float(np.std(features))
                min_val = float(np.min(features))
                max_val = float(np.max(features))
                zero_ratio = float(np.sum(features == 0) / features.size)
                
                validation_results["statistics"][str(file_path.relative_to(OUTPUT_ROOT))] = {
                    "mean": mean_val,
                    "std": std_val,
                    "min": min_val,
                    "max": max_val,
                    "zero_ratio": zero_ratio
                }
                
                # æª¢æŸ¥å•é¡Œ
                if zero_ratio > 0.9:
                    validation_results["issues"].append({
                        "file": str(file_path.relative_to(OUTPUT_ROOT)),
                        "issue": "è¶…é90%çš„å€¼ç‚ºé›¶"
                    })
                
                if std_val < 1e-6:
                    validation_results["issues"].append({
                        "file": str(file_path.relative_to(OUTPUT_ROOT)),
                        "issue": "ç‰¹å¾µè®Šç•°åº¦éä½"
                    })
        
        except Exception as e:
            validation_results["issues"].append({
                "file": str(file_path.relative_to(OUTPUT_ROOT)),
                "issue": f"è®€å–éŒ¯èª¤: {str(e)}"
            })
    
    print(f"âœ… é©—è­‰å®Œæˆ: {len(sample_files)} å€‹æª”æ¡ˆ")
    if validation_results["issues"]:
        print(f"âš ï¸  ç™¼ç¾ {len(validation_results['issues'])} å€‹å•é¡Œ")
    
    return validation_results


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¬ å„ªåŒ–ç‰ˆ MediaPipe ç‰¹å¾µæå–å™¨")
    # å…ˆç¢ºä¿æ¨¡å‹æª”æ¡ˆå·²ä¸‹è¼‰å¥½ï¼ˆåªåœ¨ä¸»é€²ç¨‹ä¸‹è¼‰ï¼‰
    ensure_models_ready(force_download=False)
    # åŸ·è¡Œç‰¹å¾µæå–
    results = run_optimized_dataset_extraction(
        limit_per_class=None,
        parallel=True,
        normalize_method="standard",
        confidence_threshold=0.3,
        sample_validation=50
    )
    return results


if __name__ == "__main__":
    main()