#!/usr/bin/env python3
import os
import sys
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
import time
from tqdm import tqdm
import json
import numpy as np
import zipfile
import random
import cv2
from typing import Dict, List, Optional, Tuple
from pathlib import Path as _PathAlias  # alias to avoid confusion with typing
import hashlib
from pathlib import Path as _P

# é™åˆ¶å¤šåŸ·è¡Œç·’ï¼Œé¿å…èˆ‡å¤šé€²ç¨‹äº’æ¶ CPU
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
try:
    cv2.setNumThreads(0)
except Exception:
    pass

PROJECT_ROOT = _P(__file__).parent
# æœ¬åœ°è³‡æ–™é›†èˆ‡è¼¸å‡ºè·¯å¾‘ï¼ˆèˆ‡è¨“ç·´è…³æœ¬ä¸€è‡´ï¼‰
DATASET_PATH = PROJECT_ROOT / "bai_datasets"
OUTPUT_ROOT = PROJECT_ROOT / "features" / "optical_flow_features"
# ä¿å®ˆ worker æ•¸ï¼Œé¿å…æœ¬æ©Ÿè³‡æºçˆ­ç”¨
NUM_WORKERS = 2
TARGET_FRAMES_DEFAULT = 100


class OpticalFlowExtractor:
    def __init__(self, 
                 target_frames: int = 100,
                 flow_method: str = 'farneback',
                 resize_dims: Tuple[int, int] = (224, 224),
                 confidence_threshold: float = 0.5,
                 roi_mode: str = 'dynamic',
                 dynamic_roi_params: Optional[Dict[str, float]] = None):
        """
        å…‰æµé‹å‹•ç‰¹å¾µæå–å™¨
        
        Args:
            target_frames: ç›®æ¨™å¹€æ•¸ï¼ˆç·šæ€§æ’å€¼çš„çµ±ä¸€é•·åº¦ï¼‰
            flow_method: å…‰æµè¨ˆç®—æ–¹æ³• ('farneback', 'lucas_kanade', 'tvl1')
            resize_dims: å½±åƒç¸®æ”¾å°ºå¯¸
            confidence_threshold: ç½®ä¿¡åº¦é–¾å€¼
        """
        self.target_frames = target_frames
        self.flow_method = flow_method
        self.resize_dims = resize_dims
        self.confidence_threshold = confidence_threshold
        self.roi_mode = roi_mode  # 'dynamic' or 'static'
        
        # å…‰æµåƒæ•¸é…ç½®
        self.farneback_params = {
            'pyr_scale': 0.5,
            'levels': 3,
            'winsize': 15,
            'iterations': 3,
            'poly_n': 5,
            'poly_sigma': 1.2,
            'flags': 0
        }
        
        self.lucas_kanade_params = {
            'winSize': (15, 15),
            'maxLevel': 2,
            'criteria': (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)
        }
        
        # ROIå€åŸŸå®šç¾© (æ‰‹èªé—œæ³¨å€åŸŸ)
        self.roi_regions = {
            'hands': [(0.1, 0.3, 0.9, 0.9)],  # æ‰‹éƒ¨å€åŸŸ (x1, y1, x2, y2)
            'upper_body': [(0.2, 0.1, 0.8, 0.7)],  # ä¸ŠåŠèº«
            'face': [(0.3, 0.0, 0.7, 0.4)],  # é¢éƒ¨è¡¨æƒ…
            'full_frame': [(0.0, 0.0, 1.0, 1.0)]  # å…¨ç•«é¢
        }
        # å‹•æ…‹ ROI åƒæ•¸èˆ‡ç‹€æ…‹
        self.dynamic_roi_params = dynamic_roi_params or {
            'mag_percentile': 85.0,       # ä»¥å¹…åº¦åˆ†ä½æ•¸æ±ºå®šå‹•ä½œå€åŸŸ
            'min_ratio_pixels': 0.0025,   # æœ€å°åƒç´ æ¯”ä¾‹ï¼Œå¤ªå°‘å‰‡å¿½ç•¥
            'expand_ratio': 0.15,         # é‚Šç•Œæ“´å¼µæ¯”ä¾‹
            'ema_alpha': 0.3,             # ROI å¹³æ»‘ä¿‚æ•¸
            'min_points': 10              # LK æœ€å°‘é»æ•¸é–€æª»
        }
        self._roi_bbox_norm: Optional[Tuple[float, float, float, float]] = None
        
        print(f"âœ… OpticalFlowExtractor åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ¯ å…‰æµæ–¹æ³•: {flow_method}")
        print(f"ğŸ“ ç›®æ¨™å°ºå¯¸: {resize_dims}")
        print(f"ğŸ¬ ç›®æ¨™å¹€æ•¸: {target_frames}")
        print(f"ğŸ“¦ ROI æ¨¡å¼: {self.roi_mode}")

    def extract_video_features(self, video_path: str) -> Optional[Dict]:
        """å¾å½±ç‰‡ä¸­æå–å…‰æµç‰¹å¾µ"""
        if not os.path.exists(video_path):
            return None
            
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return None
            
        # ç²å–å½±ç‰‡è³‡è¨Š
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps is None or fps <= 1e-6 or not np.isfinite(fps):
            fps = 30.0
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
        duration = (frame_count / fps) if fps else 0.0
        
        features = {
            'video_info': {
                'path': video_path,
                'fps': fps,
                'frame_count': frame_count,
                'duration': duration,
                'flow_method': self.flow_method,
                'roi_mode': self.roi_mode
            },
            'frames_data': []
        }
        
        # ä¾ç›®æ¨™å¹€æ•¸å–æ¨£ï¼Œä¸²æµæ–¹å¼å³æ™‚è¨ˆç®—å…‰æµï¼Œé™ä½è¨˜æ†¶é«”ç”¨é‡
        step = max(1, int(round(frame_count / float(self.target_frames)))) if frame_count > 0 else 1
        prev_gray: Optional[np.ndarray] = None
        processed_idx = 0
        flow_sequence: List[Dict] = []

        frame_idx = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            if (frame_idx % step) != 0:
                frame_idx += 1
                continue

            resized_frame = cv2.resize(frame, self.resize_dims)
            gray_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2GRAY)

            if prev_gray is not None:
                # è¨ˆç®—å…‰æµç‰¹å¾µï¼ˆä¾æ–¹æ³•åˆ†æ´¾ï¼‰
                if self.flow_method == 'farneback':
                    fb = self.farneback_params
                    flow = cv2.calcOpticalFlowFarneback(
                        prev_gray, gray_frame, None,
                        fb['pyr_scale'], fb['levels'], fb['winsize'],
                        fb['iterations'], fb['poly_n'], fb['poly_sigma'], fb['flags']
                    )
                    # è‹¥ä½¿ç”¨å‹•æ…‹ ROIï¼Œå…ˆæ›´æ–° ROI bbox å†æŠ½ç‰¹å¾µ
                    if self.roi_mode == 'dynamic':
                        self._update_dynamic_roi_from_flow(flow)
                        flow_features = self._extract_farneback_features(flow, processed_idx - 1, use_dynamic_roi=True)
                    else:
                        flow_features = self._extract_farneback_features(flow, processed_idx - 1)
                elif self.flow_method == 'lucas_kanade':
                    flow_features = self._extract_lucas_kanade_features(prev_gray, gray_frame, processed_idx - 1)
                elif self.flow_method == 'tvl1':
                    try:
                        flow_features = self._extract_tvl1_features(prev_gray, gray_frame, processed_idx - 1)
                    except Exception:
                        # æ²’æœ‰ contrib æˆ–å»ºç«‹å¤±æ•—æ™‚é€€å› Farneback
                        fb = self.farneback_params
                        flow = cv2.calcOpticalFlowFarneback(
                            prev_gray, gray_frame, None,
                            fb['pyr_scale'], fb['levels'], fb['winsize'],
                            fb['iterations'], fb['poly_n'], fb['poly_sigma'], fb['flags']
                        )
                        if self.roi_mode == 'dynamic':
                            self._update_dynamic_roi_from_flow(flow)
                            flow_features = self._extract_farneback_features(flow, processed_idx - 1, use_dynamic_roi=True)
                        else:
                            flow_features = self._extract_farneback_features(flow, processed_idx - 1)
                else:
                    # é è¨­ Farneback
                    fb = self.farneback_params
                    flow = cv2.calcOpticalFlowFarneback(
                        prev_gray, gray_frame, None,
                        fb['pyr_scale'], fb['levels'], fb['winsize'],
                        fb['iterations'], fb['poly_n'], fb['poly_sigma'], fb['flags']
                    )
                    if self.roi_mode == 'dynamic':
                        self._update_dynamic_roi_from_flow(flow)
                        flow_features = self._extract_farneback_features(flow, processed_idx - 1, use_dynamic_roi=True)
                    else:
                        flow_features = self._extract_farneback_features(flow, processed_idx - 1)

                if flow_features is not None:
                    flow_sequence.append(flow_features)

            prev_gray = gray_frame
            processed_idx += 1
            frame_idx += 1

        cap.release()

        if len(flow_sequence) < 1:
            return None

        features['frames_data'] = flow_sequence
        
        # æ‡‰ç”¨ç·šæ€§æ’å€¼é€²è¡Œæ™‚åºå°é½Š
        aligned_features = self._apply_temporal_interpolation(features)
        
        return aligned_features

    def _compute_optical_flow_sequence(self, frames: List[np.ndarray]) -> List[Dict]:
        """è¨ˆç®—å…‰æµåºåˆ—ï¼ˆä¿ç•™ä»¥å…¼å®¹ï¼Œä½†é è¨­ä¸ä½¿ç”¨å…¨é‡è¼‰å…¥ï¼‰"""
        flow_sequence = []
        for i in range(len(frames) - 1):
            prev_frame = frames[i]
            curr_frame = frames[i + 1]

            if self.flow_method == 'farneback':
                fb = self.farneback_params
                flow = cv2.calcOpticalFlowFarneback(
                    prev_frame, curr_frame, None,
                    fb['pyr_scale'], fb['levels'], fb['winsize'],
                    fb['iterations'], fb['poly_n'], fb['poly_sigma'], fb['flags']
                )
                flow_features = self._extract_farneback_features(flow, i)
            elif self.flow_method == 'lucas_kanade':
                flow_features = self._extract_lucas_kanade_features(prev_frame, curr_frame, i)
            elif self.flow_method == 'tvl1':
                try:
                    flow_features = self._extract_tvl1_features(prev_frame, curr_frame, i)
                except Exception:
                    fb = self.farneback_params
                    flow = cv2.calcOpticalFlowFarneback(
                        prev_frame, curr_frame, None,
                        fb['pyr_scale'], fb['levels'], fb['winsize'],
                        fb['iterations'], fb['poly_n'], fb['poly_sigma'], fb['flags']
                    )
                    flow_features = self._extract_farneback_features(flow, i)
            else:
                fb = self.farneback_params
                flow = cv2.calcOpticalFlowFarneback(
                    prev_frame, curr_frame, None,
                    fb['pyr_scale'], fb['levels'], fb['winsize'],
                    fb['iterations'], fb['poly_n'], fb['poly_sigma'], fb['flags']
                )
                flow_features = self._extract_farneback_features(flow, i)

            if flow_features is not None:
                flow_sequence.append(flow_features)

        return flow_sequence

    def _extract_farneback_features(self, flow: np.ndarray, frame_idx: int, use_dynamic_roi: bool = False) -> Optional[Dict]:
        """æå–Farnebackå…‰æµç‰¹å¾µ"""
        if flow is None:
            return None
            
        # è¨ˆç®—å…‰æµçš„å¹…åº¦å’Œè§’åº¦
        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        
        # æå–ROIå€åŸŸç‰¹å¾µ
        roi_features = {}
        
        if use_dynamic_roi and self._roi_bbox_norm is not None:
            # ä»¥å‹•æ…‹ ROI ç‚º hands å€åŸŸï¼Œå…¶é¤˜æ²¿ç”¨éœæ…‹
            dyn_roi = self._roi_bbox_norm
            roi_features['hands'] = self._extract_roi_flow_features(magnitude, angle, dyn_roi)
            for roi_name, roi_coords in self.roi_regions.items():
                if roi_name == 'hands':
                    continue
                roi_data = self._extract_roi_flow_features(magnitude, angle, roi_coords[0])
                roi_features[roi_name] = roi_data
        else:
            for roi_name, roi_coords in self.roi_regions.items():
                roi_data = self._extract_roi_flow_features(magnitude, angle, roi_coords[0])
                roi_features[roi_name] = roi_data
        
        # å…¨å±€çµ±è¨ˆç‰¹å¾µ
        global_features = {
            'mean_magnitude': float(np.mean(magnitude)),
            'std_magnitude': float(np.std(magnitude)),
            'max_magnitude': float(np.max(magnitude)),
            'mean_angle': float(np.mean(angle)),
            'std_angle': float(np.std(angle))
        }
        
        return {
            'frame_idx': frame_idx,
            'flow_method': 'farneback',
            'roi_features': roi_features,
            'global_features': global_features,
            'flow_shape': flow.shape[:2]
        }

    def _update_dynamic_roi_from_flow(self, flow: np.ndarray):
        """æ ¹æ“šç•¶å‰å…‰æµæ›´æ–°å‹•æ…‹ ROIï¼ˆä¸ä¾è³´é—œéµé»ï¼‰"""
        if flow is None or flow.size == 0:
            return
        mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        h, w = mag.shape
        if h == 0 or w == 0:
            return

        params = self.dynamic_roi_params
        # ä»¥åˆ†ä½æ•¸åšé–€æª»
        thresh = np.percentile(mag, params['mag_percentile'])
        motion_mask = mag >= thresh

        # ä¿éšœæœ€å°åƒç´ æ¯”ä¾‹
        min_pixels = max(1, int(params['min_ratio_pixels'] * h * w))
        if motion_mask.sum() < min_pixels:
            # æ²’æœ‰è¶³å¤ å‹•ä½œï¼Œä¸æ›´æ–°ï¼ˆä¿æŒèˆŠ ROIï¼‰
            return

        ys, xs = np.where(motion_mask)
        x1, x2 = xs.min(), xs.max()
        y1, y2 = ys.min(), ys.max()

        # é‚Šç•Œæ“´å¼µ
        expand_x = int(params['expand_ratio'] * (x2 - x1 + 1))
        expand_y = int(params['expand_ratio'] * (y2 - y1 + 1))
        x1 = max(0, x1 - expand_x)
        y1 = max(0, y1 - expand_y)
        x2 = min(w - 1, x2 + expand_x)
        y2 = min(h - 1, y2 + expand_y)

        # è½‰ç‚ºç›¸å°åº§æ¨™
        new_roi = (
            float(x1) / float(w),
            float(y1) / float(h),
            float(x2 + 1) / float(w),
            float(y2 + 1) / float(h)
        )

        # æŒ‡æ•¸ç§»å‹•å¹³å‡ï¼Œå¹³æ»‘ ROI
        if self._roi_bbox_norm is None:
            self._roi_bbox_norm = new_roi
        else:
            alpha = params['ema_alpha']
            old = self._roi_bbox_norm
            self._roi_bbox_norm = (
                old[0] * (1 - alpha) + new_roi[0] * alpha,
                old[1] * (1 - alpha) + new_roi[1] * alpha,
                old[2] * (1 - alpha) + new_roi[2] * alpha,
                old[3] * (1 - alpha) + new_roi[3] * alpha,
            )

    def _extract_lucas_kanade_features(self, prev_frame: np.ndarray, curr_frame: np.ndarray, frame_idx: int) -> Optional[Dict]:
        """æå–Lucas-Kanadeå…‰æµç‰¹å¾µ"""
        # æª¢æ¸¬ç‰¹å¾µé»
        corners = cv2.goodFeaturesToTrack(
            prev_frame, 
            maxCorners=200,
            qualityLevel=0.3,
            minDistance=7,
            blockSize=7
        )
        
        if corners is None or len(corners) == 0:
            return self._create_empty_lk_features(frame_idx)
        
        # è¨ˆç®—å…‰æµ
        next_points, status, error = cv2.calcOpticalFlowPyrLK(
            prev_frame, curr_frame, corners, None, **self.lucas_kanade_params
        )
        
        # é¸æ“‡å¥½çš„é»ï¼Œä¸¦æ•´ç†ç‚º (N, 2)
        good_old = corners[status == 1].reshape(-1, 2)
        good_new = next_points[status == 1].reshape(-1, 2)
        
        if len(good_old) == 0:
            return self._create_empty_lk_features(frame_idx)
        
        # è¨ˆç®—é‹å‹•å‘é‡
        motion_vectors = good_new - good_old
        
        # æå–ROIå€åŸŸç‰¹å¾µ
        roi_features = {}
        for roi_name, roi_coords in self.roi_regions.items():
            roi_data = self._extract_roi_point_features(good_old, motion_vectors, roi_coords[0])
            roi_features[roi_name] = roi_data
        
        # å…¨å±€çµ±è¨ˆç‰¹å¾µ
        magnitudes = np.sqrt(motion_vectors[:, 0]**2 + motion_vectors[:, 1]**2)
        angles = np.arctan2(motion_vectors[:, 1], motion_vectors[:, 0])
        
        global_features = {
            'mean_magnitude': float(np.mean(magnitudes)),
            'std_magnitude': float(np.std(magnitudes)),
            'max_magnitude': float(np.max(magnitudes)),
            'mean_angle': float(np.mean(angles)),
            'std_angle': float(np.std(angles)),
            'num_points': len(good_old)
        }
        
        return {
            'frame_idx': frame_idx,
            'flow_method': 'lucas_kanade',
            'roi_features': roi_features,
            'global_features': global_features,
            'num_tracked_points': len(good_old)
        }

    def _extract_tvl1_features(self, prev_frame: np.ndarray, curr_frame: np.ndarray, frame_idx: int) -> Optional[Dict]:
        """æå–TV-L1å…‰æµç‰¹å¾µ"""
        # å‰µå»ºTV-L1å…‰æµæª¢æ¸¬å™¨
        if not hasattr(cv2, 'optflow'):
            raise RuntimeError('opencv-contrib optflow æ¨¡çµ„ä¸å¯ç”¨')
        tvl1 = cv2.optflow.DualTVL1OpticalFlow_create()
        
        # è¨ˆç®—å…‰æµ
        flow = tvl1.calc(prev_frame, curr_frame, None)
        
        if flow is None:
            return None
        
        # è¨ˆç®—å…‰æµçš„å¹…åº¦å’Œè§’åº¦
        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        
        # æå–ROIå€åŸŸç‰¹å¾µ
        roi_features = {}
        for roi_name, roi_coords in self.roi_regions.items():
            roi_data = self._extract_roi_flow_features(magnitude, angle, roi_coords[0])
            roi_features[roi_name] = roi_data
        
        # å…¨å±€çµ±è¨ˆç‰¹å¾µ
        global_features = {
            'mean_magnitude': float(np.mean(magnitude)),
            'std_magnitude': float(np.std(magnitude)),
            'max_magnitude': float(np.max(magnitude)),
            'mean_angle': float(np.mean(angle)),
            'std_angle': float(np.std(angle))
        }
        
        return {
            'frame_idx': frame_idx,
            'flow_method': 'tvl1',
            'roi_features': roi_features,
            'global_features': global_features,
            'flow_shape': flow.shape[:2]
        }

    def _extract_roi_flow_features(self, magnitude: np.ndarray, angle: np.ndarray, roi_coords: Tuple[float, float, float, float]) -> Dict:
        """æå–ROIå€åŸŸçš„å…‰æµç‰¹å¾µ"""
        x1, y1, x2, y2 = roi_coords
        h, w = magnitude.shape
        
        # è½‰æ›ç‚ºåƒç´ åº§æ¨™
        x1_px = int(x1 * w)
        y1_px = int(y1 * h)
        x2_px = int(x2 * w)
        y2_px = int(y2 * h)
        
        # è£åˆ‡ROIå€åŸŸ
        roi_mag = magnitude[y1_px:y2_px, x1_px:x2_px]
        roi_ang = angle[y1_px:y2_px, x1_px:x2_px]
        
        if roi_mag.size == 0:
            return {
                'mean_magnitude': 0.0,
                'std_magnitude': 0.0,
                'max_magnitude': 0.0,
                'mean_angle': 0.0,
                'std_angle': 0.0,
                'histogram_mag': [0.0] * 8,
                'histogram_ang': [0.0] * 8
            }
        
        # çµ±è¨ˆç‰¹å¾µ
        mean_mag = float(np.mean(roi_mag))
        std_mag = float(np.std(roi_mag))
        max_mag = float(np.max(roi_mag))
        mean_ang = float(np.mean(roi_ang))
        std_ang = float(np.std(roi_ang))
        
        # ç›´æ–¹åœ–ç‰¹å¾µ
        hist_mag, _ = np.histogram(roi_mag.flatten(), bins=8, range=(0, np.max(roi_mag) + 1e-6))
        hist_ang, _ = np.histogram(roi_ang.flatten(), bins=8, range=(0, 2*np.pi))
        
        return {
            'mean_magnitude': mean_mag,
            'std_magnitude': std_mag,
            'max_magnitude': max_mag,
            'mean_angle': mean_ang,
            'std_angle': std_ang,
            'histogram_mag': hist_mag.tolist(),
            'histogram_ang': hist_ang.tolist()
        }

    def _extract_roi_point_features(self, points: np.ndarray, motion_vectors: np.ndarray, roi_coords: Tuple[float, float, float, float]) -> Dict:
        """æå–ROIå€åŸŸçš„é»ç‰¹å¾µ"""
        x1, y1, x2, y2 = roi_coords
        w, h = self.resize_dims
        
        # è½‰æ›ç‚ºåƒç´ åº§æ¨™
        x1_px = x1 * w
        y1_px = y1 * h
        x2_px = x2 * w
        y2_px = y2 * h
        
        # æ‰¾å‡ºåœ¨ROIå…§çš„é»
        roi_mask = (
            (points[:, 0] >= x1_px) & (points[:, 0] <= x2_px) &
            (points[:, 1] >= y1_px) & (points[:, 1] <= y2_px)
        )
        
        roi_points = points[roi_mask]
        roi_vectors = motion_vectors[roi_mask]
        
        if len(roi_vectors) == 0:
            return {
                'mean_magnitude': 0.0,
                'std_magnitude': 0.0,
                'max_magnitude': 0.0,
                'mean_angle': 0.0,
                'std_angle': 0.0,
                'num_points': 0
            }
        
        # è¨ˆç®—çµ±è¨ˆç‰¹å¾µ
        magnitudes = np.sqrt(roi_vectors[:, 0]**2 + roi_vectors[:, 1]**2)
        angles = np.arctan2(roi_vectors[:, 1], roi_vectors[:, 0])
        
        return {
            'mean_magnitude': float(np.mean(magnitudes)),
            'std_magnitude': float(np.std(magnitudes)),
            'max_magnitude': float(np.max(magnitudes)),
            'mean_angle': float(np.mean(angles)),
            'std_angle': float(np.std(angles)),
            'num_points': len(roi_vectors)
        }

    def _create_empty_lk_features(self, frame_idx: int) -> Dict:
        """å‰µå»ºç©ºçš„Lucas-Kanadeç‰¹å¾µ"""
        empty_roi = {
            'mean_magnitude': 0.0,
            'std_magnitude': 0.0,
            'max_magnitude': 0.0,
            'mean_angle': 0.0,
            'std_angle': 0.0,
            'num_points': 0
        }
        
        roi_features = {roi_name: empty_roi.copy() for roi_name in self.roi_regions.keys()}
        
        return {
            'frame_idx': frame_idx,
            'flow_method': 'lucas_kanade',
            'roi_features': roi_features,
            'global_features': empty_roi.copy(),
            'num_tracked_points': 0
        }

    def _apply_temporal_interpolation(self, features: Dict) -> Dict:
        """æ‡‰ç”¨ç·šæ€§æ’å€¼é€²è¡Œæ™‚åºå°é½Š"""
        frames_data = features['frames_data']
        if len(frames_data) == 0:
            return features
        
        target_length = self.target_frames
        original_length = len(frames_data)
        
        if original_length == target_length:
            return features
        
        # æº–å‚™æ’å€¼æ•¸æ“š
        interpolated_data = []
        
        # æå–æ‰€æœ‰ç‰¹å¾µåºåˆ—
        feature_sequences = self._extract_feature_sequences(frames_data)
        
        # åŸ·è¡Œç·šæ€§æ’å€¼
        if original_length >= 2:
            original_indices = np.linspace(0, original_length - 1, original_length)
            target_indices = np.linspace(0, original_length - 1, target_length)
            
            interpolated_sequences = {}
            for feature_name, sequence in feature_sequences.items():
                if len(sequence) > 0:
                    interpolated_sequences[feature_name] = np.interp(target_indices, original_indices, sequence)
                else:
                    interpolated_sequences[feature_name] = np.zeros(target_length)
        else:
            # å¦‚æœåªæœ‰ä¸€å€‹æ•¸æ“šé»ï¼Œç›´æ¥è¤‡è£½
            interpolated_sequences = {}
            for feature_name, sequence in feature_sequences.items():
                if len(sequence) > 0:
                    interpolated_sequences[feature_name] = np.full(target_length, sequence[0])
                else:
                    interpolated_sequences[feature_name] = np.zeros(target_length)
        
        # é‡çµ„æ•¸æ“š
        for i in range(target_length):
            frame_data = self._reconstruct_frame_data(interpolated_sequences, i, frames_data[0])
            interpolated_data.append(frame_data)
        
        # æ›´æ–°ç‰¹å¾µæ•¸æ“š
        features['frames_data'] = interpolated_data
        features['interpolated'] = True
        features['target_frames'] = target_length
        features['original_frames'] = original_length
        
        return features

    def _extract_feature_sequences(self, frames_data: List[Dict]) -> Dict[str, List[float]]:
        """æå–æ‰€æœ‰ç‰¹å¾µåºåˆ—ç”¨æ–¼æ’å€¼"""
        sequences = {}
        
        # å…¨å±€ç‰¹å¾µ
        for global_key in ['mean_magnitude', 'std_magnitude', 'max_magnitude', 'mean_angle', 'std_angle']:
            sequences[f'global_{global_key}'] = [
                frame_data.get('global_features', {}).get(global_key, 0.0) 
                for frame_data in frames_data
            ]
        
        # ROIç‰¹å¾µ
        for roi_name in self.roi_regions.keys():
            for feature_key in ['mean_magnitude', 'std_magnitude', 'max_magnitude', 'mean_angle', 'std_angle']:
                seq_key = f'roi_{roi_name}_{feature_key}'
                sequences[seq_key] = [
                    frame_data.get('roi_features', {}).get(roi_name, {}).get(feature_key, 0.0)
                    for frame_data in frames_data
                ]
            
            # ç›´æ–¹åœ–ç‰¹å¾µ
            for hist_type in ['histogram_mag', 'histogram_ang']:
                for bin_idx in range(8):
                    seq_key = f'roi_{roi_name}_{hist_type}_bin_{bin_idx}'
                    sequences[seq_key] = [
                        frame_data.get('roi_features', {}).get(roi_name, {}).get(hist_type, [0.0]*8)[bin_idx]
                        for frame_data in frames_data
                    ]
        
        return sequences

    def _reconstruct_frame_data(self, interpolated_sequences: Dict[str, np.ndarray], frame_idx: int, template_frame: Dict) -> Dict:
        """é‡æ§‹æ’å€¼å¾Œçš„å¹€æ•¸æ“š"""
        frame_data = {
            'frame_idx': frame_idx,
            'flow_method': template_frame.get('flow_method', self.flow_method),
            'global_features': {},
            'roi_features': {}
        }
        
        # é‡æ§‹å…¨å±€ç‰¹å¾µ
        for global_key in ['mean_magnitude', 'std_magnitude', 'max_magnitude', 'mean_angle', 'std_angle']:
            seq_key = f'global_{global_key}'
            if seq_key in interpolated_sequences:
                frame_data['global_features'][global_key] = float(interpolated_sequences[seq_key][frame_idx])
        
        # é‡æ§‹ROIç‰¹å¾µ
        for roi_name in self.roi_regions.keys():
            frame_data['roi_features'][roi_name] = {}
            
            # åŸºæœ¬çµ±è¨ˆç‰¹å¾µ
            for feature_key in ['mean_magnitude', 'std_magnitude', 'max_magnitude', 'mean_angle', 'std_angle']:
                seq_key = f'roi_{roi_name}_{feature_key}'
                if seq_key in interpolated_sequences:
                    frame_data['roi_features'][roi_name][feature_key] = float(interpolated_sequences[seq_key][frame_idx])
            
            # ç›´æ–¹åœ–ç‰¹å¾µ
            for hist_type in ['histogram_mag', 'histogram_ang']:
                histogram = []
                for bin_idx in range(8):
                    seq_key = f'roi_{roi_name}_{hist_type}_bin_{bin_idx}'
                    if seq_key in interpolated_sequences:
                        histogram.append(float(interpolated_sequences[seq_key][frame_idx]))
                    else:
                        histogram.append(0.0)
                frame_data['roi_features'][roi_name][hist_type] = histogram
        
        return frame_data

    def extract_normalized_features(self, features: Dict) -> np.ndarray:
        """æå–ä¸¦æ¨™æº–åŒ–ç‰¹å¾µå‘é‡"""
        if not features or not features['frames_data']:
            return np.array([])
        
        frames_data = features['frames_data']
        num_frames = len(frames_data)
        
        # è¨ˆç®—ç‰¹å¾µç¶­åº¦
        feature_dim = self._calculate_feature_dimension()
        
        # åˆå§‹åŒ–ç‰¹å¾µçŸ©é™£
        feature_matrix = np.zeros((num_frames, feature_dim))
        
        for i, frame_data in enumerate(frames_data):
            feature_vector = self._extract_frame_feature_vector(frame_data)
            feature_matrix[i] = feature_vector
        
        # æ¨™æº–åŒ–è™•ç†
        feature_matrix = self._normalize_features(feature_matrix)
        
        return feature_matrix

    def _calculate_feature_dimension(self) -> int:
        """è¨ˆç®—ç‰¹å¾µç¶­åº¦"""
        # å…¨å±€ç‰¹å¾µç¶­åº¦: 5å€‹çµ±è¨ˆç‰¹å¾µ
        global_dim = 5
        
        # æ¯å€‹ROIå€åŸŸç‰¹å¾µç¶­åº¦: 5å€‹çµ±è¨ˆç‰¹å¾µ + 2å€‹ç›´æ–¹åœ–(8 bins each) = 21
        roi_dim = 5 + 8 + 8
        num_rois = len(self.roi_regions)
        
        total_dim = global_dim + (roi_dim * num_rois)
        
        return total_dim

    def _extract_frame_feature_vector(self, frame_data: Dict) -> np.ndarray:
        """å¾å¹€æ•¸æ“šä¸­æå–ç‰¹å¾µå‘é‡"""
        feature_vector = []
        
        # å…¨å±€ç‰¹å¾µ
        global_features = frame_data.get('global_features', {})
        for key in ['mean_magnitude', 'std_magnitude', 'max_magnitude', 'mean_angle', 'std_angle']:
            feature_vector.append(global_features.get(key, 0.0))
        
        # ROIç‰¹å¾µ
        roi_features = frame_data.get('roi_features', {})
        for roi_name in self.roi_regions.keys():
            roi_data = roi_features.get(roi_name, {})
            
            # çµ±è¨ˆç‰¹å¾µ
            for key in ['mean_magnitude', 'std_magnitude', 'max_magnitude', 'mean_angle', 'std_angle']:
                feature_vector.append(roi_data.get(key, 0.0))
            
            # ç›´æ–¹åœ–ç‰¹å¾µ
            for hist_type in ['histogram_mag', 'histogram_ang']:
                histogram = roi_data.get(hist_type, [0.0] * 8)
                feature_vector.extend(histogram)
        
        return np.array(feature_vector)

    def _normalize_features(self, feature_matrix: np.ndarray) -> np.ndarray:
        """æ¨™æº–åŒ–ç‰¹å¾µ"""
        if feature_matrix.shape[0] == 0:
            return feature_matrix
        
        # Z-scoreæ¨™æº–åŒ–
        mean = np.mean(feature_matrix, axis=0)
        std = np.std(feature_matrix, axis=0)
        
        # é¿å…é™¤ä»¥é›¶
        std[std == 0] = 1.0
        
        normalized_matrix = (feature_matrix - mean) / std
        
        return normalized_matrix

    def save_features(self, features: Dict, output_path: str):
        """å„²å­˜ç‰¹å¾µåˆ°æ–‡ä»¶ï¼ˆåƒ…ä¿å­˜.npyæª”æ¡ˆï¼‰"""
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        # åªå„²å­˜æ¨™æº–åŒ–ç‰¹å¾µçŸ©é™£
        feature_matrix = self.extract_normalized_features(features)
        np.save(output_path, feature_matrix.astype(np.float32))

class KaggleOpticalFlowExtractor:
    def __init__(self, 
                 dataset_root: str,
                 output_root: str = None,
                 num_workers: int = None,
                 flow_method: str = 'farneback',
                 confidence_threshold: float = 0.5):
        """
        Kaggle Tesla P100 å…‰æµç‰¹å¾µæå–å™¨
        """
        self.dataset_root = Path(dataset_root)
        self.output_root = Path(output_root) if output_root else self.dataset_root.parent / "optical_flow_features"
        self.num_workers = num_workers or max(1, mp.cpu_count() - 1)
        self.flow_method = flow_method
        self.confidence_threshold = confidence_threshold
            
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        self.output_root.mkdir(parents=True, exist_ok=True)
        print("ğŸš€ KaggleOpticalFlowExtractor åˆå§‹åŒ–å®Œæˆï¼ˆå·²æ·˜æ±°ï¼Œè«‹æ”¹ç”¨æœ¬åœ° MPS æµç¨‹æˆ– MpsOpticalFlowExtractorï¼‰")
        print(f"ğŸ“‚ è³‡æ–™é›†è·¯å¾‘: {self.dataset_root}")
        print(f"ğŸ“ è¼¸å‡ºè·¯å¾‘: {self.output_root}")
        print(f"âš¡ å·¥ä½œé€²ç¨‹æ•¸: {self.num_workers}")
        print(f"ğŸŒŠ å…‰æµæ–¹æ³•: {flow_method}")
        print(f"ğŸ¯ GPU: Tesla P100 å„ªåŒ–æ¨¡å¼")

    def discover_videos(self) -> dict:
        """ç™¼ç¾æ‰€æœ‰å½±ç‰‡æ–‡ä»¶"""
        videos_by_class = {}
        # æ“´å……æ”¯æ´æ›´å¤šå½±ç‰‡æ ¼å¼ï¼ŒåŒ…å«å¤§å°å¯«
        video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.MP4', '.AVI', '.MOV', '.MKV', 
                           '.m4v', '.M4V', '.webm', '.WEBM', '.flv', '.FLV'}
        
        print(f"ğŸ” æƒæå…‰æµè³‡æ–™é›†ç›®éŒ„: {self.dataset_root}")
        
        if not self.dataset_root.exists():
            print(f"âŒ è³‡æ–™é›†ç›®éŒ„ä¸å­˜åœ¨: {self.dataset_root}")
            return videos_by_class
            
        subdirs = [d for d in self.dataset_root.iterdir() if d.is_dir()]
        print(f"ğŸ“ æ‰¾åˆ° {len(subdirs)} å€‹å­ç›®éŒ„")
        
        for class_dir in subdirs:
            if class_dir.is_dir():
                class_name = class_dir.name
                videos = []
                
                # å…ˆæª¢æŸ¥ç›®éŒ„æ˜¯å¦å¯è®€å–
                try:
                    all_files = list(class_dir.iterdir())
                    print(f"ğŸ“‚ {class_name}: ç¸½å…± {len(all_files)} å€‹æª”æ¡ˆ")
                    
                    for video_file in all_files:
                        if video_file.is_file() and video_file.suffix in video_extensions:
                            videos.append(str(video_file))
                        elif video_file.is_file():
                            # èª¿è©¦ï¼šé¡¯ç¤ºéå½±ç‰‡æª”æ¡ˆçš„å‰¯æª”å
                            if len(videos) < 3:  # åªé¡¯ç¤ºå‰å¹¾å€‹
                                print(f"   ğŸ” ç™¼ç¾å…¶ä»–æª”æ¡ˆ: {video_file.name} (å‰¯æª”å: '{video_file.suffix}')")
                    
                    if videos:
                        videos_by_class[class_name] = sorted(videos)
                        print(f"   âœ… {class_name}: æ‰¾åˆ° {len(videos)} å€‹å½±ç‰‡")
                    else:
                        print(f"   âŒ {class_name}: æœªæ‰¾åˆ°å½±ç‰‡æª”æ¡ˆ")
                        
                except Exception as e:
                    print(f"   âŒ ç„¡æ³•è®€å–ç›®éŒ„ {class_name}: {e}")
                    
        print(f"ğŸ“Š ç¸½çµ: {len(videos_by_class)} å€‹é¡åˆ¥åŒ…å«å½±ç‰‡")
        return videos_by_class

    def _read_failed_video_paths_from_report(self, report_path: Optional[str] = None) -> List[str]:
        """å¾è™•ç†å ±å‘Šä¸­è®€å–å¤±æ•—/éŒ¯èª¤å½±ç‰‡çš„å®Œæ•´è·¯å¾‘æ¸…å–®"""
        report = Path(report_path) if report_path else (self.output_root / "optical_flow_processing_report.json")
        if not report.exists():
            print(f"âŒ æ‰¾ä¸åˆ°è™•ç†å ±å‘Š: {report}")
            return []
        try:
            with open(report, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            print(f"âŒ ç„¡æ³•è®€å–å ±å‘Š {report}: {e}")
            return []

        failed_paths: List[str] = []
        for item in data.get('details', []):
            if item.get('status') in ['failed', 'error'] and item.get('video_path'):
                failed_paths.append(item['video_path'])

        # å»é‡ & åƒ…ä¿ç•™ä»å­˜åœ¨æ–¼è³‡æ–™é›†çš„å½±ç‰‡
        unique_paths = []
        seen = set()
        for p in failed_paths:
            if p not in seen:
                seen.add(p)
                if Path(p).exists():
                    unique_paths.append(p)
                else:
                    # å˜—è©¦ä»¥è³‡æ–™é›†æ ¹ç›®éŒ„é‡æ–°å®šä½ï¼ˆä¿éšªï¼‰
                    candidate = self.dataset_root / Path(p).name[: Path(p).name.find('_')] / Path(p).name
                    if candidate.exists():
                        unique_paths.append(str(candidate))
        print(f"ğŸ§¾ å¾å ±å‘Šå–å¾—éœ€é‡è©¦å½±ç‰‡: {len(unique_paths)} ç­†")
        return unique_paths

    def _resolve_video_names_to_paths(self, names: List[str]) -> List[str]:
        """å°‡æª”åï¼ˆå¦‚ apple_0850.mp4ï¼‰è§£æç‚ºè³‡æ–™é›†å…§çš„å®Œæ•´è·¯å¾‘"""
        resolved: List[str] = []
        for name in names:
            base = Path(name).name
            # å…ˆç”¨é¡åˆ¥=åº•ç·šå‰ç¶´çš„è¦å‰‡
            class_name = base.split('_')[0] if '_' in base else None
            candidates: List[Path] = []
            if class_name:
                candidates.append(self.dataset_root / class_name / base)
            # å¾Œå‚™æ–¹æ¡ˆï¼šå…¨åŸŸæœå°‹ï¼ˆåƒ…é™æª”ååŒ¹é…ï¼‰
            if not any(p.exists() for p in candidates):
                for p in self.dataset_root.rglob(base):
                    if p.is_file():
                        candidates.append(p)
                        break
            # æ”¶æ–‚ç‚ºç¬¬ä¸€å€‹å­˜åœ¨çš„å€™é¸
            target = next((str(p) for p in candidates if p.exists()), None)
            if target:
                resolved.append(target)
            else:
                print(f"âš ï¸ æ‰¾ä¸åˆ°å½±ç‰‡: {name}")
        print(f"ğŸ§­ æŒ‡å®šæª”åè§£æå®Œæˆï¼Œå¯è™•ç†: {len(resolved)} / {len(names)}")
        return resolved

    def _process_specific_video_paths(self, video_paths: List[str]) -> dict:
        """ä¸¦è¡Œè™•ç†æŒ‡å®šçš„ä¸€çµ„å½±ç‰‡è·¯å¾‘"""
        if not video_paths:
            print("âŒ æ²’æœ‰å¯è™•ç†çš„å½±ç‰‡æ¸…å–®")
            return {}

        batches = self.create_batches(video_paths)
        print(f"ğŸ“¦ é‡è©¦æ‰¹æ¬¡æ•¸: {len(batches)} ï¼›å½±ç‰‡æ•¸: {len(video_paths)}")

        results = {
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'error': 0,
            'details': [],
            'start_time': time.time()
        }

        with ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            future_to_batch = {executor.submit(self.process_video_batch, batch): batch for batch in batches}
            with tqdm(total=len(video_paths), desc="é‡è©¦å½±ç‰‡", unit="å½±ç‰‡") as pbar:
                for future in as_completed(future_to_batch):
                    batch_results = future.result()
                    for result in batch_results:
                        results['details'].append(result)
                        results[result['status']] += 1
                        pbar.update(1)
                    pbar.set_postfix({
                        'æˆåŠŸ': results['success'],
                        'å¤±æ•—': results['failed'],
                        'è·³é': results['skipped'],
                        'éŒ¯èª¤': results['error']
                    })

        results['end_time'] = time.time()
        results['total_time'] = results['end_time'] - results['start_time']

        # è¼¸å‡ºé‡è©¦å ±å‘Š
        retry_report = self.output_root / "optical_flow_retry_report.json"
        try:
            with open(retry_report, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•å¯«å…¥é‡è©¦å ±å‘Š: {e}")

        return results

    def retry_failed_from_report(self, report_path: Optional[str] = None) -> dict:
        """åªé‡å°å ±å‘Šä¸­çš„å¤±æ•—/éŒ¯èª¤å½±ç‰‡åšå†æ¬¡æå–"""
        failed_paths = self._read_failed_video_paths_from_report(report_path)
        if not failed_paths:
            print("âœ… æ²’æœ‰å¯é‡è©¦çš„å¤±æ•—å½±ç‰‡ï¼ˆæˆ–æ‰¾ä¸åˆ°å ±å‘Šï¼‰")
            return {}
        print("ğŸš€ é–‹å§‹é‡è©¦å ±å‘Šä¸­çš„å¤±æ•—å½±ç‰‡...")
        return self._process_specific_video_paths(failed_paths)

    def retry_specific_videos(self, names: List[str]) -> dict:
        """åªé‡å°æ‰‹å‹•æŒ‡å®šçš„å½±ç‰‡æª”ååšå†æ¬¡æå–ï¼ˆå¦‚ ['apple_0850.mp4']ï¼‰"""
        video_paths = self._resolve_video_names_to_paths(names)
        if not video_paths:
            print("âŒ æŒ‡å®šçš„å½±ç‰‡çš†ç„¡æ³•è§£æè·¯å¾‘ï¼Œç„¡æ³•é‡è©¦")
            return {}
        print("ğŸš€ é–‹å§‹é‡è©¦æŒ‡å®šæ¸…å–®å½±ç‰‡...")
        return self._process_specific_video_paths(video_paths)

    def get_processing_stats(self, videos_by_class: dict) -> dict:
        """ç²å–è™•ç†çµ±è¨ˆä¿¡æ¯"""
        stats = {
            'total_classes': len(videos_by_class),
            'total_videos': sum(len(videos) for videos in videos_by_class.values()),
            'videos_per_class': {},
            'estimated_time': 0
        }
        
        for class_name, videos in videos_by_class.items():
            stats['videos_per_class'][class_name] = len(videos)
        
        # Tesla P100 å…‰æµè¨ˆç®—å„ªåŒ–å¾Œä¼°ç®—æ™‚é–“ (æ¯å€‹å½±ç‰‡ç´„4-6ç§’)
        avg_time_per_video = 5.0
        stats['estimated_time'] = stats['total_videos'] * avg_time_per_video / self.num_workers
        
        return stats

    def process_video_batch(self, video_paths_batch: list) -> list:
        """è™•ç†ä¸€æ‰¹å½±ç‰‡çš„å·¥ä½œå‡½æ•¸"""
        extractor = OpticalFlowExtractor(
            target_frames=100,
            flow_method=self.flow_method,
            confidence_threshold=self.confidence_threshold
        )
        results = []
        
        for video_path in video_paths_batch:
            try:
                # è¨ˆç®—è¼¸å‡ºè·¯å¾‘ï¼Œä¿æŒå’Œè³‡æ–™é›†ç›¸åŒçš„ç›®éŒ„çµæ§‹
                video_path_obj = Path(video_path)
                class_name = video_path_obj.parent.name
                output_dir = self.output_root / class_name
                output_dir.mkdir(parents=True, exist_ok=True)
                
                # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
                if not output_dir.exists():
                    print(f"âš ï¸ å‰µå»ºå…‰æµè¼¸å‡ºç›®éŒ„: {output_dir}")
                    output_dir.mkdir(parents=True, exist_ok=True)
                
                output_path = output_dir / f"{video_path_obj.stem}.npy"
                
                # è·³éå·²å­˜åœ¨çš„æ–‡ä»¶
                if output_path.exists():
                    results.append({
                        'video_path': video_path,
                        'output_path': str(output_path),
                        'status': 'skipped',
                        'message': 'æ–‡ä»¶å·²å­˜åœ¨'
                    })
                    continue
                
                # æå–ç‰¹å¾µ
                features = extractor.extract_video_features(video_path)
                
                if features is not None:
                    # å„²å­˜ç‰¹å¾µ
                    extractor.save_features(features, str(output_path))
                    
                    # ç²å–ç‰¹å¾µçµ±è¨ˆ
                    feature_matrix = extractor.extract_normalized_features(features)
                    
                    results.append({
                        'video_path': video_path,
                        'output_path': str(output_path),
                        'status': 'success',
                        'original_frames': features.get('original_frames', len(features['frames_data'])),
                        'target_frames': features.get('target_frames', 100),
                        'feature_shape': feature_matrix.shape,
                        'duration': features['video_info']['duration'],
                        'flow_method': features['video_info']['flow_method']
                    })
                else:
                    results.append({
                        'video_path': video_path,
                        'output_path': str(output_path),
                        'status': 'failed',
                        'message': 'å…‰æµç‰¹å¾µæå–å¤±æ•—'
                    })
                    
            except Exception as e:
                results.append({
                    'video_path': video_path,
                    'output_path': '',
                    'status': 'error',
                    'message': str(e)
                })
        
        return results

    def create_batches(self, all_videos: list, batch_size: int = None) -> list:
        """å°‡å½±ç‰‡åˆ—è¡¨åˆ†æˆæ‰¹æ¬¡"""
        if batch_size is None:
            batch_size = max(1, len(all_videos) // (self.num_workers * 4))
        
        batches = []
        for i in range(0, len(all_videos), batch_size):
            batches.append(all_videos[i:i + batch_size])
        
        return batches

    def process_dataset(self, limit_per_class: int = None) -> dict:
        """è™•ç†æ•´å€‹è³‡æ–™é›†"""
        print(f"ğŸ” æƒæå½±ç‰‡æ–‡ä»¶...")
        videos_by_class = self.discover_videos()
        
        if not videos_by_class:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•å½±ç‰‡æ–‡ä»¶")
            return {}
        
        # é™åˆ¶æ¯å€‹é¡åˆ¥çš„å½±ç‰‡æ•¸é‡
        if limit_per_class:
            for class_name in videos_by_class:
                videos_by_class[class_name] = videos_by_class[class_name][:limit_per_class]
        
        # ç²å–çµ±è¨ˆä¿¡æ¯
        stats = self.get_processing_stats(videos_by_class)
        
        print(f"ğŸ“Š ç™¼ç¾ {stats['total_classes']} å€‹é¡åˆ¥ï¼Œå…± {stats['total_videos']} å€‹å½±ç‰‡")
        print(f"â±ï¸  ä¼°ç®—è™•ç†æ™‚é–“: {stats['estimated_time']/60:.1f} åˆ†é˜")
        
        # Kaggleè‡ªå‹•æ¨¡å¼
        print("ğŸ¤– Kaggleæ¨¡å¼ï¼Œè‡ªå‹•é–‹å§‹è™•ç†...")
        
        # å‰µå»ºæ‰€æœ‰å½±ç‰‡è·¯å¾‘åˆ—è¡¨
        all_videos = []
        for class_name, videos in videos_by_class.items():
            all_videos.extend(videos)
        
        # å‰µå»ºæ‰¹æ¬¡
        batches = self.create_batches(all_videos)
        print(f"ğŸ“¦ å‰µå»º {len(batches)} å€‹æ‰¹æ¬¡")
        
        # è™•ç†çµ±è¨ˆ
        results = {
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'error': 0,
            'details': [],
            'start_time': time.time()
        }
        
        # ä½¿ç”¨é€²ç¨‹æ± ä¸¦è¡Œè™•ç†
        print(f"ğŸš€ é–‹å§‹ä¸¦è¡Œè™•ç† ({self.num_workers} å€‹å·¥ä½œé€²ç¨‹)...")
        
        with ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_batch = {
                executor.submit(self.process_video_batch, batch): batch 
                for batch in batches
            }
            
            # ä½¿ç”¨é€²åº¦æ¢é¡¯ç¤ºè™•ç†é€²åº¦
            with tqdm(total=len(all_videos), desc="è™•ç†å½±ç‰‡", unit="å½±ç‰‡") as pbar:
                for future in as_completed(future_to_batch):
                    batch_results = future.result()
                    
                    for result in batch_results:
                        results['details'].append(result)
                        results[result['status']] += 1
                        pbar.update(1)
                        
                        # æ›´æ–°é€²åº¦æ¢æè¿°
                        pbar.set_postfix({
                            'æˆåŠŸ': results['success'],
                            'å¤±æ•—': results['failed'],
                            'è·³é': results['skipped'],
                            'éŒ¯èª¤': results['error']
                        })
        
        # è¨ˆç®—è™•ç†æ™‚é–“
        results['end_time'] = time.time()
        results['total_time'] = results['end_time'] - results['start_time']
        
        # å„²å­˜è™•ç†å ±å‘Š
        report_path = self.output_root / "optical_flow_processing_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        return results

    def validate_extracted_features(self, sample_size: int = 100) -> dict:
        """é©—è­‰æå–çš„ç‰¹å¾µå“è³ª"""
        features_path = self.output_root
        
        if not features_path.exists():
            print(f"âŒ ç‰¹å¾µç›®éŒ„ä¸å­˜åœ¨: {features_path}")
            return {}
        
        print(f"ğŸ” é©—è­‰å…‰æµç‰¹å¾µç›®éŒ„: {features_path}")
        
        # æ”¶é›†æ‰€æœ‰.npyæ–‡ä»¶
        all_npy_files = []
        for class_dir in features_path.iterdir():
            if class_dir.is_dir():
                npy_files = list(class_dir.glob('*.npy'))
                for npy_file in npy_files:
                    all_npy_files.append((class_dir.name, npy_file))
        
        if not all_npy_files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•.npyç‰¹å¾µæ–‡ä»¶")
            return {}
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(all_npy_files)} å€‹å…‰æµç‰¹å¾µæ–‡ä»¶")
        
        # éš¨æ©ŸæŠ½æ¨£
        sample_files = random.sample(all_npy_files, min(sample_size, len(all_npy_files)))
        
        validation_stats = {
            'total_files': len(all_npy_files),
            'sample_size': len(sample_files),
            'shapes': [],
            'feature_stats': {
                'min_vals': [],
                'max_vals': [],
                'mean_vals': [],
                'std_vals': []
            },
            'classes': {},
            'corrupted_files': [],
            'dimension_consistency': True,
            'flow_method': self.flow_method
        }
        
        expected_shape = None
        
        print(f"ğŸ§ª é©—è­‰ {len(sample_files)} å€‹æ¨£æœ¬...")
        
        for class_name, npy_file in tqdm(sample_files, desc="é©—è­‰å…‰æµç‰¹å¾µ"):
            try:
                # è¼‰å…¥ç‰¹å¾µ
                features = np.load(npy_file)
                
                # æª¢æŸ¥å½¢ç‹€
                if expected_shape is None:
                    expected_shape = features.shape
                    print(f"ğŸ“ é æœŸå…‰æµç‰¹å¾µå½¢ç‹€: {expected_shape}")
                
                validation_stats['shapes'].append(features.shape)
                
                if features.shape != expected_shape:
                    validation_stats['dimension_consistency'] = False
                    print(f"âš ï¸  å½¢ç‹€ä¸ä¸€è‡´: {npy_file.name} -> {features.shape}")
                
                # çµ±è¨ˆè³‡è¨Š
                validation_stats['feature_stats']['min_vals'].append(np.min(features))
                validation_stats['feature_stats']['max_vals'].append(np.max(features))
                validation_stats['feature_stats']['mean_vals'].append(np.mean(features))
                validation_stats['feature_stats']['std_vals'].append(np.std(features))
                
                # é¡åˆ¥çµ±è¨ˆ
                if class_name not in validation_stats['classes']:
                    validation_stats['classes'][class_name] = 0
                validation_stats['classes'][class_name] += 1
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ç•°å¸¸å€¼
                if np.isnan(features).any() or np.isinf(features).any():
                    validation_stats['corrupted_files'].append(str(npy_file))
                    print(f"âŒ ç™¼ç¾ç•°å¸¸å€¼: {npy_file.name}")
                
            except Exception as e:
                validation_stats['corrupted_files'].append(str(npy_file))
                print(f"âŒ è¼‰å…¥å¤±æ•—: {npy_file.name} -> {e}")
        
        # çµ±è¨ˆåˆ†æ
        print(f"\nğŸ“ˆ å…‰æµç‰¹å¾µé©—è­‰çµæœ:")
        print(f"âœ… æˆåŠŸè¼‰å…¥: {len(sample_files) - len(validation_stats['corrupted_files'])} å€‹æ–‡ä»¶")
        print(f"âŒ æå£æ–‡ä»¶: {len(validation_stats['corrupted_files'])} å€‹")
        print(f"ğŸ“ å½¢ç‹€ä¸€è‡´æ€§: {'âœ… é€šé' if validation_stats['dimension_consistency'] else 'âŒ å¤±æ•—'}")
        print(f"ğŸŒŠ å…‰æµæ–¹æ³•: {self.flow_method}")
        
        if expected_shape:
            print(f"ğŸ“Š ç‰¹å¾µç¶­åº¦: {expected_shape[0]} å¹€ Ã— {expected_shape[1]} å…‰æµç‰¹å¾µ")
        
        # ç‰¹å¾µçµ±è¨ˆ
        if validation_stats['feature_stats']['min_vals']:
            min_vals = validation_stats['feature_stats']['min_vals']
            max_vals = validation_stats['feature_stats']['max_vals']
            mean_vals = validation_stats['feature_stats']['mean_vals']
            std_vals = validation_stats['feature_stats']['std_vals']
            
            print(f"ğŸ“Š å…‰æµç‰¹å¾µæ•¸å€¼çµ±è¨ˆ:")
            print(f"   æœ€å°å€¼ç¯„åœ: {np.min(min_vals):.3f} è‡³ {np.max(min_vals):.3f}")
            print(f"   æœ€å¤§å€¼ç¯„åœ: {np.min(max_vals):.3f} è‡³ {np.max(max_vals):.3f}")
            print(f"   å¹³å‡å€¼ç¯„åœ: {np.min(mean_vals):.3f} è‡³ {np.max(mean_vals):.3f}")
            print(f"   æ¨™æº–å·®ç¯„åœ: {np.min(std_vals):.3f} è‡³ {np.max(std_vals):.3f}")
        
        # é¡åˆ¥åˆ†ä½ˆ
        print(f"\nğŸ“Š é¡åˆ¥åˆ†ä½ˆï¼ˆæ¨£æœ¬ï¼‰:")
        for class_name, count in sorted(validation_stats['classes'].items()):
            print(f"   {class_name}: {count} å€‹æ¨£æœ¬")
        
        # ç”Ÿæˆé©—è­‰å ±å‘Š
        validation_stats['summary'] = {
            'total_files': len(all_npy_files),
            'sample_validated': len(sample_files),
            'success_rate': (len(sample_files) - len(validation_stats['corrupted_files'])) / len(sample_files),
            'dimension_consistent': validation_stats['dimension_consistency'],
            'expected_shape': expected_shape,
            'flow_method': self.flow_method
        }
        
        # å„²å­˜å ±å‘Š
        report_path = features_path / "optical_flow_validation_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(validation_stats, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“ˆ å…‰æµé©—è­‰å ±å‘Šå·²å„²å­˜è‡³: {report_path}")
        
        return validation_stats

    def create_features_zip(self, zip_name: str = None) -> str:
        """æ‰“åŒ…ç‰¹å¾µæ–‡ä»¶ç‚ºzip"""
        if zip_name is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            zip_name = f"optical_flow_features_{self.flow_method}_{timestamp}.zip"
        
        zip_path = self.output_root.parent / zip_name
        
        print(f"ğŸ“¦ é–‹å§‹æ‰“åŒ…å…‰æµç‰¹å¾µæ–‡ä»¶...")
        
        # æ”¶é›†æ‰€æœ‰.npyæ–‡ä»¶
        all_npy_files = []
        for class_dir in self.output_root.iterdir():
            if class_dir.is_dir():
                npy_files = list(class_dir.glob('*.npy'))
                all_npy_files.extend(npy_files)
        
        if not all_npy_files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•.npyç‰¹å¾µæ–‡ä»¶")
            return ""
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(all_npy_files)} å€‹å…‰æµç‰¹å¾µæ–‡ä»¶")
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED, compresslevel=9) as zipf:
            for npy_file in tqdm(all_npy_files, desc="æ‰“åŒ…æ–‡ä»¶"):
                # ä¿æŒç›®éŒ„çµæ§‹
                arcname = npy_file.relative_to(self.output_root)
                zipf.write(npy_file, arcname)
            
            # æ·»åŠ å ±å‘Šæ–‡ä»¶
            report_files = ['optical_flow_processing_report.json', 'optical_flow_validation_report.json']
            for report_file in report_files:
                report_path = self.output_root / report_file
                if report_path.exists():
                    zipf.write(report_path, report_file)
        
        zip_size_mb = zip_path.stat().st_size / (1024 * 1024)
        print(f"âœ… å…‰æµç‰¹å¾µæ‰“åŒ…å®Œæˆ: {zip_path}")
        print(f"ğŸ“Š å£“ç¸®æª”å¤§å°: {zip_size_mb:.1f} MB")
        print(f"ğŸŒŠ å…‰æµæ–¹æ³•: {self.flow_method}")
        
        return str(zip_path)

    def print_results_summary(self, results: dict):
        """æ‰“å°çµæœæ‘˜è¦"""
        if not results:
            return
            
        print(f"\nğŸ“ˆ å…‰æµç‰¹å¾µè™•ç†å®Œæˆæ‘˜è¦:")
        print(f"â±ï¸  ç¸½è€—æ™‚: {results['total_time']/60:.1f} åˆ†é˜")
        print(f"âœ… æˆåŠŸ: {results['success']} å€‹å½±ç‰‡")
        print(f"â­ï¸  è·³é: {results['skipped']} å€‹å½±ç‰‡")
        print(f"âŒ å¤±æ•—: {results['failed']} å€‹å½±ç‰‡")
        print(f"ğŸ’¥ éŒ¯èª¤: {results['error']} å€‹å½±ç‰‡")
        print(f"ğŸŒŠ å…‰æµæ–¹æ³•: {self.flow_method}")
        
        total_processed = results['success'] + results['failed'] + results['skipped'] + results['error']
        if total_processed > 0:
            success_rate = (results['success'] / total_processed) * 100
            print(f"ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}%")
        
        if results['failed'] > 0 or results['error'] > 0:
            print(f"\nâš ï¸  å¤±æ•—çš„å½±ç‰‡:")
            for detail in results['details']:
                if detail['status'] in ['failed', 'error']:
                    print(f"   - {Path(detail['video_path']).name}: {detail.get('message', 'æœªçŸ¥éŒ¯èª¤')}")

    def run_complete_pipeline(self, limit_per_class: int = None, sample_size: int = 100):
        """åŸ·è¡Œå®Œæ•´çš„å…‰æµç‰¹å¾µæå–ã€é©—è­‰å’Œæ‰“åŒ…æµç¨‹"""
        print("ğŸš€ é–‹å§‹å®Œæ•´çš„å…‰æµç‰¹å¾µæå–æµç¨‹...")
        print(f"ğŸŒŠ ä½¿ç”¨å…‰æµæ–¹æ³•: {self.flow_method}")
        
        # 1. å…‰æµç‰¹å¾µæå–
        print("\nğŸ“ æ­¥é©Ÿ1: å…‰æµç‰¹å¾µæå–")
        results = self.process_dataset(limit_per_class=limit_per_class)
        self.print_results_summary(results)
        
        # 2. ç‰¹å¾µé©—è­‰
        print("\nğŸ“ æ­¥é©Ÿ2: å…‰æµç‰¹å¾µé©—è­‰")
        validation_stats = self.validate_extracted_features(sample_size=sample_size)
        
        # 3. æ‰“åŒ…ç‰¹å¾µ
        print("\nğŸ“ æ­¥é©Ÿ3: æ‰“åŒ…å…‰æµç‰¹å¾µ")
        zip_path = self.create_features_zip()
        print("ğŸ‰ å®Œæ•´å…‰æµè™•ç†æµç¨‹å®Œæˆ!")
        print(f"ğŸ“¦ å…‰æµç‰¹å¾µå£“ç¸®æª”: {zip_path}")
        print(f"ğŸŒŠ å…‰æµæ–¹æ³•: {self.flow_method}")
        return {
            'extraction_results': results,
            'validation_stats': validation_stats,
            'zip_path': zip_path,
            'flow_method': self.flow_method
        }


# --- MPS å‹å–„åŒ…è£ï¼šç©©å®šè¼¸å‡ºèˆ‡å¯é‡ç¾æ€§ï¼Œé è¨­è¼¸å‡ºåˆ°å°ˆæ¡ˆ features/optical_flow_features ---
class MpsOpticalFlowExtractor:
    def __init__(self,
                 target_frames: int = 100,
                 resize_dims: Tuple[int, int] = (224, 224),
                 flow_method: str = 'farneback',
                 output_root: Optional[str] = None,
                 roi_mode: str = 'static'):
        self.target_frames = int(target_frames)
        self.resize_dims = resize_dims
        self.flow_method = flow_method
        self.roi_mode = roi_mode
        self.output_root = Path(output_root) if output_root else OUTPUT_ROOT

    def extract_and_save(self, video_path: str, class_name: Optional[str] = None) -> Path:
        """æŠ½å–å–®æ”¯å½±ç‰‡å…‰æµç‰¹å¾µï¼Œä¿å­˜ç‚º .npy ä¸¦å›å‚³è·¯å¾‘ã€‚"""
        extractor = OpticalFlowExtractor(
            target_frames=self.target_frames,
            flow_method=self.flow_method,
            resize_dims=self.resize_dims,
            roi_mode=self.roi_mode,
        )
        vp = Path(video_path)
        cls = class_name or vp.parent.name
        out_dir = self.output_root / cls
        out_dir.mkdir(parents=True, exist_ok=True)
        out_path = out_dir / (vp.stem + ".npy")

        # è·³éå·²å­˜åœ¨
        if out_path.exists():
            return out_path

        feats = extractor.extract_video_features(str(vp))
        if feats is None:
            # å¯«å…¥é›¶çŸ©é™£ä»¥ä¿æŒé…å°ï¼Œå½¢ç‹€ [T, 8]
            np.save(out_path.as_posix(), np.zeros((self.target_frames, 8), dtype=np.float32))
            return out_path

        mat = extractor.extract_normalized_features(feats)
        # ç·šæ€§æ’å€¼å°é½Šé•·åº¦ï¼ˆä¿éšªï¼‰
        if mat.shape[0] != self.target_frames:
            x_old = np.linspace(0, 1, mat.shape[0], dtype=np.float32)
            x_new = np.linspace(0, 1, self.target_frames, dtype=np.float32)
            aligned = np.vstack([np.interp(x_new, x_old, mat[:, d]) for d in range(mat.shape[1])]).T
            mat = aligned.astype(np.float32)

        np.save(out_path.as_posix(), mat.astype(np.float32))
        return out_path


# ---- æœ¬åœ° MPS è³‡æ–™é›†æ‰¹æ¬¡è™•ç†ï¼ˆé ‚å±¤å‡½å¼ï¼Œä¾¿æ–¼ main() ç›´æ¥å‘¼å«ï¼‰ ----
_PROC_OF_EXTRACTOR = None


def _discover_videos_local(dataset_root: Path) -> Dict[str, List[str]]:
    videos_by_class: Dict[str, List[str]] = {}
    exts = {".mp4", ".avi", ".mov", ".mkv", ".m4v", ".webm", ".flv",
            ".MP4", ".AVI", ".MOV", ".MKV", ".M4V", ".WEBM", ".FLV"}
    if not dataset_root.exists():
        return videos_by_class
    for class_dir in sorted([d for d in dataset_root.iterdir() if d.is_dir()]):
        files = [f.as_posix() for f in class_dir.iterdir() if f.is_file() and f.suffix in exts]
        if files:
            videos_by_class[class_dir.name] = sorted(files)
    return videos_by_class


def _init_worker_of(target_frames: int, resize_dims: Tuple[int, int], flow_method: str, roi_mode: str):
    global _PROC_OF_EXTRACTOR
    _PROC_OF_EXTRACTOR = MpsOpticalFlowExtractor(
        target_frames=target_frames,
        resize_dims=resize_dims,
        flow_method=flow_method,
        roi_mode=roi_mode,
    )


def _worker_process_video_of(task: Tuple[str, str]) -> dict:
    cls, vp = task
    try:
        global _PROC_OF_EXTRACTOR
        if _PROC_OF_EXTRACTOR is None:
            _PROC_OF_EXTRACTOR = MpsOpticalFlowExtractor()
        out_path = _PROC_OF_EXTRACTOR.extract_and_save(vp, class_name=cls)
        arr = np.load(out_path)
        return {"video": vp, "status": "success", "shape": list(arr.shape), "output": out_path.as_posix()}
    except Exception as e:
        return {"video": vp, "status": "error", "msg": str(e)}


def run_dataset_extraction_local(dataset_root: Optional[str] = None,
                                 target_frames: int = 100,
                                 resize_dims: Tuple[int, int] = (224, 224),
                                 flow_method: str = 'farneback',
                                 roi_mode: str = 'static',
                                 limit_per_class: Optional[int] = None,
                                 parallel: bool = True,
                                 num_workers: int = NUM_WORKERS) -> dict:
    ds_root = Path(dataset_root) if dataset_root else DATASET_PATH
    print("ğŸš€ æœ¬åœ° MPS å…‰æµç‰¹å¾µæå–")
    print(f"ğŸ“‚ è³‡æ–™é›†: {ds_root}")
    print(f"ğŸ“ è¼¸å‡º:   {OUTPUT_ROOT}")

    vids = _discover_videos_local(ds_root)
    if not vids:
        print("âŒ æ‰¾ä¸åˆ°ä»»ä½•å½±ç‰‡ï¼Œè«‹ç¢ºèªè³‡æ–™é›†è·¯å¾‘")
        return {}

    if limit_per_class:
        vids = {k: v[:limit_per_class] for k, v in vids.items()}

    total = sum(len(v) for v in vids.values())
    print(f"ğŸ“Š é¡åˆ¥: {len(vids)}ï¼Œå½±ç‰‡ç¸½æ•¸: {total}")

    results = {"success": 0, "failed": 0, "details": [], "start_time": time.time()}

    all_videos: List[Tuple[str, str]] = []
    for cls, paths in vids.items():
        for vp in paths:
            all_videos.append((cls, vp))

    if not parallel:
        extractor = MpsOpticalFlowExtractor(target_frames=target_frames, resize_dims=resize_dims, flow_method=flow_method, roi_mode=roi_mode)
        with tqdm(total=total, desc="è™•ç†å½±ç‰‡", unit="æ”¯", dynamic_ncols=True) as pbar:
            for cls, vp in all_videos:
                try:
                    out_path = extractor.extract_and_save(vp, class_name=cls)
                    arr = np.load(out_path)
                    results["success"] += 1
                    results["details"].append({"video": vp, "status": "success", "shape": list(arr.shape)})
                except Exception as e:
                    results["failed"] += 1
                    results["details"].append({"video": vp, "status": "error", "msg": str(e)})
                finally:
                    pbar.update(1)
                    pbar.set_postfix({"æˆåŠŸ": results["success"], "å¤±æ•—": results["failed"], "æœ€è¿‘": Path(vp).name})
    else:
        print(f"ğŸš€ å¹³è¡Œè™•ç†é–‹å•Ÿï¼ˆworkers={num_workers}ï¼‰...")
        with ProcessPoolExecutor(max_workers=num_workers,
                                 initializer=_init_worker_of,
                                 initargs=(target_frames, resize_dims, flow_method, roi_mode)) as ex:
            chunk_size = max(1, len(all_videos)//(num_workers*4) or 1)
            with tqdm(total=total, desc="è™•ç†å½±ç‰‡", unit="æ”¯", dynamic_ncols=True) as pbar:
                for res in ex.map(_worker_process_video_of, all_videos, chunksize=chunk_size):
                    results["details"].append(res)
                    status = res.get("status", "error")
                    if status == "success":
                        results["success"] += 1
                    else:
                        results["failed"] += 1
                    pbar.update(1)
                    name = Path(res.get("video", "")).name if res else ""
                    pbar.set_postfix({"æˆåŠŸ": results["success"], "å¤±æ•—": results["failed"], "æœ€è¿‘": name})

    results["end_time"] = time.time()
    results["total_time"] = results["end_time"] - results["start_time"]

    OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)
    try:
        with open((OUTPUT_ROOT / "optical_flow_processing_report_local.json").as_posix(), "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
    except Exception:
        pass

    print("\nğŸ“ˆ å®Œæˆã€‚")
    print(f"â±ï¸  è€—æ™‚: {results['total_time']/60:.1f} åˆ†é˜")
    print(f"âœ… æˆåŠŸ: {results['success']} | âŒ å¤±æ•—: {results['failed']}")
    return results


def main():
    # æœ¬åœ°é è¨­ï¼šèˆ‡è¨“ç·´ç¨‹å¼è®€å–è·¯å¾‘ä¸€è‡´
    run_dataset_extraction_local(
        dataset_root=str(DATASET_PATH),
        target_frames=100,
        resize_dims=(224, 224),
        flow_method='farneback',
        roi_mode='static',
        limit_per_class=None,
        parallel=True,
    )


if __name__ == "__main__":
    main()