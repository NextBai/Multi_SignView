#!/usr/bin/env python3
"""
æ–‡å­—ç‰¹å¾µæå–å™¨ - Multi_SignView å¤šæ¨¡æ…‹æ‰‹èªè¾¨è­˜ç³»çµ±
æ•´åˆ Word2Vecã€FastTextã€BERT å¤šå±¤æ¬¡èªç¾©ç‰¹å¾µæå–
æ”¯æ´30å€‹æ‰‹èªè©å½™çš„èªç¾©åˆ†æå’Œç›¸ä¼¼åº¦è¨ˆç®—
æ§‹å»ºè©å½™èªç¾©ç©ºé–“ï¼Œè¼”åŠ©æ‰‹èªåˆ†é¡ä»»å‹™

æŠ€è¡“ç‰¹é»ï¼š
- Word2Vec(300) + FastText(300) + BERT(768) å¤šå±¤æ¬¡è©åµŒå…¥
- 30Ã—30 è©å½™èªç¾©ç›¸ä¼¼åº¦çŸ©é™£è¨ˆç®—
- èªç¾©èšé¡åˆ†æ (å‹•ä½œé¡ã€ç‰©å“é¡ã€äººç‰©é¡ç­‰)
- åŒç¾©è©æ“´å±•å’Œèªç¾©ç¶²çµ¡æ§‹å»º
- æ”¯æ´ä¸­è‹±æ–‡æ··åˆè©å½™è™•ç†

Author: Claude Code + Multi_SignView Team
Date: 2024
"""

import os
import sys
import json
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Set
import pickle
import time

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# NLP æ ¸å¿ƒä¾è³´
import gensim
from gensim.models import Word2Vec, FastText
from gensim.models.keyedvectors import KeyedVectors

# Transformers å’Œ BERT
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F

# NLTK è‡ªç„¶èªè¨€è™•ç†
import nltk
try:
    from nltk.corpus import wordnet as wn
    from nltk.corpus import stopwords
    nltk.download('wordnet', quiet=True)
    nltk.download('omw-1.4', quiet=True)
    nltk.download('stopwords', quiet=True)
except:
    print("âš ï¸  NLTK è³‡æºä¸‹è¼‰å¤±æ•—ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—é™")

# è·¯å¾‘é…ç½®
PROJECT_ROOT = Path(__file__).parent
OUTPUT_ROOT = PROJECT_ROOT / "features" / "text_embeddings"
SEMANTIC_OUTPUT_ROOT = PROJECT_ROOT / "features" / "semantic_features"
MODEL_DIR = PROJECT_ROOT / "model" / "text_models"

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# 30å€‹æ‰‹èªè©å½™åˆ—è¡¨
SIGN_LANGUAGE_VOCABULARY = [
    'again', 'bird', 'book', 'computer', 'cousin', 'deaf', 'drink', 'eat',
    'finish', 'fish', 'friend', 'good', 'happy', 'learn', 'like', 'mother',
    'need', 'nice', 'no', 'orange', 'school', 'sister', 'student', 'table',
    'teacher', 'tired', 'want', 'what', 'white', 'yes'
]


class VocabularyManager:
    """
    è©å½™ç®¡ç†å™¨

    è² è²¬ç®¡ç†30å€‹æ‰‹èªè©å½™çš„èªç¾©åˆ†æã€èšé¡ã€ç›¸ä¼¼åº¦è¨ˆç®—
    """

    def __init__(self, vocabulary: List[str] = SIGN_LANGUAGE_VOCABULARY):
        """
        åˆå§‹åŒ–è©å½™ç®¡ç†å™¨

        Args:
            vocabulary: æ‰‹èªè©å½™åˆ—è¡¨
        """
        self.vocabulary = vocabulary
        self.vocab_size = len(vocabulary)
        self.word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}
        self.idx_to_word = {idx: word for idx, word in enumerate(vocabulary)}

        # èªç¾©åˆ†æçµæœ
        self.semantic_clusters = {}
        self.similarity_matrix = None
        self.synonyms_dict = {}

        print(f"ğŸ“ VocabularyManager åˆå§‹åŒ–å®Œæˆ")
        print(f"   - è©å½™æ•¸é‡: {self.vocab_size}")
        print(f"   - è©å½™åˆ—è¡¨: {', '.join(vocabulary[:10])}..." if len(vocabulary) > 10 else f"   - è©å½™åˆ—è¡¨: {', '.join(vocabulary)}")

    def analyze_semantic_categories(self) -> Dict[str, List[str]]:
        """
        åˆ†æè©å½™çš„èªç¾©åˆ†é¡

        Returns:
            èªç¾©åˆ†é¡å­—å…¸
        """
        try:
            # æ‰‹å‹•å®šç¾©èªç¾©åˆ†é¡ (åŸºæ–¼æ‰‹èªèªç¾©å­¸)
            semantic_categories = {
                'å‹•ä½œé¡': ['again', 'drink', 'eat', 'finish', 'learn', 'like', 'need', 'want'],
                'äººç‰©é¡': ['cousin', 'deaf', 'friend', 'mother', 'sister', 'student', 'teacher'],
                'ç‰©å“é¡': ['bird', 'book', 'computer', 'fish', 'orange', 'table'],
                'ç‹€æ…‹é¡': ['good', 'happy', 'nice', 'tired', 'white'],
                'å ´æ‰€é¡': ['school'],
                'ç–‘å•é¡': ['what'],
                'è‚¯å®šå¦å®šé¡': ['no', 'yes']
            }

            self.semantic_clusters = semantic_categories

            print(f"ğŸ“Š èªç¾©åˆ†é¡å®Œæˆ:")
            for category, words in semantic_categories.items():
                print(f"   - {category}: {len(words)}å€‹è©å½™ -> {', '.join(words)}")

            return semantic_categories

        except Exception as e:
            print(f"âŒ èªç¾©åˆ†é¡å¤±æ•—: {str(e)}")
            return {}

    def extract_synonyms(self) -> Dict[str, List[str]]:
        """
        ä½¿ç”¨ WordNet æå–åŒç¾©è©

        Returns:
            åŒç¾©è©å­—å…¸
        """
        try:
            synonyms_dict = {}

            for word in self.vocabulary:
                synonyms = set()

                try:
                    # ä½¿ç”¨ WordNet æŸ¥æ‰¾åŒç¾©è©
                    synsets = wn.synsets(word)
                    for synset in synsets:
                        for lemma in synset.lemmas():
                            synonym = lemma.name().replace('_', ' ').lower()
                            if synonym != word:
                                synonyms.add(synonym)

                    synonyms_dict[word] = list(synonyms)[:5]  # é™åˆ¶å‰5å€‹åŒç¾©è©

                except:
                    synonyms_dict[word] = []

            self.synonyms_dict = synonyms_dict

            print(f"ğŸ”— åŒç¾©è©æå–å®Œæˆ:")
            for word, syns in list(synonyms_dict.items())[:5]:
                if syns:
                    print(f"   - {word}: {', '.join(syns)}")

            return synonyms_dict

        except Exception as e:
            print(f"âŒ åŒç¾©è©æå–å¤±æ•—: {str(e)}")
            return {}

    def compute_semantic_similarity_matrix(self, embeddings: np.ndarray) -> np.ndarray:
        """
        è¨ˆç®—è©å½™èªç¾©ç›¸ä¼¼åº¦çŸ©é™£

        Args:
            embeddings: è©å½™åµŒå…¥çŸ©é™£ (vocab_size, embedding_dim)

        Returns:
            ç›¸ä¼¼åº¦çŸ©é™£ (vocab_size, vocab_size)
        """
        try:
            # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
            cosine_sim = cosine_similarity(embeddings)

            # è¨ˆç®—æ­æ°è·é›¢ (è½‰æ›ç‚ºç›¸ä¼¼åº¦)
            euclidean_dist = euclidean_distances(embeddings)
            euclidean_sim = 1 / (1 + euclidean_dist)  # è·é›¢è½‰ç›¸ä¼¼åº¦

            # åŠ æ¬Šèåˆå…©ç¨®ç›¸ä¼¼åº¦
            similarity_matrix = 0.7 * cosine_sim + 0.3 * euclidean_sim

            self.similarity_matrix = similarity_matrix

            print(f"ğŸ“Š ç›¸ä¼¼åº¦çŸ©é™£è¨ˆç®—å®Œæˆ: {similarity_matrix.shape}")

            return similarity_matrix

        except Exception as e:
            print(f"âŒ ç›¸ä¼¼åº¦çŸ©é™£è¨ˆç®—å¤±æ•—: {str(e)}")
            return np.eye(self.vocab_size)

    def find_most_similar_words(self, word: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """
        æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„è©å½™

        Args:
            word: æŸ¥è©¢è©å½™
            top_k: è¿”å›å‰Kå€‹ç›¸ä¼¼è©

        Returns:
            ç›¸ä¼¼è©å½™åˆ—è¡¨ [(è©å½™, ç›¸ä¼¼åº¦), ...]
        """
        try:
            if word not in self.word_to_idx or self.similarity_matrix is None:
                return []

            word_idx = self.word_to_idx[word]
            similarities = self.similarity_matrix[word_idx]

            # æ’é™¤è‡ªå·±ï¼Œå–å‰Kå€‹
            sorted_indices = np.argsort(similarities)[::-1][1:top_k+1]

            similar_words = [
                (self.idx_to_word[idx], similarities[idx])
                for idx in sorted_indices
            ]

            return similar_words

        except Exception as e:
            print(f"âŒ ç›¸ä¼¼è©æŸ¥æ‰¾å¤±æ•—: {str(e)}")
            return []

    def visualize_semantic_space(self, embeddings: np.ndarray, save_path: Optional[Path] = None):
        """
        è¦–è¦ºåŒ–è©å½™èªç¾©ç©ºé–“

        Args:
            embeddings: è©å½™åµŒå…¥çŸ©é™£
            save_path: å„²å­˜è·¯å¾‘
        """
        try:
            # ä½¿ç”¨ PCA é™ç¶­åˆ° 2D
            pca = PCA(n_components=2, random_state=42)
            embeddings_2d = pca.fit_transform(embeddings)

            # å‰µå»ºèªç¾©åˆ†é¡é¡è‰²æ˜ å°„
            category_colors = {
                'å‹•ä½œé¡': 'red',
                'äººç‰©é¡': 'blue',
                'ç‰©å“é¡': 'green',
                'ç‹€æ…‹é¡': 'orange',
                'å ´æ‰€é¡': 'purple',
                'ç–‘å•é¡': 'brown',
                'è‚¯å®šå¦å®šé¡': 'pink'
            }

            # ç¹ªè£½æ•£é»åœ–
            plt.figure(figsize=(12, 8))

            for category, words in self.semantic_clusters.items():
                color = category_colors.get(category, 'gray')
                indices = [self.word_to_idx[word] for word in words if word in self.word_to_idx]

                if indices:
                    x_coords = embeddings_2d[indices, 0]
                    y_coords = embeddings_2d[indices, 1]
                    plt.scatter(x_coords, y_coords, c=color, label=category, alpha=0.7, s=100)

                    # æ·»åŠ è©å½™æ¨™ç±¤
                    for i, word in enumerate(words):
                        if word in self.word_to_idx:
                            idx = self.word_to_idx[word]
                            plt.annotate(word, (embeddings_2d[idx, 0], embeddings_2d[idx, 1]),
                                       fontsize=8, ha='center', va='bottom')

            plt.title('æ‰‹èªè©å½™èªç¾©ç©ºé–“åˆ†å¸ƒ (PCA 2D)', fontsize=16)
            plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)', fontsize=12)
            plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)', fontsize=12)
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.grid(True, alpha=0.3)
            plt.tight_layout()

            if save_path:
                plt.savefig(save_path / 'semantic_space_visualization.png', dpi=300, bbox_inches='tight')
                print(f"ğŸ“Š èªç¾©ç©ºé–“è¦–è¦ºåŒ–å·²å„²å­˜: {save_path / 'semantic_space_visualization.png'}")

            plt.show()

        except Exception as e:
            print(f"âŒ èªç¾©ç©ºé–“è¦–è¦ºåŒ–å¤±æ•—: {str(e)}")


class TextFeatureExtractor:
    """
    å¤šå±¤æ¬¡æ–‡å­—èªç¾©ç‰¹å¾µæå–å™¨

    æ•´åˆ Word2Vecã€FastTextã€BERT ç­‰å¤šç¨®è©åµŒå…¥æŠ€è¡“
    """

    def __init__(self,
                 vocabulary: List[str] = SIGN_LANGUAGE_VOCABULARY,
                 bert_model_name: str = 'bert-base-uncased',
                 embedding_dims: Dict[str, int] = None):
        """
        åˆå§‹åŒ–æ–‡å­—ç‰¹å¾µæå–å™¨

        Args:
            vocabulary: è©å½™åˆ—è¡¨
            bert_model_name: BERT æ¨¡å‹åç¨±
            embedding_dims: å„ç¨®åµŒå…¥çš„ç¶­åº¦é…ç½®
        """
        self.vocabulary = vocabulary
        self.vocab_manager = VocabularyManager(vocabulary)
        self.bert_model_name = bert_model_name

        # é è¨­åµŒå…¥ç¶­åº¦
        if embedding_dims is None:
            embedding_dims = {
                'word2vec': 300,
                'fasttext': 300,
                'bert': 768
            }
        self.embedding_dims = embedding_dims

        # æ¨¡å‹å„²å­˜è·¯å¾‘
        self.model_dir = MODEL_DIR
        self.model_dir.mkdir(parents=True, exist_ok=True)

        # è¼‰å…¥çš„æ¨¡å‹
        self.word2vec_model = None
        self.fasttext_model = None
        self.bert_tokenizer = None
        self.bert_model = None

        # è©å½™åµŒå…¥çŸ©é™£
        self.embeddings = {}

        print(f"ğŸ“ TextFeatureExtractor åˆå§‹åŒ–å®Œæˆ")
        print(f"   - è©å½™æ•¸é‡: {len(vocabulary)}")
        print(f"   - BERT æ¨¡å‹: {bert_model_name}")
        print(f"   - åµŒå…¥ç¶­åº¦: {embedding_dims}")

    def load_pretrained_embeddings(self):
        """
        è¼‰å…¥é è¨“ç·´çš„è©åµŒå…¥æ¨¡å‹
        """
        try:
            print("ğŸ”„ æ­£åœ¨è¼‰å…¥é è¨“ç·´è©åµŒå…¥æ¨¡å‹...")

            # 1. è¼‰å…¥ Word2Vec (ä½¿ç”¨ Google News é è¨“ç·´æ¨¡å‹)
            try:
                word2vec_path = self.model_dir / "GoogleNews-vectors-negative300.bin"
                if word2vec_path.exists():
                    print("   ğŸ“š è¼‰å…¥ Word2Vec æ¨¡å‹...")
                    self.word2vec_model = KeyedVectors.load_word2vec_format(
                        str(word2vec_path), binary=True, limit=500000
                    )
                else:
                    print("   âš ï¸  Word2Vec é è¨“ç·´æ¨¡å‹æœªæ‰¾åˆ°ï¼Œå°‡ä½¿ç”¨éš¨æ©Ÿåˆå§‹åŒ–")
                    self.word2vec_model = None
            except Exception as e:
                print(f"   âŒ Word2Vec è¼‰å…¥å¤±æ•—: {str(e)}")
                self.word2vec_model = None

            # 2. è¼‰å…¥ FastText (ä½¿ç”¨ Common Crawl é è¨“ç·´æ¨¡å‹)
            try:
                fasttext_path = self.model_dir / "cc.en.300.vec"
                if fasttext_path.exists():
                    print("   ğŸ“š è¼‰å…¥ FastText æ¨¡å‹...")
                    self.fasttext_model = KeyedVectors.load_word2vec_format(
                        str(fasttext_path), binary=False, limit=500000
                    )
                else:
                    print("   âš ï¸  FastText é è¨“ç·´æ¨¡å‹æœªæ‰¾åˆ°ï¼Œå°‡ä½¿ç”¨éš¨æ©Ÿåˆå§‹åŒ–")
                    self.fasttext_model = None
            except Exception as e:
                print(f"   âŒ FastText è¼‰å…¥å¤±æ•—: {str(e)}")
                self.fasttext_model = None

            # 3. è¼‰å…¥ BERT
            try:
                print(f"   ğŸ“š è¼‰å…¥ BERT æ¨¡å‹: {self.bert_model_name}")
                self.bert_tokenizer = AutoTokenizer.from_pretrained(self.bert_model_name)
                self.bert_model = AutoModel.from_pretrained(self.bert_model_name)
                self.bert_model.eval()

                # æª¢æŸ¥æ˜¯å¦æœ‰ GPU
                self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                self.bert_model.to(self.device)
                print(f"   ğŸ–¥ï¸  BERT é‹è¡Œè¨­å‚™: {self.device}")

            except Exception as e:
                print(f"   âŒ BERT è¼‰å…¥å¤±æ•—: {str(e)}")
                self.bert_tokenizer = None
                self.bert_model = None

            print("âœ… é è¨“ç·´æ¨¡å‹è¼‰å…¥å®Œæˆ")

        except Exception as e:
            print(f"âŒ é è¨“ç·´æ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}")

    def extract_word2vec_embeddings(self) -> np.ndarray:
        """
        æå– Word2Vec è©åµŒå…¥

        Returns:
            Word2Vec åµŒå…¥çŸ©é™£ (vocab_size, 300)
        """
        try:
            embeddings = []

            for word in self.vocabulary:
                if self.word2vec_model and word in self.word2vec_model:
                    embedding = self.word2vec_model[word]
                else:
                    # éš¨æ©Ÿåˆå§‹åŒ–æœªæ‰¾åˆ°çš„è©å½™
                    embedding = np.random.normal(0, 0.1, self.embedding_dims['word2vec'])

                embeddings.append(embedding)

            embeddings_matrix = np.array(embeddings, dtype=np.float32)

            print(f"ğŸ“Š Word2Vec åµŒå…¥æå–å®Œæˆ: {embeddings_matrix.shape}")
            return embeddings_matrix

        except Exception as e:
            print(f"âŒ Word2Vec åµŒå…¥æå–å¤±æ•—: {str(e)}")
            return np.random.normal(0, 0.1, (len(self.vocabulary), self.embedding_dims['word2vec']))

    def extract_fasttext_embeddings(self) -> np.ndarray:
        """
        æå– FastText è©åµŒå…¥

        Returns:
            FastText åµŒå…¥çŸ©é™£ (vocab_size, 300)
        """
        try:
            embeddings = []

            for word in self.vocabulary:
                if self.fasttext_model and word in self.fasttext_model:
                    embedding = self.fasttext_model[word]
                else:
                    # éš¨æ©Ÿåˆå§‹åŒ–æœªæ‰¾åˆ°çš„è©å½™
                    embedding = np.random.normal(0, 0.1, self.embedding_dims['fasttext'])

                embeddings.append(embedding)

            embeddings_matrix = np.array(embeddings, dtype=np.float32)

            print(f"ğŸ“Š FastText åµŒå…¥æå–å®Œæˆ: {embeddings_matrix.shape}")
            return embeddings_matrix

        except Exception as e:
            print(f"âŒ FastText åµŒå…¥æå–å¤±æ•—: {str(e)}")
            return np.random.normal(0, 0.1, (len(self.vocabulary), self.embedding_dims['fasttext']))

    def extract_bert_embeddings(self) -> np.ndarray:
        """
        æå– BERT è©åµŒå…¥

        Returns:
            BERT åµŒå…¥çŸ©é™£ (vocab_size, 768)
        """
        try:
            if self.bert_model is None or self.bert_tokenizer is None:
                print("âš ï¸  BERT æ¨¡å‹æœªè¼‰å…¥ï¼Œä½¿ç”¨éš¨æ©Ÿåˆå§‹åŒ–")
                return np.random.normal(0, 0.1, (len(self.vocabulary), self.embedding_dims['bert']))

            embeddings = []

            with torch.no_grad():
                for word in self.vocabulary:
                    # Tokenize è©å½™
                    inputs = self.bert_tokenizer(
                        word,
                        return_tensors='pt',
                        padding=True,
                        truncation=True,
                        max_length=512
                    ).to(self.device)

                    # ç²å– BERT è¼¸å‡º
                    outputs = self.bert_model(**inputs)

                    # ä½¿ç”¨ [CLS] token çš„è¡¨ç¤ºä½œç‚ºè©å½™åµŒå…¥
                    cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                    embeddings.append(cls_embedding.squeeze())

            embeddings_matrix = np.array(embeddings, dtype=np.float32)

            print(f"ğŸ“Š BERT åµŒå…¥æå–å®Œæˆ: {embeddings_matrix.shape}")
            return embeddings_matrix

        except Exception as e:
            print(f"âŒ BERT åµŒå…¥æå–å¤±æ•—: {str(e)}")
            return np.random.normal(0, 0.1, (len(self.vocabulary), self.embedding_dims['bert']))

    def extract_all_embeddings(self) -> Dict[str, np.ndarray]:
        """
        æå–æ‰€æœ‰é¡å‹çš„è©åµŒå…¥

        Returns:
            æ‰€æœ‰åµŒå…¥å­—å…¸
        """
        print("ğŸ”„ é–‹å§‹æå–å¤šå±¤æ¬¡è©åµŒå…¥...")

        # è¼‰å…¥é è¨“ç·´æ¨¡å‹
        self.load_pretrained_embeddings()

        # æå–å„ç¨®åµŒå…¥
        self.embeddings = {
            'word2vec': self.extract_word2vec_embeddings(),
            'fasttext': self.extract_fasttext_embeddings(),
            'bert': self.extract_bert_embeddings()
        }

        print("âœ… å¤šå±¤æ¬¡è©åµŒå…¥æå–å®Œæˆ")
        return self.embeddings

    def create_unified_embedding(self, weights: Dict[str, float] = None) -> np.ndarray:
        """
        å‰µå»ºçµ±ä¸€çš„å¤šæ¨¡æ…‹è©åµŒå…¥

        Args:
            weights: å„ç¨®åµŒå…¥çš„æ¬Šé‡

        Returns:
            çµ±ä¸€çš„è©åµŒå…¥çŸ©é™£
        """
        try:
            if weights is None:
                weights = {'word2vec': 0.3, 'fasttext': 0.3, 'bert': 0.4}

            # æ¨™æº–åŒ–å„ç¨®åµŒå…¥
            normalized_embeddings = {}
            for emb_type, embeddings in self.embeddings.items():
                scaler = StandardScaler()
                normalized_embeddings[emb_type] = scaler.fit_transform(embeddings)

            # åŠ æ¬Šèåˆ
            unified_embedding = None
            for emb_type, weight in weights.items():
                if emb_type in normalized_embeddings:
                    if unified_embedding is None:
                        unified_embedding = weight * normalized_embeddings[emb_type]
                    else:
                        # ç¶­åº¦å°é½Š (å¦‚æœéœ€è¦)
                        if unified_embedding.shape[1] != normalized_embeddings[emb_type].shape[1]:
                            # ä½¿ç”¨ PCA çµ±ä¸€ç¶­åº¦åˆ° 300
                            pca = PCA(n_components=300)
                            normalized_embeddings[emb_type] = pca.fit_transform(normalized_embeddings[emb_type])
                            if unified_embedding.shape[1] != 300:
                                pca_unified = PCA(n_components=300)
                                unified_embedding = pca_unified.fit_transform(unified_embedding)

                        unified_embedding += weight * normalized_embeddings[emb_type]

            print(f"ğŸ“Š çµ±ä¸€è©åµŒå…¥å‰µå»ºå®Œæˆ: {unified_embedding.shape}")
            print(f"   - èåˆæ¬Šé‡: {weights}")

            return unified_embedding

        except Exception as e:
            print(f"âŒ çµ±ä¸€è©åµŒå…¥å‰µå»ºå¤±æ•—: {str(e)}")
            return np.random.normal(0, 0.1, (len(self.vocabulary), 300))

    def analyze_semantic_structure(self) -> Dict:
        """
        åˆ†æè©å½™èªç¾©çµæ§‹

        Returns:
            èªç¾©åˆ†æçµæœ
        """
        try:
            print("ğŸ” é–‹å§‹èªç¾©çµæ§‹åˆ†æ...")

            # ç²å–çµ±ä¸€åµŒå…¥
            unified_embedding = self.create_unified_embedding()

            # èªç¾©åˆ†é¡
            semantic_categories = self.vocab_manager.analyze_semantic_categories()

            # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
            similarity_matrix = self.vocab_manager.compute_semantic_similarity_matrix(unified_embedding)

            # æå–åŒç¾©è©
            synonyms_dict = self.vocab_manager.extract_synonyms()

            # K-means èšé¡
            n_clusters = len(semantic_categories)
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            cluster_labels = kmeans.fit_predict(unified_embedding)

            # å»ºç«‹èšé¡çµæœ
            clustering_result = {}
            for i, word in enumerate(self.vocabulary):
                cluster_id = cluster_labels[i]
                if cluster_id not in clustering_result:
                    clustering_result[cluster_id] = []
                clustering_result[cluster_id].append(word)

            analysis_result = {
                'semantic_categories': semantic_categories,
                'similarity_matrix': similarity_matrix.tolist(),
                'synonyms_dict': synonyms_dict,
                'clustering_result': clustering_result,
                'unified_embedding': unified_embedding.tolist(),
                'vocabulary': self.vocabulary,
                'embedding_info': {
                    'dimensions': unified_embedding.shape,
                    'embedding_types': list(self.embeddings.keys()),
                    'total_words': len(self.vocabulary)
                }
            }

            print("âœ… èªç¾©çµæ§‹åˆ†æå®Œæˆ")
            return analysis_result

        except Exception as e:
            print(f"âŒ èªç¾©çµæ§‹åˆ†æå¤±æ•—: {str(e)}")
            return {}

    def get_word_embedding(self, word: str, embedding_type: str = 'unified') -> Optional[np.ndarray]:
        """
        ç²å–ç‰¹å®šè©å½™çš„åµŒå…¥å‘é‡

        Args:
            word: è©å½™
            embedding_type: åµŒå…¥é¡å‹ ('word2vec', 'fasttext', 'bert', 'unified')

        Returns:
            è©åµŒå…¥å‘é‡æˆ– None
        """
        try:
            if word not in self.vocab_manager.word_to_idx:
                return None

            word_idx = self.vocab_manager.word_to_idx[word]

            if embedding_type == 'unified':
                unified_embedding = self.create_unified_embedding()
                return unified_embedding[word_idx]
            elif embedding_type in self.embeddings:
                return self.embeddings[embedding_type][word_idx]
            else:
                print(f"âŒ æœªçŸ¥çš„åµŒå…¥é¡å‹: {embedding_type}")
                return None

        except Exception as e:
            print(f"âŒ è©åµŒå…¥ç²å–å¤±æ•—: {str(e)}")
            return None


def run_text_feature_extraction(
    output_root: Optional[str] = None,
    semantic_output_root: Optional[str] = None,
    save_models: bool = True
) -> Dict:
    """
    åŸ·è¡Œå®Œæ•´çš„æ–‡å­—ç‰¹å¾µæå–æµç¨‹

    Args:
        output_root: åµŒå…¥è¼¸å‡ºè·¯å¾‘
        semantic_output_root: èªç¾©åˆ†æè¼¸å‡ºè·¯å¾‘
        save_models: æ˜¯å¦å„²å­˜æ¨¡å‹

    Returns:
        è™•ç†çµæœ
    """
    # è·¯å¾‘é…ç½®
    output_path = Path(output_root) if output_root else OUTPUT_ROOT
    semantic_path = Path(semantic_output_root) if semantic_output_root else SEMANTIC_OUTPUT_ROOT

    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    output_path.mkdir(parents=True, exist_ok=True)
    semantic_path.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ“ é–‹å§‹æ–‡å­—ç‰¹å¾µæå–")
    print(f"   ğŸ“‚ åµŒå…¥è¼¸å‡ºè·¯å¾‘: {output_path}")
    print(f"   ğŸ“‚ èªç¾©åˆ†æè¼¸å‡ºè·¯å¾‘: {semantic_path}")

    # åˆå§‹åŒ–ç‰¹å¾µæå–å™¨
    extractor = TextFeatureExtractor()

    try:
        # æå–æ‰€æœ‰åµŒå…¥
        all_embeddings = extractor.extract_all_embeddings()

        # å„²å­˜å„ç¨®åµŒå…¥
        for emb_type, embeddings in all_embeddings.items():
            emb_file = output_path / f"{emb_type}_embeddings.npy"
            np.save(emb_file, embeddings)
            print(f"ğŸ’¾ {emb_type} åµŒå…¥å·²å„²å­˜: {emb_file}")

        # å‰µå»ºçµ±ä¸€åµŒå…¥
        unified_embedding = extractor.create_unified_embedding()
        unified_file = output_path / "unified_embeddings.npy"
        np.save(unified_file, unified_embedding)
        print(f"ğŸ’¾ çµ±ä¸€åµŒå…¥å·²å„²å­˜: {unified_file}")

        # èªç¾©çµæ§‹åˆ†æ
        semantic_analysis = extractor.analyze_semantic_structure()

        # å„²å­˜èªç¾©åˆ†æçµæœ
        semantic_file = semantic_path / "semantic_analysis.json"
        with open(semantic_file, 'w', encoding='utf-8') as f:
            json.dump(semantic_analysis, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ èªç¾©åˆ†æçµæœå·²å„²å­˜: {semantic_file}")

        # å„²å­˜è©å½™æ˜ å°„
        vocab_mapping = {
            'vocabulary': extractor.vocabulary,
            'word_to_idx': extractor.vocab_manager.word_to_idx,
            'idx_to_word': extractor.vocab_manager.idx_to_word
        }
        vocab_file = output_path / "vocabulary_mapping.json"
        with open(vocab_file, 'w', encoding='utf-8') as f:
            json.dump(vocab_mapping, f, indent=2, ensure_ascii=False)

        # è¦–è¦ºåŒ–èªç¾©ç©ºé–“
        extractor.vocab_manager.visualize_semantic_space(unified_embedding, semantic_path)

        # è™•ç†çµæœ
        results = {
            'success': True,
            'vocabulary_size': len(extractor.vocabulary),
            'embedding_types': list(all_embeddings.keys()),
            'embedding_dimensions': {k: v.shape for k, v in all_embeddings.items()},
            'semantic_categories': len(semantic_analysis.get('semantic_categories', {})),
            'output_files': {
                'embeddings': str(output_path),
                'semantic_analysis': str(semantic_path)
            }
        }

        print(f"ğŸ“ æ–‡å­—ç‰¹å¾µæå–å®Œæˆ!")
        print(f"   âœ… è©å½™æ•¸é‡: {results['vocabulary_size']}")
        print(f"   âœ… åµŒå…¥é¡å‹: {', '.join(results['embedding_types'])}")
        print(f"   âœ… èªç¾©åˆ†é¡: {results['semantic_categories']}å€‹é¡åˆ¥")

        return results

    except Exception as e:
        print(f"âŒ æ–‡å­—ç‰¹å¾µæå–å¤±æ•—: {str(e)}")
        return {'success': False, 'error': str(e)}


if __name__ == "__main__":
    # åŸ·è¡Œæ–‡å­—ç‰¹å¾µæå–
    results = run_text_feature_extraction()