#!/usr/bin/env python3
"""
å¤šæ¨¡æ…‹èåˆå™¨ - Multi_SignView å¤šæ¨¡æ…‹æ‰‹èªè¾¨è­˜ç³»çµ±
æ•´åˆè¦–è¦ºã€èªéŸ³ã€æ–‡å­—ä¸‰å¤§æ¨¡æ…‹ç‰¹å¾µï¼Œå¯¦ç¾çµ±ä¸€çš„å¤šæ¨¡æ…‹è¡¨ç¤ºå­¸ç¿’
æ”¯æ´æ—©æœŸèåˆã€ä¸­æœŸèåˆã€æ™šæœŸèåˆå¤šå±¤æ¬¡ç­–ç•¥

æŠ€è¡“æ¶æ§‹ï¼š
- è¦–è¦ºç‰¹å¾µï¼šMediaPipe(æ‰‹éƒ¨+å§¿æ…‹+è‡‰éƒ¨) + å…‰æµé‹å‹• â†’ æ¨™æº–åŒ–åˆ°512ç¶­
- èªéŸ³ç‰¹å¾µï¼šè©å½™TTSèªéŸ³çš„MFCC+Spectral+Temporal â†’ æ¨™æº–åŒ–åˆ°24ç¶­
- æ–‡å­—ç‰¹å¾µï¼šWord2Vec+FastText+BERTè©åµŒå…¥ â†’ æ¨™æº–åŒ–åˆ°300ç¶­
- èåˆè¼¸å‡ºï¼šä¸‰æ¨¡æ…‹æ³¨æ„åŠ›èåˆ â†’ çµ±ä¸€256ç¶­è¡¨ç¤º

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. è·¨æ¨¡æ…‹ç‰¹å¾µå°é½Šå’Œæ¨™æº–åŒ–
2. Multi-Head Cross-Modal Attention æ³¨æ„åŠ›æ©Ÿåˆ¶
3. æ—©æœŸ/ä¸­æœŸ/æ™šæœŸå¤šå±¤æ¬¡èåˆç­–ç•¥
4. è‡ªé©æ‡‰æ¨¡æ…‹æ¬Šé‡å­¸ç¿’
5. ç¼ºå¤±æ¨¡æ…‹è£œå„Ÿå’Œé­¯æ£’æ€§è¨­è¨ˆ

Author: Claude Code + Multi_SignView Team
Date: 2024
"""

import os
import sys
import json
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any
import pickle
import time

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.ensemble import VotingClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# æ·±åº¦å­¸ç¿’æ¡†æ¶
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim

# è·¯å¾‘é…ç½®
PROJECT_ROOT = Path(__file__).parent
FEATURES_ROOT = PROJECT_ROOT / "features"
VISUAL_FEATURES_ROOT = FEATURES_ROOT / "mediapipe_features"
OPTICAL_FLOW_ROOT = FEATURES_ROOT / "optical_flow_features"
AUDIO_FEATURES_ROOT = FEATURES_ROOT / "audio_features"
TEXT_FEATURES_ROOT = FEATURES_ROOT / "text_embeddings"
OUTPUT_ROOT = FEATURES_ROOT / "multimodal_features"

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# 30å€‹æ‰‹èªè©å½™
SIGN_LANGUAGE_VOCABULARY = [
    'again', 'bird', 'book', 'computer', 'cousin', 'deaf', 'drink', 'eat',
    'finish', 'fish', 'friend', 'good', 'happy', 'learn', 'like', 'mother',
    'need', 'nice', 'no', 'orange', 'school', 'sister', 'student', 'table',
    'teacher', 'tired', 'want', 'what', 'white', 'yes'
]


class ModalityAlignment:
    """
    è·¨æ¨¡æ…‹ç‰¹å¾µå°é½Šå’Œæ¨™æº–åŒ–æ¨¡çµ„

    è² è²¬å°‡ä¸åŒç¶­åº¦å’Œåˆ†ä½ˆçš„æ¨¡æ…‹ç‰¹å¾µå°é½Šåˆ°çµ±ä¸€ç©ºé–“
    """

    def __init__(self, target_visual_dim: int = 512,
                 target_audio_dim: int = 64,
                 target_text_dim: int = 256):
        """
        åˆå§‹åŒ–æ¨¡æ…‹å°é½Šå™¨

        Args:
            target_visual_dim: è¦–è¦ºç‰¹å¾µç›®æ¨™ç¶­åº¦
            target_audio_dim: éŸ³è¨Šç‰¹å¾µç›®æ¨™ç¶­åº¦
            target_text_dim: æ–‡å­—ç‰¹å¾µç›®æ¨™ç¶­åº¦
        """
        self.target_dims = {
            'visual': target_visual_dim,
            'audio': target_audio_dim,
            'text': target_text_dim
        }

        # ç‰¹å¾µæ¨™æº–åŒ–å™¨
        self.scalers = {}
        self.pca_reducers = {}

        print(f"ğŸ”— ModalityAlignment åˆå§‹åŒ–å®Œæˆ")
        print(f"   - ç›®æ¨™ç¶­åº¦: Visual({target_visual_dim}) + Audio({target_audio_dim}) + Text({target_text_dim})")

    def fit_alignment(self,
                     visual_features: np.ndarray,
                     audio_features: np.ndarray,
                     text_features: np.ndarray) -> Dict[str, Any]:
        """
        è¨“ç·´æ¨¡æ…‹å°é½Šåƒæ•¸

        Args:
            visual_features: è¦–è¦ºç‰¹å¾µçŸ©é™£ (N, visual_dim)
            audio_features: éŸ³è¨Šç‰¹å¾µçŸ©é™£ (N, audio_dim)
            text_features: æ–‡å­—ç‰¹å¾µçŸ©é™£ (N, text_dim)

        Returns:
            å°é½Šåƒæ•¸å­—å…¸
        """
        try:
            print("ğŸ”„ é–‹å§‹è¨“ç·´è·¨æ¨¡æ…‹ç‰¹å¾µå°é½Š...")

            alignment_info = {}

            # 1. è¦–è¦ºç‰¹å¾µå°é½Š
            if visual_features.shape[1] != self.target_dims['visual']:
                print(f"   ğŸ“¹ è¦–è¦ºç‰¹å¾µç¶­åº¦èª¿æ•´: {visual_features.shape[1]} â†’ {self.target_dims['visual']}")

                # æ¨™æº–åŒ–
                visual_scaler = StandardScaler()
                visual_features_scaled = visual_scaler.fit_transform(visual_features)
                self.scalers['visual'] = visual_scaler

                # ç¶­åº¦èª¿æ•´ï¼ˆPCAæˆ–æ“´å±•ï¼‰
                if visual_features.shape[1] > self.target_dims['visual']:
                    # é™ç¶­
                    visual_pca = PCA(n_components=self.target_dims['visual'], random_state=42)
                    visual_pca.fit(visual_features_scaled)
                    self.pca_reducers['visual'] = visual_pca
                    alignment_info['visual_variance_ratio'] = np.sum(visual_pca.explained_variance_ratio_)
                else:
                    # å‡ç¶­ï¼ˆé›¶å¡«å……ï¼‰
                    self.pca_reducers['visual'] = None

            # 2. éŸ³è¨Šç‰¹å¾µå°é½Š
            if audio_features.shape[1] != self.target_dims['audio']:
                print(f"   ğŸ¤ éŸ³è¨Šç‰¹å¾µç¶­åº¦èª¿æ•´: {audio_features.shape[1]} â†’ {self.target_dims['audio']}")

                # æ¨™æº–åŒ–
                audio_scaler = StandardScaler()
                audio_features_scaled = audio_scaler.fit_transform(audio_features)
                self.scalers['audio'] = audio_scaler

                # ç¶­åº¦èª¿æ•´
                if audio_features.shape[1] > self.target_dims['audio']:
                    audio_pca = PCA(n_components=self.target_dims['audio'], random_state=42)
                    audio_pca.fit(audio_features_scaled)
                    self.pca_reducers['audio'] = audio_pca
                    alignment_info['audio_variance_ratio'] = np.sum(audio_pca.explained_variance_ratio_)
                else:
                    self.pca_reducers['audio'] = None

            # 3. æ–‡å­—ç‰¹å¾µå°é½Š
            if text_features.shape[1] != self.target_dims['text']:
                print(f"   ğŸ“ æ–‡å­—ç‰¹å¾µç¶­åº¦èª¿æ•´: {text_features.shape[1]} â†’ {self.target_dims['text']}")

                # æ¨™æº–åŒ–
                text_scaler = StandardScaler()
                text_features_scaled = text_scaler.fit_transform(text_features)
                self.scalers['text'] = text_scaler

                # ç¶­åº¦èª¿æ•´
                if text_features.shape[1] > self.target_dims['text']:
                    text_pca = PCA(n_components=self.target_dims['text'], random_state=42)
                    text_pca.fit(text_features_scaled)
                    self.pca_reducers['text'] = text_pca
                    alignment_info['text_variance_ratio'] = np.sum(text_pca.explained_variance_ratio_)
                else:
                    self.pca_reducers['text'] = None

            print("âœ… è·¨æ¨¡æ…‹ç‰¹å¾µå°é½Šè¨“ç·´å®Œæˆ")
            return alignment_info

        except Exception as e:
            print(f"âŒ è·¨æ¨¡æ…‹ç‰¹å¾µå°é½Šè¨“ç·´å¤±æ•—: {str(e)}")
            return {}

    def transform_modalities(self,
                           visual_features: Optional[np.ndarray] = None,
                           audio_features: Optional[np.ndarray] = None,
                           text_features: Optional[np.ndarray] = None) -> Dict[str, np.ndarray]:
        """
        è½‰æ›æ¨¡æ…‹ç‰¹å¾µåˆ°å°é½Šç©ºé–“

        Args:
            visual_features: è¦–è¦ºç‰¹å¾µ
            audio_features: éŸ³è¨Šç‰¹å¾µ
            text_features: æ–‡å­—ç‰¹å¾µ

        Returns:
            å°é½Šå¾Œçš„ç‰¹å¾µå­—å…¸
        """
        try:
            aligned_features = {}

            # 1. è½‰æ›è¦–è¦ºç‰¹å¾µ
            if visual_features is not None:
                if 'visual' in self.scalers:
                    visual_aligned = self.scalers['visual'].transform(visual_features)
                else:
                    visual_aligned = visual_features.copy()

                if self.pca_reducers.get('visual') is not None:
                    visual_aligned = self.pca_reducers['visual'].transform(visual_aligned)
                elif visual_aligned.shape[1] < self.target_dims['visual']:
                    # é›¶å¡«å……æ“´å±•
                    padding = np.zeros((visual_aligned.shape[0],
                                      self.target_dims['visual'] - visual_aligned.shape[1]))
                    visual_aligned = np.concatenate([visual_aligned, padding], axis=1)

                aligned_features['visual'] = visual_aligned

            # 2. è½‰æ›éŸ³è¨Šç‰¹å¾µ
            if audio_features is not None:
                if 'audio' in self.scalers:
                    audio_aligned = self.scalers['audio'].transform(audio_features)
                else:
                    audio_aligned = audio_features.copy()

                if self.pca_reducers.get('audio') is not None:
                    audio_aligned = self.pca_reducers['audio'].transform(audio_aligned)
                elif audio_aligned.shape[1] < self.target_dims['audio']:
                    # é›¶å¡«å……æ“´å±•
                    padding = np.zeros((audio_aligned.shape[0],
                                      self.target_dims['audio'] - audio_aligned.shape[1]))
                    audio_aligned = np.concatenate([audio_aligned, padding], axis=1)

                aligned_features['audio'] = audio_aligned

            # 3. è½‰æ›æ–‡å­—ç‰¹å¾µ
            if text_features is not None:
                if 'text' in self.scalers:
                    text_aligned = self.scalers['text'].transform(text_features)
                else:
                    text_aligned = text_features.copy()

                if self.pca_reducers.get('text') is not None:
                    text_aligned = self.pca_reducers['text'].transform(text_aligned)
                elif text_aligned.shape[1] < self.target_dims['text']:
                    # é›¶å¡«å……æ“´å±•
                    padding = np.zeros((text_aligned.shape[0],
                                      self.target_dims['text'] - text_aligned.shape[1]))
                    text_aligned = np.concatenate([text_aligned, padding], axis=1)

                aligned_features['text'] = text_aligned

            return aligned_features

        except Exception as e:
            print(f"âŒ æ¨¡æ…‹ç‰¹å¾µè½‰æ›å¤±æ•—: {str(e)}")
            return {}


class CrossModalAttention(nn.Module):
    """
    è·¨æ¨¡æ…‹æ³¨æ„åŠ›æ©Ÿåˆ¶

    å¯¦ç¾Multi-Head Cross-Modal Attentionï¼Œè¨ˆç®—æ¨¡æ…‹é–“çš„ä¾è³´é—œä¿‚
    """

    def __init__(self,
                 visual_dim: int = 512,
                 audio_dim: int = 64,
                 text_dim: int = 256,
                 hidden_dim: int = 256,
                 num_heads: int = 8):
        """
        åˆå§‹åŒ–è·¨æ¨¡æ…‹æ³¨æ„åŠ›

        Args:
            visual_dim: è¦–è¦ºç‰¹å¾µç¶­åº¦
            audio_dim: éŸ³è¨Šç‰¹å¾µç¶­åº¦
            text_dim: æ–‡å­—ç‰¹å¾µç¶­åº¦
            hidden_dim: éš±è—å±¤ç¶­åº¦
            num_heads: æ³¨æ„åŠ›é ­æ•¸
        """
        super(CrossModalAttention, self).__init__()

        self.visual_dim = visual_dim
        self.audio_dim = audio_dim
        self.text_dim = text_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads

        # ç¢ºä¿hidden_dimèƒ½è¢«num_headsæ•´é™¤
        assert hidden_dim % num_heads == 0
        self.head_dim = hidden_dim // num_heads

        # æ¨¡æ…‹æŠ•å½±å±¤
        self.visual_proj = nn.Linear(visual_dim, hidden_dim)
        self.audio_proj = nn.Linear(audio_dim, hidden_dim)
        self.text_proj = nn.Linear(text_dim, hidden_dim)

        # Multi-Head Attention çµ„ä»¶
        self.query_proj = nn.Linear(hidden_dim, hidden_dim)
        self.key_proj = nn.Linear(hidden_dim, hidden_dim)
        self.value_proj = nn.Linear(hidden_dim, hidden_dim)

        # è¼¸å‡ºæŠ•å½±
        self.output_proj = nn.Linear(hidden_dim, hidden_dim)

        # LayerNorm å’Œ Dropout
        self.layer_norm = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(0.1)

        print(f"ğŸ”— CrossModalAttention åˆå§‹åŒ–å®Œæˆ")
        print(f"   - è¼¸å…¥ç¶­åº¦: Visual({visual_dim}) + Audio({audio_dim}) + Text({text_dim})")
        print(f"   - éš±è—ç¶­åº¦: {hidden_dim}, æ³¨æ„åŠ›é ­æ•¸: {num_heads}")

    def forward(self,
                visual_feat: Optional[torch.Tensor] = None,
                audio_feat: Optional[torch.Tensor] = None,
                text_feat: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        å‰å‘å‚³æ’­

        Args:
            visual_feat: è¦–è¦ºç‰¹å¾µ (batch_size, visual_dim)
            audio_feat: éŸ³è¨Šç‰¹å¾µ (batch_size, audio_dim)
            text_feat: æ–‡å­—ç‰¹å¾µ (batch_size, text_dim)

        Returns:
            è·¨æ¨¡æ…‹æ³¨æ„åŠ›çµæœå­—å…¸
        """
        batch_size = None
        modalities = {}

        # æŠ•å½±åˆ°çµ±ä¸€éš±è—ç©ºé–“
        if visual_feat is not None:
            batch_size = visual_feat.size(0)
            modalities['visual'] = self.visual_proj(visual_feat)

        if audio_feat is not None:
            batch_size = audio_feat.size(0)
            modalities['audio'] = self.audio_proj(audio_feat)

        if text_feat is not None:
            batch_size = text_feat.size(0)
            modalities['text'] = self.text_proj(text_feat)

        if not modalities:
            return {}

        # è¨ˆç®—è·¨æ¨¡æ…‹æ³¨æ„åŠ›
        attention_results = {}
        modality_names = list(modalities.keys())

        for i, query_modal in enumerate(modality_names):
            for j, key_modal in enumerate(modality_names):
                if i != j:  # è·¨æ¨¡æ…‹æ³¨æ„åŠ›ï¼Œä¸è¨ˆç®—è‡ªæ³¨æ„åŠ›
                    attention_key = f"{query_modal}_to_{key_modal}"

                    # Multi-Head Attention
                    query = self.query_proj(modalities[query_modal])  # (batch, hidden)
                    key = self.key_proj(modalities[key_modal])        # (batch, hidden)
                    value = self.value_proj(modalities[key_modal])    # (batch, hidden)

                    # Reshapeç‚ºå¤šé ­
                    query = query.view(batch_size, self.num_heads, self.head_dim)  # (batch, heads, head_dim)
                    key = key.view(batch_size, self.num_heads, self.head_dim)
                    value = value.view(batch_size, self.num_heads, self.head_dim)

                    # è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸
                    attention_scores = torch.sum(query * key, dim=-1) / np.sqrt(self.head_dim)  # (batch, heads)
                    attention_weights = F.softmax(attention_scores, dim=-1)  # (batch, heads)

                    # æ‡‰ç”¨æ³¨æ„åŠ›åˆ°value
                    attended_value = attention_weights.unsqueeze(-1) * value  # (batch, heads, head_dim)
                    attended_value = attended_value.view(batch_size, -1)  # (batch, hidden)

                    # è¼¸å‡ºæŠ•å½±å’Œæ®˜å·®é€£æ¥
                    attended_output = self.output_proj(attended_value)
                    attended_output = self.layer_norm(attended_output + modalities[query_modal])
                    attended_output = self.dropout(attended_output)

                    attention_results[attention_key] = {
                        'attended_features': attended_output,
                        'attention_weights': attention_weights
                    }

        return attention_results

    def get_fusion_features(self, attention_results: Dict[str, Dict]) -> torch.Tensor:
        """
        å¾æ³¨æ„åŠ›çµæœä¸­æå–èåˆç‰¹å¾µ

        Args:
            attention_results: æ³¨æ„åŠ›è¨ˆç®—çµæœ

        Returns:
            èåˆå¾Œçš„ç‰¹å¾µå‘é‡
        """
        try:
            # æ”¶é›†æ‰€æœ‰attended features
            attended_features = []
            for attention_key, result in attention_results.items():
                attended_features.append(result['attended_features'])

            if attended_features:
                # å¹³å‡æ± åŒ–èåˆ
                fusion_features = torch.mean(torch.stack(attended_features), dim=0)
                return fusion_features
            else:
                return torch.empty(0)

        except Exception as e:
            print(f"âŒ èåˆç‰¹å¾µæå–å¤±æ•—: {str(e)}")
            return torch.empty(0)


class MultiModalFusion:
    """
    å¤šæ¨¡æ…‹èåˆä¸»æ§åˆ¶å™¨

    æ•´åˆç‰¹å¾µå°é½Šã€è·¨æ¨¡æ…‹æ³¨æ„åŠ›ã€å¤šå±¤æ¬¡èåˆç­–ç•¥
    """

    def __init__(self,
                 visual_dim: int = 512,
                 audio_dim: int = 64,
                 text_dim: int = 256,
                 output_dim: int = 256,
                 fusion_strategies: List[str] = ['early', 'attention', 'late']):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ…‹èåˆå™¨

        Args:
            visual_dim: è¦–è¦ºç‰¹å¾µç¶­åº¦
            audio_dim: éŸ³è¨Šç‰¹å¾µç¶­åº¦
            text_dim: æ–‡å­—ç‰¹å¾µç¶­åº¦
            output_dim: è¼¸å‡ºç‰¹å¾µç¶­åº¦
            fusion_strategies: èåˆç­–ç•¥åˆ—è¡¨
        """
        self.visual_dim = visual_dim
        self.audio_dim = audio_dim
        self.text_dim = text_dim
        self.output_dim = output_dim
        self.fusion_strategies = fusion_strategies

        # åˆå§‹åŒ–çµ„ä»¶
        self.alignment = ModalityAlignment(visual_dim, audio_dim, text_dim)

        # PyTorchè¨­å‚™
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # è·¨æ¨¡æ…‹æ³¨æ„åŠ›ï¼ˆå»¶é²åˆå§‹åŒ–ï¼‰
        self.cross_attention = None

        # èåˆçµæœå­˜å„²
        self.fusion_results = {}

        print(f"ğŸ”€ MultiModalFusion åˆå§‹åŒ–å®Œæˆ")
        print(f"   - è¨­å‚™: {self.device}")
        print(f"   - èåˆç­–ç•¥: {', '.join(fusion_strategies)}")
        print(f"   - è¼¸å‡ºç¶­åº¦: {output_dim}")

    def fit(self,
            visual_features: Optional[np.ndarray] = None,
            audio_features: Optional[np.ndarray] = None,
            text_features: Optional[np.ndarray] = None) -> Dict[str, Any]:
        """
        è¨“ç·´å¤šæ¨¡æ…‹èåˆå™¨

        Args:
            visual_features: è¦–è¦ºç‰¹å¾µçŸ©é™£
            audio_features: éŸ³è¨Šç‰¹å¾µçŸ©é™£
            text_features: æ–‡å­—ç‰¹å¾µçŸ©é™£

        Returns:
            è¨“ç·´ä¿¡æ¯å­—å…¸
        """
        try:
            print("ğŸ”„ é–‹å§‹è¨“ç·´å¤šæ¨¡æ…‹èåˆå™¨...")

            training_info = {}

            # 1. ç‰¹å¾µå°é½Šè¨“ç·´
            if all(feat is not None for feat in [visual_features, audio_features, text_features]):
                alignment_info = self.alignment.fit_alignment(
                    visual_features, audio_features, text_features
                )
                training_info['alignment'] = alignment_info

                # 2. åˆå§‹åŒ–è·¨æ¨¡æ…‹æ³¨æ„åŠ›
                self.cross_attention = CrossModalAttention(
                    self.visual_dim, self.audio_dim, self.text_dim,
                    hidden_dim=self.output_dim, num_heads=8
                ).to(self.device)

                training_info['cross_attention_initialized'] = True

                print("âœ… å¤šæ¨¡æ…‹èåˆå™¨è¨“ç·´å®Œæˆ")
            else:
                print("âš ï¸  éƒ¨åˆ†æ¨¡æ…‹ç‰¹å¾µç¼ºå¤±ï¼Œåƒ…é€²è¡Œéƒ¨åˆ†è¨“ç·´")

            return training_info

        except Exception as e:
            print(f"âŒ å¤šæ¨¡æ…‹èåˆå™¨è¨“ç·´å¤±æ•—: {str(e)}")
            return {}

    def fuse_features(self,
                     visual_features: Optional[np.ndarray] = None,
                     audio_features: Optional[np.ndarray] = None,
                     text_features: Optional[np.ndarray] = None,
                     strategy: str = 'attention') -> Dict[str, np.ndarray]:
        """
        åŸ·è¡Œå¤šæ¨¡æ…‹ç‰¹å¾µèåˆ

        Args:
            visual_features: è¦–è¦ºç‰¹å¾µ
            audio_features: éŸ³è¨Šç‰¹å¾µ
            text_features: æ–‡å­—ç‰¹å¾µ
            strategy: èåˆç­–ç•¥ ('early', 'attention', 'late')

        Returns:
            èåˆçµæœå­—å…¸
        """
        try:
            print(f"ğŸ”€ åŸ·è¡Œ {strategy} èåˆç­–ç•¥...")

            fusion_results = {}

            # 1. ç‰¹å¾µå°é½Š
            aligned_features = self.alignment.transform_modalities(
                visual_features, audio_features, text_features
            )

            if not aligned_features:
                return {}

            # 2. æ ¹æ“šç­–ç•¥åŸ·è¡Œèåˆ
            if strategy == 'early':
                # æ—©æœŸèåˆï¼šç‰¹å¾µæ‹¼æ¥
                fusion_results = self._early_fusion(aligned_features)

            elif strategy == 'attention' and self.cross_attention:
                # ä¸­æœŸèåˆï¼šè·¨æ¨¡æ…‹æ³¨æ„åŠ›
                fusion_results = self._attention_fusion(aligned_features)

            elif strategy == 'late':
                # æ™šæœŸèåˆï¼šæ±ºç­–ç´šèåˆ
                fusion_results = self._late_fusion(aligned_features)

            else:
                print(f"âŒ ä¸æ”¯æ´çš„èåˆç­–ç•¥æˆ–æœªåˆå§‹åŒ–: {strategy}")
                return {}

            print(f"âœ… {strategy} èåˆå®Œæˆ")
            return fusion_results

        except Exception as e:
            print(f"âŒ å¤šæ¨¡æ…‹ç‰¹å¾µèåˆå¤±æ•—: {str(e)}")
            return {}

    def _early_fusion(self, aligned_features: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """
        æ—©æœŸèåˆï¼šç‰¹å¾µæ‹¼æ¥å’Œé™ç¶­

        Args:
            aligned_features: å°é½Šå¾Œçš„ç‰¹å¾µå­—å…¸

        Returns:
            æ—©æœŸèåˆçµæœ
        """
        try:
            # æ‹¼æ¥æ‰€æœ‰å¯ç”¨ç‰¹å¾µ
            feature_list = []
            modality_info = []

            for modality, features in aligned_features.items():
                feature_list.append(features)
                modality_info.append(f"{modality}({features.shape[1]})")

            if not feature_list:
                return {}

            # ç‰¹å¾µæ‹¼æ¥
            concatenated_features = np.concatenate(feature_list, axis=1)

            print(f"   ğŸ“Š æ—©æœŸèåˆ: {' + '.join(modality_info)} = {concatenated_features.shape[1]}ç¶­")

            # é™ç¶­åˆ°ç›®æ¨™ç¶­åº¦
            if concatenated_features.shape[1] > self.output_dim:
                pca = PCA(n_components=self.output_dim, random_state=42)
                fused_features = pca.fit_transform(concatenated_features)
                variance_ratio = np.sum(pca.explained_variance_ratio_)
                print(f"   ğŸ“‰ PCAé™ç¶­ä¿ç•™ {variance_ratio:.2%} æ–¹å·®")
            else:
                fused_features = concatenated_features

            return {
                'early_fusion': fused_features,
                'concatenated_features': concatenated_features,
                'modality_info': modality_info
            }

        except Exception as e:
            print(f"âŒ æ—©æœŸèåˆå¤±æ•—: {str(e)}")
            return {}

    def _attention_fusion(self, aligned_features: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """
        æ³¨æ„åŠ›èåˆï¼šè·¨æ¨¡æ…‹æ³¨æ„åŠ›æ©Ÿåˆ¶

        Args:
            aligned_features: å°é½Šå¾Œçš„ç‰¹å¾µå­—å…¸

        Returns:
            æ³¨æ„åŠ›èåˆçµæœ
        """
        try:
            # è½‰æ›ç‚ºtorch tensor
            torch_features = {}
            for modality, features in aligned_features.items():
                torch_features[modality] = torch.tensor(features, dtype=torch.float32).to(self.device)

            # è·¨æ¨¡æ…‹æ³¨æ„åŠ›è¨ˆç®—
            with torch.no_grad():
                attention_results = self.cross_attention(
                    visual_feat=torch_features.get('visual'),
                    audio_feat=torch_features.get('audio'),
                    text_feat=torch_features.get('text')
                )

                if attention_results:
                    # æå–èåˆç‰¹å¾µ
                    fusion_features = self.cross_attention.get_fusion_features(attention_results)
                    fusion_features_np = fusion_features.cpu().numpy()

                    # æå–æ³¨æ„åŠ›æ¬Šé‡
                    attention_weights = {}
                    for key, result in attention_results.items():
                        attention_weights[key] = result['attention_weights'].cpu().numpy()

                    print(f"   ğŸ”— æ³¨æ„åŠ›èåˆ: {len(attention_results)} å€‹è·¨æ¨¡æ…‹é€£æ¥")

                    return {
                        'attention_fusion': fusion_features_np,
                        'attention_weights': attention_weights,
                        'attention_details': attention_results
                    }

            return {}

        except Exception as e:
            print(f"âŒ æ³¨æ„åŠ›èåˆå¤±æ•—: {str(e)}")
            return {}

    def _late_fusion(self, aligned_features: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """
        æ™šæœŸèåˆï¼šæ±ºç­–ç´šèåˆ

        Args:
            aligned_features: å°é½Šå¾Œçš„ç‰¹å¾µå­—å…¸

        Returns:
            æ™šæœŸèåˆçµæœ
        """
        try:
            # å°æ¯å€‹æ¨¡æ…‹åˆ†åˆ¥é€²è¡Œç‰¹å¾µè™•ç†
            modality_representations = {}

            for modality, features in aligned_features.items():
                # æ¨™æº–åŒ–
                scaler = StandardScaler()
                normalized_features = scaler.fit_transform(features)

                # é™ç¶­åˆ°çµ±ä¸€ç¶­åº¦
                if normalized_features.shape[1] > self.output_dim // len(aligned_features):
                    target_dim = self.output_dim // len(aligned_features)
                    pca = PCA(n_components=target_dim, random_state=42)
                    reduced_features = pca.fit_transform(normalized_features)
                else:
                    reduced_features = normalized_features

                modality_representations[modality] = reduced_features

            # åŠ æ¬Šèåˆï¼ˆå¯ä»¥æ˜¯å­¸ç¿’çš„æ¬Šé‡ï¼‰
            modality_weights = {
                'visual': 0.5,
                'audio': 0.3,
                'text': 0.2
            }

            # æ¨™æº–åŒ–æ¬Šé‡
            available_modalities = list(modality_representations.keys())
            total_weight = sum(modality_weights.get(m, 1.0) for m in available_modalities)
            normalized_weights = {m: modality_weights.get(m, 1.0) / total_weight
                                for m in available_modalities}

            # åŠ æ¬Šèåˆ
            fusion_result = None
            for modality, features in modality_representations.items():
                weight = normalized_weights[modality]
                if fusion_result is None:
                    fusion_result = weight * features
                else:
                    # ç¶­åº¦å°é½Š
                    min_dim = min(fusion_result.shape[1], features.shape[1])
                    fusion_result = fusion_result[:, :min_dim] + weight * features[:, :min_dim]

            print(f"   âš–ï¸  æ™šæœŸèåˆæ¬Šé‡: {normalized_weights}")

            return {
                'late_fusion': fusion_result,
                'modality_weights': normalized_weights,
                'modality_representations': modality_representations
            }

        except Exception as e:
            print(f"âŒ æ™šæœŸèåˆå¤±æ•—: {str(e)}")
            return {}

    def save_fusion_model(self, save_path: Path):
        """
        ä¿å­˜èåˆæ¨¡å‹

        Args:
            save_path: ä¿å­˜è·¯å¾‘
        """
        try:
            save_data = {
                'alignment': self.alignment,
                'fusion_params': {
                    'visual_dim': self.visual_dim,
                    'audio_dim': self.audio_dim,
                    'text_dim': self.text_dim,
                    'output_dim': self.output_dim,
                    'fusion_strategies': self.fusion_strategies
                }
            }

            # ä¿å­˜è·¨æ¨¡æ…‹æ³¨æ„åŠ›æ¨¡å‹
            if self.cross_attention:
                torch.save(self.cross_attention.state_dict(),
                          save_path / 'cross_attention.pth')
                save_data['cross_attention_saved'] = True

            # ä¿å­˜å…¶ä»–åƒæ•¸
            with open(save_path / 'fusion_model.pkl', 'wb') as f:
                pickle.dump(save_data, f)

            print(f"ğŸ’¾ å¤šæ¨¡æ…‹èåˆæ¨¡å‹å·²ä¿å­˜: {save_path}")

        except Exception as e:
            print(f"âŒ æ¨¡å‹ä¿å­˜å¤±æ•—: {str(e)}")


def load_multimodal_features(features_root: Path = FEATURES_ROOT) -> Dict[str, Dict]:
    """
    è¼‰å…¥æ‰€æœ‰æ¨¡æ…‹çš„ç‰¹å¾µè³‡æ–™

    Args:
        features_root: ç‰¹å¾µæ ¹ç›®éŒ„

    Returns:
        æ¨¡æ…‹ç‰¹å¾µå­—å…¸
    """
    try:
        print("ğŸ”„ è¼‰å…¥å¤šæ¨¡æ…‹ç‰¹å¾µè³‡æ–™...")

        multimodal_data = {
            'visual': {},
            'audio': {},
            'text': {}
        }

        # 1. è¼‰å…¥è¦–è¦ºç‰¹å¾µï¼ˆMediaPipe + å…‰æµï¼‰
        visual_root = features_root / "mediapipe_features"
        optical_root = features_root / "optical_flow_features"

        # 2. è¼‰å…¥éŸ³è¨Šç‰¹å¾µ
        audio_root = features_root / "audio_features"
        if audio_root.exists():
            for audio_file in audio_root.glob("*_normalized_24d.npy"):
                word = audio_file.stem.replace('_normalized_24d', '')
                if word in SIGN_LANGUAGE_VOCABULARY:
                    multimodal_data['audio'][word] = np.load(audio_file)

        # 3. è¼‰å…¥æ–‡å­—ç‰¹å¾µ
        text_root = features_root / "text_embeddings"
        if (text_root / "unified_embeddings.npy").exists():
            unified_text = np.load(text_root / "unified_embeddings.npy")
            for i, word in enumerate(SIGN_LANGUAGE_VOCABULARY):
                if i < unified_text.shape[0]:
                    multimodal_data['text'][word] = unified_text[i]

        # çµ±è¨ˆè¼‰å…¥çµæœ
        for modality, data in multimodal_data.items():
            print(f"   ğŸ“Š {modality}: {len(data)} å€‹è©å½™ç‰¹å¾µ")

        return multimodal_data

    except Exception as e:
        print(f"âŒ å¤šæ¨¡æ…‹ç‰¹å¾µè¼‰å…¥å¤±æ•—: {str(e)}")
        return {}


def run_multimodal_fusion_pipeline(
    features_root: Optional[str] = None,
    output_root: Optional[str] = None,
    fusion_strategies: List[str] = ['early', 'attention', 'late']
) -> Dict[str, Any]:
    """
    åŸ·è¡Œå®Œæ•´çš„å¤šæ¨¡æ…‹èåˆç®¡ç·š

    Args:
        features_root: ç‰¹å¾µæ ¹ç›®éŒ„
        output_root: è¼¸å‡ºæ ¹ç›®éŒ„
        fusion_strategies: èåˆç­–ç•¥åˆ—è¡¨

    Returns:
        èåˆçµæœ
    """
    # è·¯å¾‘é…ç½®
    features_path = Path(features_root) if features_root else FEATURES_ROOT
    output_path = Path(output_root) if output_root else OUTPUT_ROOT
    output_path.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ”€ é–‹å§‹å¤šæ¨¡æ…‹èåˆç®¡ç·š")
    print(f"   ğŸ“‚ ç‰¹å¾µæ ¹ç›®éŒ„: {features_path}")
    print(f"   ğŸ“‚ è¼¸å‡ºç›®éŒ„: {output_path}")

    try:
        # 1. è¼‰å…¥å¤šæ¨¡æ…‹ç‰¹å¾µ
        multimodal_data = load_multimodal_features(features_path)

        if not any(multimodal_data.values()):
            print("âŒ æ²’æœ‰è¼‰å…¥åˆ°ä»»ä½•æ¨¡æ…‹ç‰¹å¾µ")
            return {}

        # 2. æº–å‚™è¨“ç·´è³‡æ–™
        # é€™è£¡ä½¿ç”¨æ–‡å­—å’ŒéŸ³è¨Šç‰¹å¾µä½œç‚ºç¤ºä¾‹
        words_with_all_modalities = set(multimodal_data['audio'].keys()) & set(multimodal_data['text'].keys())
        print(f"   ğŸ“Š å…·å‚™å¤šæ¨¡æ…‹ç‰¹å¾µçš„è©å½™: {len(words_with_all_modalities)} / {len(SIGN_LANGUAGE_VOCABULARY)}")

        if len(words_with_all_modalities) < 5:
            print("âš ï¸  å¯ç”¨æ–¼èåˆçš„è©å½™å¤ªå°‘ï¼Œç„¡æ³•é€²è¡Œæœ‰æ•ˆè¨“ç·´")
            return {}

        # æº–å‚™ç‰¹å¾µçŸ©é™£
        audio_matrix = []
        text_matrix = []
        word_labels = []

        for word in words_with_all_modalities:
            audio_matrix.append(multimodal_data['audio'][word])
            text_matrix.append(multimodal_data['text'][word])
            word_labels.append(word)

        audio_features = np.array(audio_matrix)
        text_features = np.array(text_matrix)

        print(f"   ğŸ“Š è¨“ç·´è³‡æ–™å½¢ç‹€: éŸ³è¨Š{audio_features.shape}, æ–‡å­—{text_features.shape}")

        # 3. åˆå§‹åŒ–ä¸¦è¨“ç·´èåˆå™¨
        fusion_system = MultiModalFusion(
            visual_dim=512,  # æš«æ™‚ä½¿ç”¨é è¨­å€¼
            audio_dim=audio_features.shape[1],
            text_dim=text_features.shape[1],
            output_dim=256,
            fusion_strategies=fusion_strategies
        )

        # è¨“ç·´èåˆå™¨ï¼ˆç›®å‰åªæœ‰éŸ³è¨Šå’Œæ–‡å­—ï¼‰
        training_info = fusion_system.fit(
            visual_features=None,
            audio_features=audio_features,
            text_features=text_features
        )

        # 4. åŸ·è¡Œèåˆæ¸¬è©¦
        fusion_results = {}
        for strategy in fusion_strategies:
            if strategy in ['early', 'late'] or (strategy == 'attention' and fusion_system.cross_attention):
                result = fusion_system.fuse_features(
                    visual_features=None,
                    audio_features=audio_features,
                    text_features=text_features,
                    strategy=strategy
                )
                if result:
                    fusion_results[strategy] = result

        # 5. ä¿å­˜çµæœ
        results = {
            'training_info': training_info,
            'fusion_results': fusion_results,
            'word_labels': word_labels,
            'feature_shapes': {
                'audio': audio_features.shape,
                'text': text_features.shape
            },
            'available_words': list(words_with_all_modalities)
        }

        # ä¿å­˜èåˆç‰¹å¾µ
        for strategy, result in fusion_results.items():
            for key, features in result.items():
                if isinstance(features, np.ndarray):
                    save_file = output_path / f"{strategy}_{key}.npy"
                    np.save(save_file, features)
                    print(f"ğŸ’¾ å·²ä¿å­˜: {save_file}")

        # ä¿å­˜èåˆå™¨æ¨¡å‹
        fusion_system.save_fusion_model(output_path)

        # ä¿å­˜çµæœæ‘˜è¦
        with open(output_path / 'fusion_summary.json', 'w', encoding='utf-8') as f:
            # å°‡numpy arrayè½‰æ›ç‚ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
            json_results = {}
            for key, value in results.items():
                if key == 'fusion_results':
                    continue  # è·³énumpy arrays
                json_results[key] = value

            json.dump(json_results, f, indent=2, ensure_ascii=False)

        print(f"ğŸ”€ å¤šæ¨¡æ…‹èåˆç®¡ç·šå®Œæˆ!")
        print(f"   âœ… æ”¯æ´ç­–ç•¥: {list(fusion_results.keys())}")
        print(f"   ğŸ“Š è™•ç†è©å½™: {len(word_labels)}")

        return results

    except Exception as e:
        print(f"âŒ å¤šæ¨¡æ…‹èåˆç®¡ç·šå¤±æ•—: {str(e)}")
        return {}


if __name__ == "__main__":
    # åŸ·è¡Œå¤šæ¨¡æ…‹èåˆç®¡ç·š
    results = run_multimodal_fusion_pipeline()