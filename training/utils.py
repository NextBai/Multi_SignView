"""
Training Utilities
è¨“ç·´å·¥å…·æ¨¡çµ„
"""

import torch
import torch.nn as nn
import numpy as np
import time
from pathlib import Path
from typing import Dict, List, Optional, Any
import json


class EarlyStopping:
    """æ—©åœæ©Ÿåˆ¶"""

    def __init__(self, patience: int = 10, min_delta: float = 0.001, mode: str = 'min'):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_value = None
        self.is_better = self._get_is_better_func()

    def _get_is_better_func(self):
        if self.mode == 'min':
            return lambda current, best: current < best - self.min_delta
        else:  # mode == 'max'
            return lambda current, best: current > best + self.min_delta

    def __call__(self, current_value: float) -> bool:
        if self.best_value is None:
            self.best_value = current_value
            return False

        if self.is_better(current_value, self.best_value):
            self.best_value = current_value
            self.counter = 0
        else:
            self.counter += 1

        return self.counter >= self.patience

    def reset(self):
        self.counter = 0
        self.best_value = None


class ModelCheckpoint:
    """æ¨¡å‹æª¢æŸ¥é»ç®¡ç†"""

    def __init__(
        self,
        save_dir: str,
        monitor: str = 'val_acc',
        mode: str = 'max',
        save_best_only: bool = True,
        filename_format: str = 'model_epoch_{epoch:03d}_acc_{acc:.3f}.pth'
    ):
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        self.monitor = monitor
        self.mode = mode
        self.save_best_only = save_best_only
        self.filename_format = filename_format
        self.best_value = None
        self.is_better = self._get_is_better_func()

    def _get_is_better_func(self):
        if self.mode == 'min':
            return lambda current, best: current < best
        else:  # mode == 'max'
            return lambda current, best: current > best

    def update(self, current_value: float, model: nn.Module, epoch: int) -> bool:
        """æ›´æ–°æª¢æŸ¥é»ï¼Œè¿”å›æ˜¯å¦ç‚ºæœ€ä½³æ¨¡å‹"""
        is_best = False

        if self.best_value is None or self.is_better(current_value, self.best_value):
            self.best_value = current_value
            is_best = True

        if not self.save_best_only or is_best:
            filename = self.filename_format.format(epoch=epoch, acc=current_value)
            filepath = self.save_dir / filename
            torch.save(model.state_dict(), filepath)

            if is_best:
                # åŒæ™‚ä¿å­˜ç‚ºbestæ¨¡å‹
                best_filepath = self.save_dir / 'best_model.pth'
                torch.save(model.state_dict(), best_filepath)

        return is_best


class TrainingLogger:
    """è¨“ç·´æ—¥èªŒè¨˜éŒ„å™¨"""

    def __init__(self, log_file: Optional[str] = None):
        self.log_file = log_file
        self.history = []

    def log(self, epoch: int, metrics: Dict[str, float], extra_info: str = ""):
        """è¨˜éŒ„è¨“ç·´æ—¥èªŒ"""
        log_entry = {
            'epoch': epoch,
            'timestamp': time.time(),
            'metrics': metrics,
            'extra_info': extra_info
        }
        self.history.append(log_entry)

        # æ§åˆ¶å°è¼¸å‡º
        metrics_str = ", ".join([f"{k}={v:.4f}" for k, v in metrics.items()])
        print(f"Epoch {epoch:3d}: {metrics_str} {extra_info}")

        # æ–‡ä»¶è¼¸å‡º
        if self.log_file:
            with open(self.log_file, 'a') as f:
                f.write(f"Epoch {epoch:3d}: {metrics_str} {extra_info}\n")

    def save_history(self, filepath: str):
        """ä¿å­˜è¨“ç·´æ­·å²"""
        with open(filepath, 'w') as f:
            json.dump(self.history, f, indent=2)


class MetricTracker:
    """æŒ‡æ¨™è¿½è¹¤å™¨"""

    def __init__(self):
        self.metrics = {}
        self.history = {}

    def update(self, **kwargs):
        """æ›´æ–°æŒ‡æ¨™"""
        for key, value in kwargs.items():
            if key not in self.metrics:
                self.metrics[key] = []
                self.history[key] = []

            self.metrics[key].append(value)

    def get_average(self, key: str) -> float:
        """ç²å–æŒ‡æ¨™å¹³å‡å€¼"""
        if key in self.metrics and self.metrics[key]:
            return sum(self.metrics[key]) / len(self.metrics[key])
        return 0.0

    def reset(self):
        """é‡ç½®ç•¶å‰epochçš„æŒ‡æ¨™"""
        for key in self.metrics:
            self.history[key].extend(self.metrics[key])
            self.metrics[key] = []

    def get_history(self, key: str) -> List[float]:
        """ç²å–æŒ‡æ¨™æ­·å²"""
        return self.history.get(key, [])


class LearningRateScheduler:
    """å­¸ç¿’ç‡èª¿åº¦å™¨"""

    def __init__(
        self,
        optimizer: torch.optim.Optimizer,
        schedule_type: str = 'cosine',
        **kwargs
    ):
        self.optimizer = optimizer
        self.schedule_type = schedule_type

        if schedule_type == 'cosine':
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=kwargs.get('T_max', 50),
                eta_min=kwargs.get('eta_min', 1e-6)
            )
        elif schedule_type == 'step':
            self.scheduler = torch.optim.lr_scheduler.StepLR(
                optimizer,
                step_size=kwargs.get('step_size', 10),
                gamma=kwargs.get('gamma', 0.5)
            )
        elif schedule_type == 'plateau':
            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode=kwargs.get('mode', 'min'),
                factor=kwargs.get('factor', 0.5),
                patience=kwargs.get('patience', 5)
            )
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„èª¿åº¦é¡å‹: {schedule_type}")

    def step(self, metric: Optional[float] = None):
        """èª¿åº¦å­¸ç¿’ç‡"""
        if self.schedule_type == 'plateau':
            if metric is not None:
                self.scheduler.step(metric)
        else:
            self.scheduler.step()

    def get_lr(self) -> float:
        """ç²å–ç•¶å‰å­¸ç¿’ç‡"""
        return self.optimizer.param_groups[0]['lr']


class ProgressBar:
    """é€²åº¦æ¢"""

    def __init__(self, total: int, desc: str = ""):
        self.total = total
        self.desc = desc
        self.current = 0
        self.start_time = time.time()

    def update(self, step: int = 1):
        """æ›´æ–°é€²åº¦"""
        self.current += step
        self._display()

    def _display(self):
        """é¡¯ç¤ºé€²åº¦æ¢"""
        percent = self.current / self.total
        bar_length = 50
        filled_length = int(bar_length * percent)

        bar = 'â–ˆ' * filled_length + 'â–’' * (bar_length - filled_length)
        elapsed_time = time.time() - self.start_time
        eta = elapsed_time / percent - elapsed_time if percent > 0 else 0

        print(f'\r{self.desc} |{bar}| {self.current}/{self.total} '
              f'({percent:.1%}) ETA: {eta:.1f}s', end='', flush=True)

        if self.current >= self.total:
            print()  # æ›è¡Œ


class GradientClipping:
    """æ¢¯åº¦è£å‰ª"""

    def __init__(self, max_norm: float = 1.0, norm_type: float = 2.0):
        self.max_norm = max_norm
        self.norm_type = norm_type

    def __call__(self, model: nn.Module) -> float:
        """åŸ·è¡Œæ¢¯åº¦è£å‰ª"""
        return torch.nn.utils.clip_grad_norm_(
            model.parameters(),
            max_norm=self.max_norm,
            norm_type=self.norm_type
        ).item()


class ModelSummary:
    """æ¨¡å‹æ‘˜è¦å·¥å…·"""

    @staticmethod
    def count_parameters(model: nn.Module) -> Dict[str, int]:
        """è¨ˆç®—æ¨¡å‹åƒæ•¸"""
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        frozen_params = total_params - trainable_params

        return {
            'total': total_params,
            'trainable': trainable_params,
            'frozen': frozen_params
        }

    @staticmethod
    def get_model_size(model: nn.Module) -> float:
        """ç²å–æ¨¡å‹å¤§å°ï¼ˆMBï¼‰"""
        param_size = 0
        buffer_size = 0

        for param in model.parameters():
            param_size += param.nelement() * param.element_size()

        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()

        size_mb = (param_size + buffer_size) / 1024 / 1024
        return size_mb

    @staticmethod
    def print_summary(model: nn.Module, model_name: str = "Model"):
        """æ‰“å°æ¨¡å‹æ‘˜è¦"""
        params = ModelSummary.count_parameters(model)
        size_mb = ModelSummary.get_model_size(model)

        print(f"\nğŸ“Š {model_name} æ‘˜è¦:")
        print(f"   ç¸½åƒæ•¸æ•¸é‡: {params['total']:,}")
        print(f"   å¯è¨“ç·´åƒæ•¸: {params['trainable']:,}")
        print(f"   å‡çµåƒæ•¸: {params['frozen']:,}")
        print(f"   æ¨¡å‹å¤§å°: {size_mb:.2f} MB")


class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""

    def __init__(self, config_file: Optional[str] = None):
        self.config = {}
        if config_file:
            self.load_config(config_file)

    def load_config(self, config_file: str):
        """è¼‰å…¥é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r') as f:
                self.config = json.load(f)
        except Exception as e:
            print(f"è¼‰å…¥é…ç½®æ–‡ä»¶å¤±æ•—: {e}")

    def save_config(self, config_file: str):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        with open(config_file, 'w') as f:
            json.dump(self.config, f, indent=2)

    def get(self, key: str, default: Any = None):
        """ç²å–é…ç½®å€¼"""
        keys = key.split('.')
        value = self.config
        for k in keys:
            if isinstance(value, dict) and k in value:
                value = value[k]
            else:
                return default
        return value

    def set(self, key: str, value: Any):
        """è¨­ç½®é…ç½®å€¼"""
        keys = key.split('.')
        config = self.config
        for k in keys[:-1]:
            if k not in config:
                config[k] = {}
            config = config[k]
        config[keys[-1]] = value


def set_random_seed(seed: int = 42):
    """è¨­ç½®éš¨æ©Ÿç¨®å­"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    import random
    random.seed(seed)

    # ç¢ºä¿å¯é‡ç¾æ€§
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def get_device(prefer_cuda: bool = True) -> torch.device:
    """ç²å–è¨ˆç®—è¨­å‚™"""
    if prefer_cuda and torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"ğŸš€ ä½¿ç”¨ GPU: {torch.cuda.get_device_name()}")
    else:
        device = torch.device('cpu')
        print("ğŸ’» ä½¿ç”¨ CPU")

    return device


def save_checkpoint(
    model: nn.Module,
    optimizer: torch.optim.Optimizer,
    scheduler: Any,
    epoch: int,
    loss: float,
    filepath: str,
    **kwargs
):
    """ä¿å­˜å®Œæ•´æª¢æŸ¥é»"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'loss': loss,
        'timestamp': time.time(),
        **kwargs
    }
    torch.save(checkpoint, filepath)


def load_checkpoint(
    filepath: str,
    model: nn.Module,
    optimizer: Optional[torch.optim.Optimizer] = None,
    scheduler: Optional[Any] = None,
    device: Optional[torch.device] = None
) -> Dict[str, Any]:
    """è¼‰å…¥å®Œæ•´æª¢æŸ¥é»"""
    checkpoint = torch.load(filepath, map_location=device)

    model.load_state_dict(checkpoint['model_state_dict'])

    if optimizer and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

    return checkpoint


if __name__ == "__main__":
    # æ¸¬è©¦å·¥å…·å‡½æ•¸
    print("ğŸ§ª æ¸¬è©¦è¨“ç·´å·¥å…·...")

    # æ¸¬è©¦æ—©åœ
    early_stopping = EarlyStopping(patience=3)
    test_losses = [1.0, 0.8, 0.9, 0.85, 0.87, 0.86]
    for i, loss in enumerate(test_losses):
        if early_stopping(loss):
            print(f"Early stopping triggered at step {i}")
            break

    # æ¸¬è©¦æŒ‡æ¨™è¿½è¹¤
    tracker = MetricTracker()
    tracker.update(loss=1.0, acc=0.8)
    tracker.update(loss=0.9, acc=0.85)
    print(f"Average loss: {tracker.get_average('loss'):.3f}")

    print("âœ… å·¥å…·æ¸¬è©¦å®Œæˆ")