"""
Multi-Modal Sign Language Dataset
ä¸‰æ¨¡æ…‹æ‰‹èªæ•¸æ“šé›†å¯¦ä½œ
"""

import json
import random
import numpy as np
import torch
from torch.utils.data import Dataset
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
import warnings

class TriModalDataset(Dataset):
    """
    ä¸‰æ¨¡æ…‹æ‰‹èªæ•¸æ“šé›†

    æ”¯æ´ç‰¹æ€§:
    - éˆæ´»çš„æ¨¡æ…‹çµ„åˆ: visual, audio, text
    - å‹•æ…‹æ•¸æ“šå¢å¼·: æ™‚é–“æŠ–å‹•ã€å™ªéŸ³æ³¨å…¥
    - æ‰¹æ¬¡è¼‰å…¥å„ªåŒ–: é è¼‰å…¥å¸¸ç”¨ç‰¹å¾µ
    - è©å½™èªç¾©åˆ†é¡: æ ¹æ“šèªç¾©é¡åˆ¥é€²è¡Œæ¡æ¨£
    """

    def __init__(
        self,
        mapping_file: str = "features/trimodal_mapping.json",
        mode: str = 'train',
        modalities: List[str] = ['visual', 'audio', 'text'],
        visual_augment: bool = True,
        audio_dropout: float = 0.1,
        text_embedding_type: str = 'unified',
        semantic_sampling: bool = False,
        max_samples_per_word: Optional[int] = None
    ):
        """
        åˆå§‹åŒ–ä¸‰æ¨¡æ…‹æ•¸æ“šé›†

        Args:
            mapping_file: ä¸‰æ¨¡æ…‹æ˜ å°„æ–‡ä»¶è·¯å¾‘
            mode: æ•¸æ“šé›†æ¨¡å¼ ('train', 'val', 'test')
            modalities: ä½¿ç”¨çš„æ¨¡æ…‹åˆ—è¡¨
            visual_augment: æ˜¯å¦ä½¿ç”¨è¦–è¦ºæ•¸æ“šå¢å¼·
            audio_dropout: éŸ³è¨Šdropoutæ¯”ä¾‹
            text_embedding_type: æ–‡å­—åµŒå…¥é¡å‹ ('unified', 'word2vec', 'fasttext', 'bert')
            semantic_sampling: æ˜¯å¦ä½¿ç”¨èªç¾©å¹³è¡¡æ¡æ¨£
            max_samples_per_word: æ¯å€‹è©å½™æœ€å¤§æ¨£æœ¬æ•¸ (ç”¨æ–¼æ•¸æ“šå¹³è¡¡)
        """
        self.mapping_file = mapping_file
        self.mode = mode
        self.modalities = modalities
        self.visual_augment = visual_augment and (mode == 'train')
        self.audio_dropout = audio_dropout if mode == 'train' else 0.0
        self.text_embedding_type = text_embedding_type
        self.semantic_sampling = semantic_sampling
        self.max_samples_per_word = max_samples_per_word

        # è¼‰å…¥æ˜ å°„æ–‡ä»¶
        self._load_mapping()

        # å»ºç«‹è©å½™ç´¢å¼•
        self._build_vocabulary()

        # æ§‹å»ºæ¨£æœ¬åˆ—è¡¨
        self._build_sample_list()

        # è¼‰å…¥æ–‡å­—åµŒå…¥ (å¦‚æœéœ€è¦)
        if 'text' in self.modalities:
            self._load_text_embeddings()

        # è¼‰å…¥èªç¾©åˆ†é¡ (å¦‚æœä½¿ç”¨èªç¾©æ¡æ¨£)
        if self.semantic_sampling:
            self._load_semantic_categories()

        print(f"ğŸ“Š æ•¸æ“šé›†åˆå§‹åŒ–å®Œæˆ:")
        print(f"   æ¨¡å¼: {self.mode}")
        print(f"   æ¨¡æ…‹: {', '.join(self.modalities)}")
        print(f"   æ¨£æœ¬æ•¸: {len(self.samples)}")
        print(f"   è©å½™æ•¸: {len(self.words)}")

    def _load_mapping(self):
        """è¼‰å…¥ä¸‰æ¨¡æ…‹æ˜ å°„æ–‡ä»¶"""
        try:
            with open(self.mapping_file, 'r', encoding='utf-8') as f:
                self.mapping = json.load(f)
        except FileNotFoundError:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ˜ å°„æ–‡ä»¶: {self.mapping_file}")
        except json.JSONDecodeError:
            raise ValueError(f"æ˜ å°„æ–‡ä»¶æ ¼å¼éŒ¯èª¤: {self.mapping_file}")

    def _build_vocabulary(self):
        """å»ºç«‹è©å½™è¡¨å’Œç´¢å¼•"""
        self.words = list(self.mapping.keys())
        self.word_to_idx = {word: idx for idx, word in enumerate(self.words)}
        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}
        self.num_classes = len(self.words)

    def _build_sample_list(self):
        """æ§‹å»ºæ¨£æœ¬åˆ—è¡¨ (word, sample_idx)"""
        self.samples = []

        for word in self.words:
            # æª¢æŸ¥è¦–è¦ºç‰¹å¾µç›®éŒ„
            visual_info = self.mapping[word]['modalities']['visual']
            if not visual_info['available']:
                warnings.warn(f"è©å½™ '{word}' è¦–è¦ºç‰¹å¾µä¸å¯ç”¨ï¼Œè·³é")
                continue

            visual_dir = Path(visual_info['mediapipe_dir'])
            if not visual_dir.exists():
                warnings.warn(f"è¦–è¦ºç‰¹å¾µç›®éŒ„ä¸å­˜åœ¨: {visual_dir}")
                continue

            # ç²å–è©²è©å½™çš„æ‰€æœ‰æ¨£æœ¬æ–‡ä»¶
            visual_files = sorted(list(visual_dir.glob('*.npy')))

            # é™åˆ¶æ¯å€‹è©å½™çš„æ¨£æœ¬æ•¸é‡ (æ•¸æ“šå¹³è¡¡)
            if self.max_samples_per_word:
                visual_files = visual_files[:self.max_samples_per_word]

            # æ ¹æ“šæ¨¡å¼åˆ†å‰²æ•¸æ“š
            if self.mode == 'train':
                # è¨“ç·´é›†: å‰80%
                split_idx = int(0.8 * len(visual_files))
                selected_files = visual_files[:split_idx]
            elif self.mode == 'val':
                # é©—è­‰é›†: 80%-90%
                split_start = int(0.8 * len(visual_files))
                split_end = int(0.9 * len(visual_files))
                selected_files = visual_files[split_start:split_end]
            else:  # test
                # æ¸¬è©¦é›†: å¾Œ10%
                split_idx = int(0.9 * len(visual_files))
                selected_files = visual_files[split_idx:]

            # æ·»åŠ æ¨£æœ¬åˆ°åˆ—è¡¨
            for i in range(len(selected_files)):
                self.samples.append((word, i, len(selected_files)))

    def _load_text_embeddings(self):
        """è¼‰å…¥æ–‡å­—åµŒå…¥çŸ©é™£"""
        embedding_files = {
            'unified': 'features/text_embeddings/unified_embeddings.npy',
            'word2vec': 'features/text_embeddings/word2vec_embeddings.npy',
            'fasttext': 'features/text_embeddings/fasttext_embeddings.npy',
            'bert': 'features/text_embeddings/bert_embeddings.npy'
        }

        if self.text_embedding_type not in embedding_files:
            raise ValueError(f"ä¸æ”¯æ´çš„æ–‡å­—åµŒå…¥é¡å‹: {self.text_embedding_type}")

        try:
            self.text_embeddings = np.load(embedding_files[self.text_embedding_type])
            print(f"âœ… è¼‰å…¥æ–‡å­—åµŒå…¥: {self.text_embedding_type} {self.text_embeddings.shape}")
        except FileNotFoundError:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡å­—åµŒå…¥æ–‡ä»¶: {embedding_files[self.text_embedding_type]}")

    def _load_semantic_categories(self):
        """è¼‰å…¥èªç¾©åˆ†é¡è³‡è¨Š"""
        try:
            with open('features/semantic_features/semantic_analysis_fixed.json', 'r', encoding='utf-8') as f:
                semantic_data = json.load(f)
            self.semantic_categories = semantic_data.get('semantic_categories', {})
            print(f"âœ… è¼‰å…¥èªç¾©åˆ†é¡: {len(self.semantic_categories)}å€‹é¡åˆ¥")
        except FileNotFoundError:
            warnings.warn("æ‰¾ä¸åˆ°èªç¾©åˆ†é¡æ–‡ä»¶ï¼Œç¦ç”¨èªç¾©æ¡æ¨£")
            self.semantic_sampling = False

    def __len__(self) -> int:
        """è¿”å›æ•¸æ“šé›†å¤§å°"""
        return len(self.samples)

    def __getitem__(self, idx: int) -> Tuple[Dict[str, torch.Tensor], int]:
        """
        ç²å–å–®å€‹æ¨£æœ¬

        Returns:
            features: å­—å…¸ï¼ŒåŒ…å«é¸å®šæ¨¡æ…‹çš„ç‰¹å¾µ
            label: è©å½™æ¨™ç±¤ç´¢å¼•
        """
        word, sample_idx, total_samples = self.samples[idx]
        label = self.word_to_idx[word]

        features = {}

        # è¼‰å…¥è¦–è¦ºç‰¹å¾µ
        if 'visual' in self.modalities:
            features['visual'] = self._load_visual_features(word, sample_idx, total_samples)

        # è¼‰å…¥éŸ³è¨Šç‰¹å¾µ
        if 'audio' in self.modalities:
            features['audio'] = self._load_audio_features(word)

        # è¼‰å…¥æ–‡å­—ç‰¹å¾µ
        if 'text' in self.modalities:
            features['text'] = self._load_text_features(label)

        return features, label

    def _load_visual_features(self, word: str, sample_idx: int, total_samples: int) -> torch.Tensor:
        """è¼‰å…¥è¦–è¦ºç‰¹å¾µ"""
        visual_dir = Path(self.mapping[word]['modalities']['visual']['mediapipe_dir'])
        visual_files = sorted(list(visual_dir.glob('*.npy')))

        # æ ¹æ“šæ•¸æ“šåˆ†å‰²é¸æ“‡æ­£ç¢ºçš„æ–‡ä»¶
        if self.mode == 'train':
            split_idx = int(0.8 * len(visual_files))
            available_files = visual_files[:split_idx]
        elif self.mode == 'val':
            split_start = int(0.8 * len(visual_files))
            split_end = int(0.9 * len(visual_files))
            available_files = visual_files[split_start:split_end]
        else:  # test
            split_idx = int(0.9 * len(visual_files))
            available_files = visual_files[split_idx:]

        # è¼‰å…¥è¦–è¦ºæ•¸æ“š
        visual_data = np.load(available_files[sample_idx])  # (100, 417)

        # æ•¸æ“šå¢å¼· (åƒ…è¨“ç·´æ¨¡å¼)
        if self.visual_augment:
            visual_data = self._augment_visual(visual_data)

        return torch.FloatTensor(visual_data)

    def _load_audio_features(self, word: str) -> torch.Tensor:
        """è¼‰å…¥éŸ³è¨Šç‰¹å¾µ"""
        # ä½¿ç”¨normalizedç‰ˆæœ¬çš„éŸ³è¨Šç‰¹å¾µ
        audio_file = f"features/audio_features/{word}_normalized_24d.npy"

        try:
            audio_data = np.load(audio_file)  # (24,)
        except FileNotFoundError:
            warnings.warn(f"æ‰¾ä¸åˆ°éŸ³è¨Šç‰¹å¾µæ–‡ä»¶: {audio_file}")
            # è¿”å›é›¶å‘é‡ä½œç‚ºfallback
            audio_data = np.zeros(24, dtype=np.float32)

        # éŸ³è¨Šdropout (åƒ…è¨“ç·´æ¨¡å¼)
        if self.audio_dropout > 0:
            dropout_mask = np.random.random(audio_data.shape) > self.audio_dropout
            audio_data = audio_data * dropout_mask

        return torch.FloatTensor(audio_data)

    def _load_text_features(self, label: int) -> torch.Tensor:
        """è¼‰å…¥æ–‡å­—ç‰¹å¾µ"""
        text_data = self.text_embeddings[label]  # (300,) or (768,)
        return torch.FloatTensor(text_data)

    def _augment_visual(self, visual_data: np.ndarray) -> np.ndarray:
        """
        è¦–è¦ºæ•¸æ“šå¢å¼·

        å¢å¼·ç­–ç•¥:
        1. æ™‚é–“è»¸éš¨æ©Ÿåç§» (Â±5å¹€)
        2. é«˜æ–¯å™ªéŸ³æ³¨å…¥ (Ïƒ=0.01)
        3. éš¨æ©Ÿå¹€dropout (æ¦‚ç‡0.05)
        4. ç©ºé–“åº§æ¨™å¾®èª¿ (Â±2åƒç´ )
        """
        augmented_data = visual_data.copy()

        # 1. æ™‚é–“è»¸éš¨æ©Ÿåç§»
        if random.random() < 0.5:
            shift = random.randint(-5, 5)
            if shift != 0:
                augmented_data = np.roll(augmented_data, shift, axis=0)

        # 2. é«˜æ–¯å™ªéŸ³æ³¨å…¥
        if random.random() < 0.3:
            noise_std = random.uniform(0.005, 0.015)
            noise = np.random.normal(0, noise_std, augmented_data.shape)
            augmented_data = augmented_data + noise

        # 3. éš¨æ©Ÿå¹€dropout
        if random.random() < 0.2:
            dropout_prob = random.uniform(0.02, 0.08)
            dropout_mask = np.random.random(augmented_data.shape[0]) > dropout_prob
            for i, keep in enumerate(dropout_mask):
                if not keep and i > 0:
                    # ä½¿ç”¨å‰ä¸€å¹€æ•¸æ“šæ›¿ä»£
                    augmented_data[i] = augmented_data[i-1]

        # 4. ç©ºé–“åº§æ¨™å¾®èª¿ (åƒ…å°x,yåº§æ¨™ï¼Œä¿æŒzåº§æ¨™å’Œvisibilityä¸è®Š)
        if random.random() < 0.4:
            # MediaPipeç‰¹å¾µçµæ§‹: æ¯3å€‹ç¶­åº¦ç‚ºä¸€çµ„ (x,y,z)
            coordinate_noise = np.random.normal(0, 0.002, augmented_data.shape)
            # åªå°x,yåº§æ¨™æ·»åŠ å™ªéŸ³ (æ¯çµ„çš„å‰å…©å€‹ç¶­åº¦)
            for i in range(0, augmented_data.shape[1], 3):
                if i+1 < augmented_data.shape[1]:
                    augmented_data[:, i:i+2] += coordinate_noise[:, i:i+2]

        return augmented_data

    def get_word_samples(self, word: str) -> List[Tuple[Dict[str, torch.Tensor], int]]:
        """ç²å–ç‰¹å®šè©å½™çš„æ‰€æœ‰æ¨£æœ¬"""
        word_samples = []
        for i, (sample_word, _, _) in enumerate(self.samples):
            if sample_word == word:
                features, label = self[i]
                word_samples.append((features, label))
        return word_samples

    def get_semantic_category_samples(self, category: str) -> List[Tuple[Dict[str, torch.Tensor], int]]:
        """æ ¹æ“šèªç¾©é¡åˆ¥ç²å–æ¨£æœ¬"""
        if not self.semantic_sampling:
            raise ValueError("èªç¾©æ¡æ¨£æœªå•Ÿç”¨")

        if category not in self.semantic_categories:
            raise ValueError(f"æœªçŸ¥èªç¾©é¡åˆ¥: {category}")

        category_words = self.semantic_categories[category]
        category_samples = []

        for word in category_words:
            if word in self.word_to_idx:
                word_samples = self.get_word_samples(word)
                category_samples.extend(word_samples)

        return category_samples

    def get_statistics(self) -> Dict[str, Union[int, float, Dict]]:
        """ç²å–æ•¸æ“šé›†çµ±è¨ˆè³‡è¨Š"""
        stats = {
            'total_samples': len(self.samples),
            'num_classes': self.num_classes,
            'modalities': self.modalities,
            'mode': self.mode,
            'samples_per_word': {}
        }

        # çµ±è¨ˆæ¯å€‹è©å½™çš„æ¨£æœ¬æ•¸
        for word in self.words:
            word_count = sum(1 for sample_word, _, _ in self.samples if sample_word == word)
            stats['samples_per_word'][word] = word_count

        # è¨ˆç®—çµ±è¨ˆé‡
        sample_counts = list(stats['samples_per_word'].values())
        stats['avg_samples_per_word'] = np.mean(sample_counts)
        stats['min_samples_per_word'] = np.min(sample_counts)
        stats['max_samples_per_word'] = np.max(sample_counts)
        stats['std_samples_per_word'] = np.std(sample_counts)

        return stats


class SemanticBalancedSampler:
    """
    èªç¾©å¹³è¡¡æ¡æ¨£å™¨
    ç¢ºä¿ä¸åŒèªç¾©é¡åˆ¥çš„æ¨£æœ¬åœ¨è¨“ç·´ä¸­å‡å‹»åˆ†å¸ƒ
    """

    def __init__(self, dataset: TriModalDataset, batch_size: int = 32):
        self.dataset = dataset
        self.batch_size = batch_size

        if not dataset.semantic_sampling:
            raise ValueError("æ•¸æ“šé›†å¿…é ˆå•Ÿç”¨èªç¾©æ¡æ¨£")

        self._build_category_indices()

    def _build_category_indices(self):
        """å»ºç«‹èªç¾©é¡åˆ¥ç´¢å¼•"""
        self.category_indices = {}

        for category, words in self.dataset.semantic_categories.items():
            indices = []
            for i, (word, _, _) in enumerate(self.dataset.samples):
                if word in words:
                    indices.append(i)
            self.category_indices[category] = indices

    def __iter__(self):
        """ç”Ÿæˆå¹³è¡¡çš„æ‰¹æ¬¡ç´¢å¼•"""
        categories = list(self.category_indices.keys())
        category_iterators = {
            cat: iter(np.random.permutation(indices))
            for cat, indices in self.category_indices.items()
        }

        while True:
            batch_indices = []

            for _ in range(self.batch_size):
                # éš¨æ©Ÿé¸æ“‡èªç¾©é¡åˆ¥
                category = random.choice(categories)

                try:
                    idx = next(category_iterators[category])
                    batch_indices.append(idx)
                except StopIteration:
                    # é‡æ–°æ´—ç‰Œè©²é¡åˆ¥
                    category_iterators[category] = iter(
                        np.random.permutation(self.category_indices[category])
                    )
                    idx = next(category_iterators[category])
                    batch_indices.append(idx)

            yield batch_indices